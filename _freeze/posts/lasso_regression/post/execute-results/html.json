{
  "hash": "d71e550f4a524f8c7c3bdb90bfb639dc",
  "result": {
    "markdown": "---\ntitle: 'Lasso Regression'\ndate: \"2024-02-06\"\ncategories: ['regression', 'R', 'lasso']\ndescription: \"What is, what is it used for, and how to use Lasso regression, with code in R.\"\nexecute: \n  message: false\n  warning: false\neditor_options: \n  chunk_output_type: console\nformat: \n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\n---\n\n\n# Introduction\n\nIn this post, I explain what Lasso regression is, what it is used for, and how to use it, with code in R.\n\n\n## What is Lasso regression?\n\nLasso regression is a modified version of linear regression whose objective is to find the simplest model possible. In order to do that, Lasso method penalizes large regression coefficients, leaving smaller coefficients and even removing some variables from the final model (i.e., setting their coefficients to zero).\n\nLasso is an acronym of **L**east **A**bsolute **S**hrinkage and **S**elector **O**perator.\n\n\n## What is Lasso regression used for?\n\nLasso regression is used mainly in two applications:\n\n* **Model variable selection**: Lasso can be used as a method to select the most important variables in a regression model. The least important variables will have their coefficients set to zero, effectively being removed from the final model.\n* **Parameter shrinkage**: Lasso's coefficients are smaller thant those of a simple lineal regression. This helps to avoid overfitting problems.\n\nGiven their two main functions, Lasso regression is usually employed in the following situations:\n\n* When we have a **high-dimensionality** dataset, i.e., with a large number of variables.\n* When we have **multicolineallity** in our model, i.e., several variables are lineally dependent of one another.\n* When we want to **automatize the model building**, via automatizing the selection of the included variables.\n\n## How does Lasso regression work?\n\nA traditional multivariable lineal regression model finds a set of regression coefficients ($\\beta_0, \\beta_1, \\beta_2...$) that minimizes the residuals' squared sum (RSS). That is, the distance between the datapoints and the model predictions.\n\nLasso regression adds another parameter called L1. L1 is defined as the sum of the absolute values of the model coefficients. Lasso method tries to minimize the sum of RSS and L1. As a consequence, Lasso finds a model with smaller regression coefficients. This whole process is known as \"L1 regularization\", and it produces a coefficient \"shrinkage\".\n\nEvery time we run a Lasso regression, whe need to specify the **lambda** parameter ($\\lambda$). Lambda represents the relative importance of the L1 parameter compared to the RSS part of the minimization formula.\n\n* With $\\lambda = 0$, there is no coefficient shrinkage, and the Lasso model is effectively equal to a regular linear regression model.\n* As $\\lambda$ grows, there is more shrinkage, and more variables are removed from the model.\n* If $\\lambda$ were to be infinite, all coefficients would be removed, and we would end up with an empty model.\n\n\n### Lasso regression formula\n\n$min(RSS + \\lambda \\sum |\\beta_j|)$\n\nWhere\n\n* $RSS$ is the residuals' square sum.\n* $\\lambda$ is Lasso's penalizing factor.\n* $\\sum |\\beta_j|$ is the sum of the absolute values of the regression coefficients.\n\n# Code in R\n\n## Getting Ready\n\nIn this example, we'll use the `glmnet` library and the example dataset in `mtcars`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"glmnet\") # Install the package (only once)\nlibrary(glmnet)\nhead(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n```\n:::\n:::\n\n\nWe'll use `mpg` (miles per galon) as the outcome variable, and `cyl` (number of cylinders), `hp` (horsepower), `wt` (weight), `gear` (gear number), and `drat` (rear axle ratio) as predictive variables. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the outcome variable\ny <- mtcars$mpg\n\n# Define the predictive variables\nx <- data.matrix(mtcars[, c(\"cyl\", \"hp\", \"wt\", \"drat\", \"gear\")])\n```\n:::\n\n\n\n## Choose a Value for Lambda\n\nWe can choose the value of $\\lambda$ that minimizes the mean-squared error (MSE). The `cv.glmnet()` function performs \"[K-fold cross-validation](https://www.statology.org/k-fold-cross-validation/)\" to identify this $\\lambda$ value.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cross-validation\ncv_model <- cv.glmnet(x, y, alpha = 1)  # Changing the alpha parameter leads to other types of regression\n\n# Find the lambda value that minimizes the MSE\nbest_lambda <- cv_model$lambda.min\nbest_lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3803991\n```\n:::\n\n```{.r .cell-code}\n# Display the results in a plot\nplot(cv_model)\n```\n\n::: {.cell-output-display}\n![](post_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe value of lambda that minimizes the MSE turns out to be 0.3803991, which in the plot corresponds to the point $Log(\\lambda)$ = -0.9665344.\n\n\n## Fitting the Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model coefficients\nbest_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)\ncoef(best_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) 35.99744772\ncyl         -0.85518839\nhp          -0.01598015\nwt          -2.88233444\ndrat         0.27858106\ngear         .         \n```\n:::\n:::\n\n\nWe can observe that the coefficient of `gear` appears as a point, indicating that the Lasso regression has eliminated the coefficient since the variable was not important enough.\n\n## Comparison with Linear Regression without Lasso\n\nFor comparison, we can see the coefficients that would result from a multiple linear regression model without parameter shrinkage or variable selection.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model <- lm(mpg ~ cyl + hp + wt + drat + gear, data = mtcars)\nmodel_table <- cbind(coef(best_model), coef(linear_model))\ncolnames(model_table) <- c(\"Lasso\", \"Linear\")\nmodel_table\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6 x 2 sparse Matrix of class \"dgCMatrix\"\n                  Lasso      Linear\n(Intercept) 35.99744772 33.99417771\ncyl         -0.85518839 -0.72169272\nhp          -0.01598015 -0.02227636\nwt          -2.88233444 -2.92715539\ndrat         0.27858106  0.73105753\ngear         .           0.16750690\n```\n:::\n:::\n\n\nThe coefficients of the Lasso model have been shrunk slightly, especially for the `drat` variable, and the `gear` variable has been automatically excluded.\n\n# Questions and Curiosities\n\nSome questions arise for further investigation and writing new posts:\n\n* How to choose the value of lambda?\n* How does the method of K-fold cross-validation work?\n* What sets Lasso apart from other similar models like Ridge?\n* What is the utility of Lasso regression in the field of public health? What databases can be used as an example?\n* What articles in the public health field are published using this methodology?\n\n\n# References\n\n* [Statology](https://www.statology.org/lasso-regression-in-r/)\n* [Geeks for Geeks](https://www.geeksforgeeks.org/lasso-regression-in-r-programming/)\n\n",
    "supporting": [
      "post_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}