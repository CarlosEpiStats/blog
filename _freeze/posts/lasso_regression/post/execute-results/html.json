{
  "hash": "7a4576c6c748a9ba877dd5e0f55731d3",
  "result": {
    "markdown": "---\ntitle: 'Lasso regression'\ndate: \"2024-01-26\"\ncategories: ['regression', 'R', 'lasso']\ndescription: \"What is, what is it used for, and how to use Lasso regression, with code in R.\"\nexecute: \n  message: false\n  warning: false\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Introduction\n\nIn this post, I explain what Lasso regression is, what it is used for, and how to use it, with code in R.\n\n\n## What is Lasso regression?\n\nLasso regression is a modified version of linear regression whose objective is to find the simplest model possible. In order to do that, Lasso method penalizes large regression coefficients, leaving smaller coefficients and even removing some variables from the final model (i.e., setting their coefficients to zero).\n\nLasso is an acronym of **L**east **A**bsolute **S**hrinkage and **S**elector **O**perator.\n\n\n## What is Lasso regression used for?\n\nLasso regression is used mainly in two applications:\n\n* Model variable selection: Lasso can be used as a method to select the most important variables in a regression model. The least important variables will have their coefficients set to zero, effectively being removed from the final model.\n* Parameter shrinkage: Lasso's coefficients are smaller thant those of a simple lineal regression. This helps to avoid overfitting problems.\n\nGiven their two main functions, Lasso regression is usually employed in the following situations:\n\n* When we have a **high-dimensionality** dataset, i.e., with a large number of variables.\n* When we have **multicolineallity** in our model, i.e., several variables are lineally dependent of one another.\n* When we want to **automatize the model building**, via automatizing the selection of the included variables.\n\n## How does Lasso regression work?\n\nA traditional multivariable lineal regression model finds a set of regression coefficients ($\\beta_0, \\beta_1, \\beta_2...$) that minimizes the residuals' squared sum (RSS). That is, the distance between the datapoints and the model predictions.\n\nLasso regression adds another parameter called L1. L1 is defined as the sum of the absolute values of the model coefficients. Lasso method tries to minimize the sum of RSS and L1. As a consequence, Lasso finds a model with smaller regression coefficients. This whole process is known as \"L1 regularization\", and it produces a coefficient \"shrinkage\".\n\nEvery time we run a Lasso regression, whe need to specify the **lambda** parameter ($\\lambda$). Lambda represents the relative importance of the L1 parameter compared to the RSS part of the minimization formula.\n\n* With $\\lambda = 0$, there is no coefficient shrinkage, and the Lasso model is effectively equal to a regular linear regression model.\n* As $\\lambda$ grows, there is more shrinkage, and more variables are removed from the model.\n* If $\\lambda$ were to be infinite, all coefficients would be removed, and we would end up with an empty model.\n\n\n### Lasso regression formula\n\n$min(RSS + \\lambda \\sum |\\beta_j|)$\n\nWhere\n\n* $RSS$ es the residuals' square sum.\n* $\\lambda$ is Lasso's penalizing factor.\n* $\\sum |\\beta_j|$ is the sum of the absolute values of the regression coefficients.\n\n# Code in R\n\n## Getting ready\n\nIn this example, we'll use the `glmnet` library and the example dataset in `mtcars`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"glmnet\") # Install the package (only once)\nlibrary(glmnet)\nhead(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n```\n:::\n:::\n\n\nWe'll use `mpg` (miles per galon) as the outcome variable, and `cyl` (number of cylinders), `hp` (horsepower), `wt` (weight), `gear` (gear number) y `drat` (axis relation?) as predictive variables. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the outcome variable\ny <- mtcars$mpg\n\n# Define the predictive variables\nx <- data.matrix(mtcars[, c(\"cyl\", \"hp\", \"wt\", \"drat\", \"gear\")])\n```\n:::\n\n\n\n## Choose a value for lambda\n\nPodemos elegir el valor de $\\lambda$ que minimice el error cuadrado medio (*mean-squared error*, MSE). La función `cv.glmnet()` realiza \"validación cruzada de k iteraciones\" ([K-fold cross-validation](https://www.statology.org/k-fold-cross-validation/)) para identificar este valor de $\\lambda$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# validación cruzada\ncv_model <- cv.glmnet(x, y, alpha = 1)  # Cambiar el parámetro alpha da lugar a otros tipos de regresión\n\n# encontrar el valor lambda que minimiza el MSE\nbest_lambda <- cv_model$lambda.min\nbest_lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.315814\n```\n:::\n\n```{.r .cell-code}\n# mostrar los resultados en un gráfico\nplot(cv_model)\n```\n\n::: {.cell-output-display}\n![](post_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nEl valor de lambda que minimiza el MSE resulta ser 0.315814, que en la gráfica corresponde al punto $Log(\\lambda)$ = -1.1526019.\n\n## Ajustar el modelo\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# coeficientes del modelo \nbest_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)\ncoef(best_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) 35.74777065\ncyl         -0.83999245\nhp          -0.01680443\nwt          -2.89776526\ndrat         0.36928132\ngear         .         \n```\n:::\n:::\n\n\nPodemos observar como el coeficiente de `gear` aparece como un punto, lo que indica que la regresión Lasso ha eliminado el coeficiente, ya que la variable no era lo suficientemente importante.\n\n## Comparación con la regresión lineal sin Lasso\n\nComo comparación, podemos ver los coeficientes que resultarían de un modelo de regresión lineal múltiple sin contracción de parámetros ni selección de variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model <- lm(mpg ~ cyl + hp + wt + drat + gear, data = mtcars)\ncbind(coef(best_model), \"Linear\" = coef(linear_model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s0      Linear\n(Intercept) 35.74777065 33.99417771\ncyl         -0.83999245 -0.72169272\nhp          -0.01680443 -0.02227636\nwt          -2.89776526 -2.92715539\ndrat         0.36928132  0.73105753\ngear         .           0.16750690\n```\n:::\n:::\n\n\nLos coeficientes del modelo Lasso se han contraído un poco, especialmente de la variable `drat`, y la variable `gear` ha sido automáticamente excluida.\n\n# Dudas y curiosidades\n\nAlgunas preguntas que me surgen para investigar y escribir nuevos posts:\n\n* ¿Cómo elegir el valor de lambda?\n* ¿Qué es el método de *K-fold cross-validation*?\n* ¿Qué diferencia a Lasso de otros modelos similares como Ridge?\n* ¿Qué utilidad tiene la regresión Lasso en el mundo de la salud pública? ¿Qué base de datos puedo usar como ejemplo?\n* ¿Qué artículos hay publicados con esta metodología?\n\n\n# Referencias\n\n* [Statology](https://www.statology.org/lasso-regression-in-r/)\n* [Geeks for Geeks](https://www.geeksforgeeks.org/lasso-regression-in-r-programming/)\n\n",
    "supporting": [
      "post_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}