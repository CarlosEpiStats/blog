{
  "hash": "1ef891cdc67d40f1b7f55a75b07267f8",
  "result": {
    "markdown": "---\ntitle: 'Regresión Lasso'\ndate: \"2024-02-26\"\ncategories: ['regresión', 'R', 'lasso']\ndescription: \"Qué es, para qué se usa y cómo usar la regresión Lasso, con código en R.\"\nexecute: \n  message: false\n  warning: false\neditor_options: \n  chunk_output_type: console\n---\n\n\n# Introducción\n\nEn este post explico qué es la regresión Lasso, para qué se usa y cómo realizarla usando código en R.\n\n\n## Qué es la regresión Lasso\n\nLa regresión Lasso es una variante de la regresión lineal que busca obtener un modelo lo más simple posible. Para ello, penaliza los modelos con coeficientes grandes, dejando coeficientes pequeños e incluso eliminando variables del modelo (igualando sus coeficientes a cero).\n\nLasso es un acrónimo de Least Absolute Shrinkage and Selector Operator.\n\n## Para qué se usa la regresión Lasso\n\nLasso tiene dos funciones principales:\n\n* Selección de variables para un modelo: como puede convertir los coeficientes de las variables en ceros, puede usarse para seleccionar las variables más importantes del modelo y excluir otras. \n* Contracción de parámetros (*shrinkage*): los coeficientes del modelo Lasso son más pequeños que en una regresión lineal normal. Esto ayuda al reducir el peligro de sobreajuste (*overfitting*) del modelo.\n\n## Cuándo se usa la regresión Lasso\n\nLasso se suele utilizar en las siguientes situaciones:\n\n* Cuando tenemos bases de datos con **alta dimensionalidad**(con muchas variables).\n* Cuando tenemos modelos con **multicolinealidad**.\n* Cuando queremos **automatizar la construcción de un modelo** (automatizando la selección de variables).\n\n## Cómo funciona la regresión Lasso\n\nUn **modelo de regresión lineal múltiple** busca el conjunto de coeficientes ($\\beta_0, \\beta_1, \\beta_2...$) que minimiza la suma de los cuadrados de los residuales. Esto es, la distancia entre los datos y la predicción del modelo (residual) se eleva al cuadrado y se suma. El método de mínimos cuadrados de la regresión lineal busca minimizar esta suma.\n\nLa **regresión Lasso añade otro parámetro conocido como L1**. L1 es la suma de los valores absolutos de los coeficientes del modelo. Lasso busca minimizar la suma de L1 y la suma de cuadrados. Por lo tanto, Lasso busca el menor valor absoluto posible de los coeficientes, que se traduce en menor L1. Este proceso se llama \"regularización L1\", y produce una \"contracción\" de los coeficientes (los hace más pequeños).\n\nToda regresión Lasso incluye un parámetro adicional, **lambda** ($\\lambda$). Lambda mide cuánta importancia le da el modelo al parámetro L1:\n\n* Si $\\lambda = 0$, no hay contracción de coeficientes y el modelo es equivalente a una regresión lineal normal.\n* Conforme $\\lambda$ aumenta, hay más contracción de coeficientes y se eliminan más variables.\n* Si $\\lambda$ = infinito, hay contracción máxima y se eliminan todos los coeficientes.\n\n## Fórmula de la regresión Lasso\n\n$min(RSS + \\lambda \\sum |\\beta_j|)$\n\nDonde\n\n* $RSS$ es la suma de residuos cuadrados.\n* $\\lambda$ es el parámetro de penalización de Lasso.\n* $\\sum |\\beta_j|$ es la suma de los valores absolutos de los coeficientes de las variables.\n\n# Código en R\n\n## Preparativos\n\nPara este ejemplo, usaremos la biblioteca `glmnet` y la biblioteca de ejemplo `mtcars`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"glmnet\") # Instalar el paquete\nlibrary(glmnet)\nhead(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n```\n:::\n:::\n\n\nUsaremos `mpg` (millas por galón) como la variable resultado, y `cyl` (nº. de cilindros), `hp` (caballos de vapor), `wt` (peso), `gear` (nº. de marchas) y `drat` (relación del eje) como predictoras. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# definir la variable resultado\ny <- mtcars$mpg\n\n# definir las variables predictoras\nx <- data.matrix(mtcars[, c(\"cyl\", \"hp\", \"wt\", \"drat\", \"gear\")])\n```\n:::\n\n\n\n## Elegir el valor de lambda\n\nPodemos elegir el valor de $\\lambda$ que minimice el error cuadrado medio (*mean-squared error*, MSE). La función `cv.glmnet()` realiza \"validación cruzada de k iteraciones\" ([K-fold cross-validation](https://www.statology.org/k-fold-cross-validation/)) para identificar este valor de $\\lambda$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# validación cruzada\ncv_model <- cv.glmnet(x, y, alpha = 1)  # Cambiar el parámetro alpha da lugar a otros tipos de regresión\n\n# encontrar el valor lambda que minimiza el MSE\nbest_lambda <- cv_model$lambda.min\nbest_lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2621943\n```\n:::\n\n```{.r .cell-code}\n# mostrar los resultados en un gráfico\nplot(cv_model)\n```\n\n::: {.cell-output-display}\n![](post_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nEl valor de lambda que minimiza el MSE resulta ser 0.2621943, que en la gráfica corresponde al punto $Log(\\lambda)$ = -1.3386694.\n\n## Ajustar el modelo\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# coeficientes del modelo \nbest_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)\ncoef(best_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) 35.53832527\ncyl         -0.82713714\nhp          -0.01749224\nwt          -2.91058977\ndrat         0.44492448\ngear         .         \n```\n:::\n:::\n\n\nPodemos observar como el coeficiente de `gear` aparece como un punto, lo que indica que la regresión Lasso ha eliminado el coeficiente, ya que la variable no era lo suficientemente importante.\n\n## Comparación con la regresión lineal sin Lasso\n\nComo comparación, podemos ver los coeficientes que resultarían de un modelo de regresión lineal múltiple sin contracción de parámetros ni selección de variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model <- lm(mpg ~ cyl + hp + wt + drat + gear, data = mtcars)\ncbind(coef(best_model), \"Linear\" = coef(linear_model))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s0      Linear\n(Intercept) 35.53832527 33.99417771\ncyl         -0.82713714 -0.72169272\nhp          -0.01749224 -0.02227636\nwt          -2.91058977 -2.92715539\ndrat         0.44492448  0.73105753\ngear         .           0.16750690\n```\n:::\n:::\n\n\nLos coeficientes del modelo Lasso se han contraído un poco, especialmente de la variable `drat`, y la variable `gear` ha sido automáticamente excluida.\n\n\n# Referencias\n\n* [Statology](https://www.statology.org/lasso-regression-in-r/)\n* [Geeks for Geeks](https://www.geeksforgeeks.org/lasso-regression-in-r-programming/)\n\n\n\n\n",
    "supporting": [
      "post_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}