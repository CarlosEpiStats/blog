[
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archivo",
    "section": "",
    "text": "Series: ggplot2-tips\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nLoading and Exploring Japanese Kanji Data Using R\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\n\n\n\n\n\n\nDiagrama aluvial (alluvial plot)\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n\n\n\n\n\n\nLasso regression\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Series\n\nggplot2-series\nDemo series of ggplot2 tips\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nSeries: ggplot2-tips\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nLoading and Exploring Japanese Kanji Data Using R\n\n\n\n\n\n\n\nR\n\n\ndata cleaning\n\n\nexploratory\n\n\n\n\nUsing R to load, explore, describe, and filter data, with a Japanese Kanji database example.\n\n\n\n\n\n\nFeb 27, 2024\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nDiagrama aluvial (alluvial plot)\n\n\n\n\n\n\n\nggplot\n\n\nplot\n\n\nalluvial\n\n\n\n\nCómo crear un diagrama aluvial en R.\n\n\n\n\n\n\nFeb 22, 2024\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nLasso regression\n\n\n\n\n\n\n\nregression\n\n\nR\n\n\nlasso\n\n\n\n\nWhat is, what is it used for, and how to use Lasso regression, with code in R.\n\n\n\n\n\n\nFeb 6, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EpiStats",
    "section": "",
    "text": "Hello there! I’m Carlos Fernández, MD, MPH, and I’m passionate about biostatistics, epidemiology, and programming.\nHere, I’ll share whatever I learn along the way regarding statistics, scientific methods, and coding in R, Stata, or Python.\nYou can reach me at carlosepistats@gmail.com\nThank you for stopping by!"
  },
  {
    "objectID": "posts/ggplot2-tips/ggplot-series.html",
    "href": "posts/ggplot2-tips/ggplot-series.html",
    "title": "Series: ggplot2-tips",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/new_blog_post/post.html",
    "href": "posts/new_blog_post/post.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/new_blog_post/post.html#merriweather",
    "href": "posts/new_blog_post/post.html#merriweather",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "posts/new_blog_post/post.html#columns",
    "href": "posts/new_blog_post/post.html#columns",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "geom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "posts/new_blog_post/post.html#margin-captions",
    "href": "posts/new_blog_post/post.html#margin-captions",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "ggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/lasso_regression/post.html",
    "href": "posts/lasso_regression/post.html",
    "title": "Lasso regression",
    "section": "",
    "text": "In this post, I explain what Lasso regression is, what it is used for, and how to use it, with code in R.\n\n\nLasso regression is a modified version of linear regression whose objective is to find the simplest model possible. In order to do that, Lasso method penalizes large regression coefficients, leaving smaller coefficients and even removing some variables from the final model (i.e., setting their coefficients to zero).\nLasso is an acronym of Least Absolute Shrinkage and Selector Operator.\n\n\n\nLasso regression is used mainly in two applications:\n\nModel variable selection: Lasso can be used as a method to select the most important variables in a regression model. The least important variables will have their coefficients set to zero, effectively being removed from the final model.\nParameter shrinkage: Lasso’s coefficients are smaller thant those of a simple lineal regression. This helps to avoid overfitting problems.\n\nGiven their two main functions, Lasso regression is usually employed in the following situations:\n\nWhen we have a high-dimensionality dataset, i.e., with a large number of variables.\nWhen we have multicolineallity in our model, i.e., several variables are lineally dependent of one another.\nWhen we want to automatize the model building, via automatizing the selection of the included variables.\n\n\n\n\nA traditional multivariable lineal regression model finds a set of regression coefficients (\\(\\beta_0, \\beta_1, \\beta_2...\\)) that minimizes the residuals’ squared sum (RSS). That is, the distance between the datapoints and the model predictions.\nLasso regression adds another parameter called L1. L1 is defined as the sum of the absolute values of the model coefficients. Lasso method tries to minimize the sum of RSS and L1. As a consequence, Lasso finds a model with smaller regression coefficients. This whole process is known as “L1 regularization”, and it produces a coefficient “shrinkage”.\nEvery time we run a Lasso regression, whe need to specify the lambda parameter (\\(\\lambda\\)). Lambda represents the relative importance of the L1 parameter compared to the RSS part of the minimization formula.\n\nWith \\(\\lambda = 0\\), there is no coefficient shrinkage, and the Lasso model is effectively equal to a regular linear regression model.\nAs \\(\\lambda\\) grows, there is more shrinkage, and more variables are removed from the model.\nIf \\(\\lambda\\) were to be infinite, all coefficients would be removed, and we would end up with an empty model.\n\n\n\n\\(min(RSS + \\lambda \\sum |\\beta_j|)\\)\nWhere\n\n\\(RSS\\) es the residuals’ square sum.\n\\(\\lambda\\) is Lasso’s penalizing factor.\n\\(\\sum |\\beta_j|\\) is the sum of the absolute values of the regression coefficients."
  },
  {
    "objectID": "posts/lasso_regression/post.html#qué-es-la-regresión-lasso",
    "href": "posts/lasso_regression/post.html#qué-es-la-regresión-lasso",
    "title": "Regresión Lasso",
    "section": "",
    "text": "La regresión Lasso es una variante de la regresión lineal que busca obtener un modelo lo más simple posible. Para ello, penaliza los modelos con coeficientes grandes, dejando coeficientes pequeños e incluso eliminando variables del modelo (igualando sus coeficientes a cero).\nLasso es un acrónimo de Least Absolute Shrinkage and Selector Operator."
  },
  {
    "objectID": "posts/lasso_regression/post.html#para-qué-se-usa-la-regresión-lasso",
    "href": "posts/lasso_regression/post.html#para-qué-se-usa-la-regresión-lasso",
    "title": "Regresión Lasso",
    "section": "",
    "text": "Lasso tiene dos funciones principales:\n\nSelección de variables para un modelo: como puede convertir los coeficientes de las variables en ceros, puede usarse para seleccionar las variables más importantes del modelo y excluir otras.\nContracción de parámetros (shrinkage): los coeficientes del modelo Lasso son más pequeños que en una regresión lineal normal. Esto ayuda al reducir el peligro de sobreajuste (overfitting) del modelo."
  },
  {
    "objectID": "posts/lasso_regression/post.html#cuándo-se-usa-la-regresión-lasso",
    "href": "posts/lasso_regression/post.html#cuándo-se-usa-la-regresión-lasso",
    "title": "Regresión Lasso",
    "section": "",
    "text": "Lasso se suele utilizar en las siguientes situaciones:\n\nCuando tenemos bases de datos con alta dimensionalidad(con muchas variables).\nCuando tenemos modelos con multicolinealidad.\nCuando queremos automatizar la construcción de un modelo (automatizando la selección de variables)."
  },
  {
    "objectID": "posts/lasso_regression/post.html#cómo-funciona-la-regresión-lasso",
    "href": "posts/lasso_regression/post.html#cómo-funciona-la-regresión-lasso",
    "title": "Regresión Lasso",
    "section": "",
    "text": "Un modelo de regresión lineal múltiple busca el conjunto de coeficientes (\\(\\beta_0, \\beta_1, \\beta_2...\\)) que minimiza la suma de los cuadrados de los residuales. Esto es, la distancia entre los datos y la predicción del modelo (residual) se eleva al cuadrado y se suma. El método de mínimos cuadrados de la regresión lineal busca minimizar esta suma.\nLa regresión Lasso añade otro parámetro conocido como L1. L1 es la suma de los valores absolutos de los coeficientes del modelo. Lasso busca minimizar la suma de L1 y la suma de cuadrados. Por lo tanto, Lasso busca el menor valor absoluto posible de los coeficientes, que se traduce en menor L1. Este proceso se llama “regularización L1”, y produce una “contracción” de los coeficientes (los hace más pequeños).\nToda regresión Lasso incluye un parámetro adicional, lambda (\\(\\lambda\\)). Lambda mide cuánta importancia le da el modelo al parámetro L1:\n\nSi \\(\\lambda = 0\\), no hay contracción de coeficientes y el modelo es equivalente a una regresión lineal normal.\nConforme \\(\\lambda\\) aumenta, hay más contracción de coeficientes y se eliminan más variables.\nSi \\(\\lambda\\) = infinito, hay contracción máxima y se eliminan todos los coeficientes."
  },
  {
    "objectID": "posts/lasso_regression/post.html#fórmula-de-la-regresión-lasso",
    "href": "posts/lasso_regression/post.html#fórmula-de-la-regresión-lasso",
    "title": "Regresión Lasso",
    "section": "",
    "text": "\\(min(RSS + \\lambda \\sum |\\beta_j|)\\)\nDonde\n\n\\(RSS\\) es la suma de residuos cuadrados.\n\\(\\lambda\\) es el parámetro de penalización de Lasso.\n\\(\\sum |\\beta_j|\\) es la suma de los valores absolutos de los coeficientes de las variables."
  },
  {
    "objectID": "posts/lasso_regression/post.html#preparativos",
    "href": "posts/lasso_regression/post.html#preparativos",
    "title": "Regresión Lasso",
    "section": "Preparativos",
    "text": "Preparativos\nPara este ejemplo, usaremos la biblioteca glmnet y la biblioteca de ejemplo mtcars.\n\n# install.packages(\"glmnet\") # Instalar el paquete\nlibrary(glmnet)\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nUsaremos mpg (millas por galón) como la variable resultado, y cyl (nº. de cilindros), hp (caballos de vapor), wt (peso), gear (nº. de marchas) y drat (relación del eje) como predictoras.\n\n# definir la variable resultado\ny &lt;- mtcars$mpg\n\n# definir las variables predictoras\nx &lt;- data.matrix(mtcars[, c(\"cyl\", \"hp\", \"wt\", \"drat\", \"gear\")])"
  },
  {
    "objectID": "posts/lasso_regression/post.html#elegir-el-valor-de-lambda",
    "href": "posts/lasso_regression/post.html#elegir-el-valor-de-lambda",
    "title": "Regresión Lasso",
    "section": "Elegir el valor de lambda",
    "text": "Elegir el valor de lambda\nPodemos elegir el valor de \\(\\lambda\\) que minimice el error cuadrado medio (mean-squared error, MSE). La función cv.glmnet() realiza “validación cruzada de k iteraciones” (K-fold cross-validation) para identificar este valor de \\(\\lambda\\).\n\n# validación cruzada\ncv_model &lt;- cv.glmnet(x, y, alpha = 1)  # Cambiar el parámetro alpha da lugar a otros tipos de regresión\n\n# encontrar el valor lambda que minimiza el MSE\nbest_lambda &lt;- cv_model$lambda.min\nbest_lambda\n\n[1] 0.2389017\n\n# mostrar los resultados en un gráfico\nplot(cv_model)\n\n\n\n\nEl valor de lambda que minimiza el MSE resulta ser 0.2389017, que en la gráfica corresponde al punto \\(Log(\\lambda)\\) = -1.4317031."
  },
  {
    "objectID": "posts/lasso_regression/post.html#correr-el-modelo",
    "href": "posts/lasso_regression/post.html#correr-el-modelo",
    "title": "Regresión Lasso",
    "section": "Correr el modelo",
    "text": "Correr el modelo\n\n# coeficientes del modelo \nbest_model &lt;- glmnet(x, y, alpha = 1, lambda = best_lambda)\ncoef(best_model)\n\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) 35.74777065\ncyl         -0.83999245\nhp          -0.01680443\nwt          -2.89776526\ndrat         0.36928132\ngear         .         \n\n\nPodemos observar como el coeficiente de gear aparece como un punto, lo que indica que la regresión Lasso ha eliminado el coeficiente, ya que la variable no era lo suficientemente importante."
  },
  {
    "objectID": "posts/lasso_regression/post.html#comparación-con-la-regresión-lineal-sin-lasso",
    "href": "posts/lasso_regression/post.html#comparación-con-la-regresión-lineal-sin-lasso",
    "title": "Lasso regression",
    "section": "Comparación con la regresión lineal sin Lasso",
    "text": "Comparación con la regresión lineal sin Lasso\nComo comparación, podemos ver los coeficientes que resultarían de un modelo de regresión lineal múltiple sin contracción de parámetros ni selección de variables.\n\nlinear_model &lt;- lm(mpg ~ cyl + hp + wt + drat + gear, data = mtcars)\ncbind(coef(best_model), \"Linear\" = coef(linear_model))\n\n6 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s0      Linear\n(Intercept) 35.74777065 33.99417771\ncyl         -0.83999245 -0.72169272\nhp          -0.01680443 -0.02227636\nwt          -2.89776526 -2.92715539\ndrat         0.36928132  0.73105753\ngear         .           0.16750690\n\n\nLos coeficientes del modelo Lasso se han contraído un poco, especialmente de la variable drat, y la variable gear ha sido automáticamente excluida."
  },
  {
    "objectID": "posts/dummy_post/post.html",
    "href": "posts/dummy_post/post.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/dummy_post/post.html#merriweather",
    "href": "posts/dummy_post/post.html#merriweather",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "posts/dummy_post/post.html#columns",
    "href": "posts/dummy_post/post.html#columns",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "geom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "posts/dummy_post/post.html#margin-captions",
    "href": "posts/dummy_post/post.html#margin-captions",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "ggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/lasso_regression/post.html#ajustar-el-modelo",
    "href": "posts/lasso_regression/post.html#ajustar-el-modelo",
    "title": "Lasso regression",
    "section": "Ajustar el modelo",
    "text": "Ajustar el modelo\n\n# coeficientes del modelo \nbest_model &lt;- glmnet(x, y, alpha = 1, lambda = best_lambda)\ncoef(best_model)\n\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) 35.74777065\ncyl         -0.83999245\nhp          -0.01680443\nwt          -2.89776526\ndrat         0.36928132\ngear         .         \n\n\nPodemos observar como el coeficiente de gear aparece como un punto, lo que indica que la regresión Lasso ha eliminado el coeficiente, ya que la variable no era lo suficientemente importante."
  },
  {
    "objectID": "dummy_post/post.html",
    "href": "dummy_post/post.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "dummy_post/post.html#merriweather",
    "href": "dummy_post/post.html#merriweather",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "dummy_post/post.html#columns",
    "href": "dummy_post/post.html#columns",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "geom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "dummy_post/post.html#margin-captions",
    "href": "dummy_post/post.html#margin-captions",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "ggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/alluvial_plot/post.html",
    "href": "posts/alluvial_plot/post.html",
    "title": "Diagrama aluvial (alluvial plot)",
    "section": "",
    "text": "En este post explico cómo crear un diagrama aluvial (alluvial plot) usando código en R.\n\n\nUn diagrama o gráfico aluvial muestra el flujo de información de un estadio al siguiente. El nombre “aluvial” alude a su parecido con el flujo de un río.\n\n\n\nUn diagrama aluvial se suele utilizar en las siguientes situaciones:\n\nPara mostrar el número de participantes de un estudio que cambian desde una categoría basal a una categoría siguiente. Por ejemplo, el número de participantes aleatorizados al principio del estudio y que continúan en seguimiento al final. Es un complemento gráfico a los diagramas de flujo, que presentan la información con flechas y cajas con texto y números.\nPara mostrar la distribución de participantes entre distintas variables categóricas. En este caso es una alternativa más “vistosa” a los diagramas de columnas apilados."
  },
  {
    "objectID": "posts/alluvial_plot/post.html#qué-es-un-diagrama-aluvial",
    "href": "posts/alluvial_plot/post.html#qué-es-un-diagrama-aluvial",
    "title": "Diagrama aluvial (alluvial plot)",
    "section": "",
    "text": "Un diagrama o gráfico aluvial muestra el flujo de información de un estadio al siguiente. El nombre “aluvial” alude a su parecido con el flujo de un río."
  },
  {
    "objectID": "posts/alluvial_plot/post.html#para-qué-se-usan-los-diagramas-aluviales",
    "href": "posts/alluvial_plot/post.html#para-qué-se-usan-los-diagramas-aluviales",
    "title": "Diagrama aluvial (alluvial plot)",
    "section": "",
    "text": "Un diagrama aluvial se suele utilizar en las siguientes situaciones:\n\nPara mostrar el número de participantes de un estudio que cambian desde una categoría basal a una categoría siguiente. Por ejemplo, el número de participantes aleatorizados al principio del estudio y que continúan en seguimiento al final. Es un complemento gráfico a los diagramas de flujo, que presentan la información con flechas y cajas con texto y números.\nPara mostrar la distribución de participantes entre distintas variables categóricas. En este caso es una alternativa más “vistosa” a los diagramas de columnas apilados."
  },
  {
    "objectID": "posts/alluvial_plot/post.html#preparativos",
    "href": "posts/alluvial_plot/post.html#preparativos",
    "title": "Diagrama aluvial (alluvial plot)",
    "section": "Preparativos",
    "text": "Preparativos\nPara este ejemplo, usaremos la biblioteca ggalluvial y ggplot2. Los datos proceden de un estudio sobre aprendizaje de idiomas usando la aplicación web LingQ (Enlace al artículo).\n\n# Cargar las bibliotecas necesarias\nlibrary(tidyverse) # Incluye ggplot2\nlibrary(ggalluvial)\nlibrary(knitr)\n\n# Datos de ejemplo\n\ndata &lt;- data.frame(\n  Inicial = rep(c(\"Primero\", \"Segundo\", \"Tercero\", \"Cuarto+\"), each = 4),\n  Final = rep(c(\"Primero\", \"Segundo\", \"Tercero\", \"Cuarto+\"), 4), \n  Frecuencia = c(28, 28, 10, 4, 3, 6, 7, 5, 0, 3, 3, 3, 0, 0, 0, 1)\n) %&gt;% \n  mutate(\n    Inicial = factor(Inicial, levels = c(\"Primero\", \"Segundo\", \"Tercero\", \"Cuarto+\")),\n    Final = factor(Final, levels = c(\"Primero\", \"Segundo\", \"Tercero\", \"Cuarto+\"))\n  )\n\nhead(data)\n\n  Inicial   Final Frecuencia\n1 Primero Primero         28\n2 Primero Segundo         28\n3 Primero Tercero         10\n4 Primero Cuarto+          4\n5 Segundo Primero          3\n6 Segundo Segundo          6\n\n\nEn este estudio, un total de 192 personas fueron seleccionados para la muestra y completaron el test de conocimientos inicial. De ellas, 101 completaron el test final y estudiaron al menos dos horas en la aplicación, y fueron incluidas en el análisis. Los datos muestran cuál era el conocimiento de idiomas equivalente en semestres de educación formal (“Primero”, “Segundo”, “Tercero”, “Cuarto o más”) al principio del estudio (“Inicial”) y tras usar la aplicación (“Final”).\nNota: Los datos se han extraído del artículo de ejemplo a partir de las tablas de resultados y son solo un ejemplo de una distribución de datos compatible con esos resultados."
  },
  {
    "objectID": "posts/alluvial_plot/post.html#gráfico",
    "href": "posts/alluvial_plot/post.html#gráfico",
    "title": "Diagrama aluvial (alluvial plot)",
    "section": "Gráfico",
    "text": "Gráfico\n\n# Diagrama aluvial\ndata %&gt;% \nggplot(aes(axis1 = fct_rev(Inicial), axis2 = fct_rev(Final),\n           y = Frecuencia)) +\n  scale_x_discrete(limits = c(\"Inicial\", \"Final\"), expand = c(.2, .05)) +\n  geom_alluvium(aes(fill = Final)) +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  theme_minimal() +\n  labs(x = \"Semestres equivalentes de educación formal\")\n\n\n\n\nEn el diagrama aluvial, el área de las zonas coloreadas es proporcional a la frecuencia, por lo que “flujos” más anchos representan más participantes.\nEn el gráfico se aprecia cómo la mayor parte del flujo ocurre de abajo arriba; es decir, el conocimiento tiende a mejorar más que a empeorar. Por ejemplo, más de la mitad de los participantes que partían del primer semestre de conocimientos subieron al segundo, tercer o cuarto semestre.\nTambién se puede ver como casi todos los participantes que acabaron con un nivel de conocimientos equivalente al primer semestre partían de ese nivel previo, y unos pocos bajaron del segundo semestre al primer semestre.\nEn definitiva, los diagramas aluviales proporcionan una opción visual para entender mejor los flujos de datos entre categorías, especialmente si hay algún componente temporal asociado a ellas."
  },
  {
    "objectID": "posts/lasso_regression/post.html#what-is-lasso-regression",
    "href": "posts/lasso_regression/post.html#what-is-lasso-regression",
    "title": "Lasso regression",
    "section": "",
    "text": "Lasso regression is a modified version of linear regression whose objective is to find the simplest model possible. In order to do that, Lasso method penalizes large regression coefficients, leaving smaller coefficients and even removing some variables from the final model (i.e., setting their coefficients to zero).\nLasso is an acronym of Least Absolute Shrinkage and Selector Operator."
  },
  {
    "objectID": "posts/lasso_regression/post.html#what-is-lasso-regression-used-for",
    "href": "posts/lasso_regression/post.html#what-is-lasso-regression-used-for",
    "title": "Lasso regression",
    "section": "",
    "text": "Lasso regression is used mainly in two applications:\n\nModel variable selection: Lasso can be used as a method to select the most important variables in a regression model. The least important variables will have their coefficients set to zero, effectively being removed from the final model.\nParameter shrinkage: Lasso’s coefficients are smaller thant those of a simple lineal regression. This helps to avoid overfitting problems.\n\nGiven their two main functions, Lasso regression is usually employed in the following situations:\n\nWhen we have a high-dimensionality dataset, i.e., with a large number of variables.\nWhen we have multicolineallity in our model, i.e., several variables are lineally dependent of one another.\nWhen we want to automatize the model building, via automatizing the selection of the included variables."
  },
  {
    "objectID": "posts/lasso_regression/post.html#how-does-lasso-regression-work",
    "href": "posts/lasso_regression/post.html#how-does-lasso-regression-work",
    "title": "Lasso regression",
    "section": "",
    "text": "A traditional multivariable lineal regression model finds a set of regression coefficients (\\(\\beta_0, \\beta_1, \\beta_2...\\)) that minimizes the residuals’ squared sum (RSS). That is, the distance between the datapoints and the model predictions.\nLasso regression adds another parameter called L1. L1 is defined as the sum of the absolute values of the model coefficients. Lasso method tries to minimize the sum of RSS and L1. As a consequence, Lasso finds a model with smaller regression coefficients. This whole process is known as “L1 regularization”, and it produces a coefficient “shrinkage”.\nEvery time we run a Lasso regression, whe need to specify the lambda parameter (\\(\\lambda\\)). Lambda represents the relative importance of the L1 parameter compared to the RSS part of the minimization formula.\n\nWith \\(\\lambda = 0\\), there is no coefficient shrinkage, and the Lasso model is effectively equal to a regular linear regression model.\nAs \\(\\lambda\\) grows, there is more shrinkage, and more variables are removed from the model.\nIf \\(\\lambda\\) were to be infinite, all coefficients would be removed, and we would end up with an empty model.\n\n\n\n\\(min(RSS + \\lambda \\sum |\\beta_j|)\\)\nWhere\n\n\\(RSS\\) es the residuals’ square sum.\n\\(\\lambda\\) is Lasso’s penalizing factor.\n\\(\\sum |\\beta_j|\\) is the sum of the absolute values of the regression coefficients."
  },
  {
    "objectID": "posts/lasso_regression/post.html#getting-ready",
    "href": "posts/lasso_regression/post.html#getting-ready",
    "title": "Lasso regression",
    "section": "Getting ready",
    "text": "Getting ready\nIn this example, we’ll use the glmnet library and the example dataset in mtcars.\n\n# install.packages(\"glmnet\") # Install the package (only once)\nlibrary(glmnet)\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nWe’ll use mpg (miles per galon) as the outcome variable, and cyl (number of cylinders), hp (horsepower), wt (weight), gear (gear number) y drat (axis relation?) as predictive variables.\n\n# Define the outcome variable\ny &lt;- mtcars$mpg\n\n# Define the predictive variables\nx &lt;- data.matrix(mtcars[, c(\"cyl\", \"hp\", \"wt\", \"drat\", \"gear\")])"
  },
  {
    "objectID": "posts/lasso_regression/post.html#choose-a-value-for-lambda",
    "href": "posts/lasso_regression/post.html#choose-a-value-for-lambda",
    "title": "Lasso regression",
    "section": "Choose a value for lambda",
    "text": "Choose a value for lambda\nPodemos elegir el valor de \\(\\lambda\\) que minimice el error cuadrado medio (mean-squared error, MSE). La función cv.glmnet() realiza “validación cruzada de k iteraciones” (K-fold cross-validation) para identificar este valor de \\(\\lambda\\).\n\n# validación cruzada\ncv_model &lt;- cv.glmnet(x, y, alpha = 1)  # Cambiar el parámetro alpha da lugar a otros tipos de regresión\n\n# encontrar el valor lambda que minimiza el MSE\nbest_lambda &lt;- cv_model$lambda.min\nbest_lambda\n\n[1] 0.315814\n\n# mostrar los resultados en un gráfico\nplot(cv_model)\n\n\n\n\nEl valor de lambda que minimiza el MSE resulta ser 0.315814, que en la gráfica corresponde al punto \\(Log(\\lambda)\\) = -1.1526019."
  },
  {
    "objectID": "posts/kanji/post.html",
    "href": "posts/kanji/post.html",
    "title": "Loading and Exploring Japanese Kanji Data Using R",
    "section": "",
    "text": "Introduction\nIn this blog post, I’ll demonstrate how to use R to load, explore, and filter data from a dataset containing Japanese characters, known as “kanji”. The datasets were obtained from an online Kanji database. We’ll focus on using the tidyverse family of packages to illustrate how to select and filter relevant information efficiently.\n\n\nSetup and Loading\nTo begin, we need to load necessary libraries and import the datasets:\n\n# | warning: false\n# Loading necessary libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\n\n# Loading datasets\n\ndata_kanji &lt;- read.csv2(here(\"data/kanji\", \"Kanji_20240227_081842.csv\")) %&gt;% \n  clean_names()\n\ndata_jukugo &lt;- read.csv2(here(\"data/kanji\", \"Jukugo_20240227_081908.csv\")) %&gt;% \n  clean_names()\n\nHere’s a breakdown of the code:\n\nlibrary(tidyverse): We load the tidyverse package, which includes dplyr, ggplot2, and other useful packages.\nlibrary(here): This package helps manage file paths conveniently.\nlibrary(janitor): Useful for standardizing variable names and data cleaning.\nWe use read.csv2() to import CSV (comma-separated value) files with semicolons (;) as separators.\nhere(\"data/kanji\", \"Kanji_20240227_081842.csv\") uses the function here() to access the data file, which is saved inside the folders data &gt; kanji.\nThe characters %&gt;% are called a “pipe” in tidyverse. It can be written simply by pressing Ctrl + Shift + M (in Windows). Basically, it tells R that we want to apply some step to the previous data. In this example, I tell R that I want to use the function clean_names() to the data that I’ve already loaded using read.csv2().\nclean_names() is a janitor function that renames all variables in a standard format to make it easier to manipulate. Specifically, clean_names() sets all names to lowercase, removes punctuation and symbols, and replaces spaces with underscores.\n\nNow I have two separate datasets: one for kanji (single characters), and one for jukugo (compound words). Let’s take a look at them.\n\n\nExploring the data\nLet’s examine the first few rows of each dataset:\n\nhead(data_kanji)\n\n    id kanji strokes grade\n1   41    一       1     1\n2  124    乙       1     7\n3 2060    了       2     7\n4 2074    力       2     1\n5 1577    二       2     1\n6 1070    人       2     1\n\nhead(data_jukugo)\n\n   id comp_word frequency          grammatical_feature pronunciation\n1 173      一部     46289 possible to use as an adverb         itibu\n2 234      一般     39274                 general noun         ippan\n3 432      一時     25126 possible to use as an adverb         itizi\n4 461      一番     24155 possible to use as an adverb        itiban\n5 481      一緒     23453    light-verb -suru attached         issyo\n6 529      一致     21388    light-verb -suru attached          itti\n  english_translation position kanji kanji_id\n1            one part        L    一       41\n2             general        L    一       41\n3         one o'clock        L    一       41\n4                best        L    一       41\n5            together        L    一       41\n6         coincidence        L    一       41\n\n\nWe’re using the base function head()to show the first rows or observations of our datasets.\nWe can see that data_kanji has four columns or variables:\n\nid shows a unique identification number.\nkanji stores the actual character.\nstrokes represents the number of distinct lines or strokes that the character has.\ngrade means the official categorization of Kanji by educational year in Japan. Grade 1 includes the easiest or most common kanji, and it goes all up to grade 7.\n\nOn the other hand, data_jukugo contains nine variables:\n\nid is the identification number for jukugos.\ncomp_word is the actual word.\nfrequency is a measure of how many times each jukugo appear in a selected corpus of Japanese literature (extracted from Japanese newspapers).\ngrammatical_feature gives us more context of how the word is used in grammatical terms.\npronunciation tells us the pronunciation in “romaji”, or the Latin alphabet.\nenglish_translation stores the English translation.\n\nThe last three variables in data_jukugo describes the kanji which is part of the jukugo:\n\nposition tells us if the kanji is used in left position “L” or right position “R”.\nkanji shows the kanji used in the jukugo. The first rows all show jukugos composed with the kanji “一”.\nkanji_id is the identification number of the kanji part. We can use this id to link data_jukugo with data_kanji if we want to.\n\nAnother way of looking into a dataset is to explore how each variable is encoded:\n\nglimpse(data_kanji)\n\nRows: 2,136\nColumns: 4\n$ id      &lt;int&gt; 41, 124, 2060, 2074, 1577, 1070, 1584, 829, 359, 1647, 1903, 1…\n$ kanji   &lt;chr&gt; \"一\", \"乙\", \"了\", \"力\", \"二\", \"人\", \"入\", \"七\", \"九\", \"八\", \"…\n$ strokes &lt;int&gt; 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3,…\n$ grade   &lt;int&gt; 1, 7, 7, 1, 1, 1, 1, 1, 1, 1, 7, 3, 1, 2, 1, 2, 6, 7, 6, 1, 2,…\n\nglimpse(data_jukugo)\n\nRows: 52,791\nColumns: 9\n$ id                  &lt;int&gt; 173, 234, 432, 461, 481, 529, 937, 1465, 1521, 156…\n$ comp_word           &lt;chr&gt; \"一部\", \"一般\", \"一時\", \"一番\", \"一緒\", \"一致\", \"…\n$ frequency           &lt;int&gt; 46289, 39274, 25126, 24155, 23453, 21388, 12477, 7…\n$ grammatical_feature &lt;chr&gt; \"possible to use as an adverb\", \"general noun\", \"p…\n$ pronunciation       &lt;chr&gt; \"itibu\", \"ippan\", \"itizi\", \"itiban\", \"issyo\", \"itt…\n$ english_translation &lt;chr&gt; \"one part\", \"general\", \"one o'clock\", \"best\", \"tog…\n$ position            &lt;chr&gt; \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", …\n$ kanji               &lt;chr&gt; \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"…\n$ kanji_id            &lt;int&gt; 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41…\n\n\nThe glimpse() function allows us to quickly glance at the data structure.\nWe can see that data_kanji has 2,136 rows or observations and 4 columns or variables. We also see the first values of each of its four variables. More importantly, we can see which data type each variable stores. The kanji variable has &lt;chr&gt; type, which means “character” or “text”, while the rest of variables have &lt;int&gt; type, which means “integer” number, or a round number. R automatically detects the data types when importing data using functions like read.csv2().\nRegarding data_jukugo, it has 52,791 rows and 9 columns, of which 3 have &lt;int&gt; type, and 6 have &lt;char&gt; type.\n\n\nManipulating the data\nNow that I’m familiarized with this dataset, it’s useful to lay down what my analysis plan is. In other words, what do I want to learn from this data? In this case, I want to be able to find words (jukugo) that only contain kanji from a selected list of kanji that I’m learning. So, for example, if I only know kanjis 一, 人, and 十, I want to know all the possible combinations of these three kanjis.\nFor this exercise, I’m interested in separate jukugos in two parts: the left kanji, and the right kanji. The dataset already has half of this information, but sometimes it tells us the left kanji, and sometimes the right kanji (usually, the simplest kanji of the two). I want to get sistematically both left and right kanjis, so I’ll create new variables called kanji_left and kanji_right.\n\ndata_jukugo &lt;- data_jukugo %&gt;% \n  mutate(kanji_left = substr(comp_word, 1, 1),\n         kanji_right = substr(comp_word, 2, 2))\n\nglimpse(data_jukugo)\n\nRows: 52,791\nColumns: 11\n$ id                  &lt;int&gt; 173, 234, 432, 461, 481, 529, 937, 1465, 1521, 156…\n$ comp_word           &lt;chr&gt; \"一部\", \"一般\", \"一時\", \"一番\", \"一緒\", \"一致\", \"…\n$ frequency           &lt;int&gt; 46289, 39274, 25126, 24155, 23453, 21388, 12477, 7…\n$ grammatical_feature &lt;chr&gt; \"possible to use as an adverb\", \"general noun\", \"p…\n$ pronunciation       &lt;chr&gt; \"itibu\", \"ippan\", \"itizi\", \"itiban\", \"issyo\", \"itt…\n$ english_translation &lt;chr&gt; \"one part\", \"general\", \"one o'clock\", \"best\", \"tog…\n$ position            &lt;chr&gt; \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", …\n$ kanji               &lt;chr&gt; \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"…\n$ kanji_id            &lt;int&gt; 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41…\n$ kanji_left          &lt;chr&gt; \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"…\n$ kanji_right         &lt;chr&gt; \"部\", \"般\", \"時\", \"番\", \"緒\", \"致\", \"定\", \"連\", \"…\n\n\nLet’s explain the code:\n\nmutate() is the dplyr function used to create or change variables. Here, I create two variables, kanji_left and kanji_right.\nsubstr() is a base function that subtracts a string of text from a character variable. substr(comp_word, 1, 1) means to subtract only the first character, and substr(comp_word, 2, 2) gets the second character.\n\nAlright, now I need to define the list of kanjis that I’m currently learning. This I need to do it manually, but later I’ll explain how to do it more dinamically.\n\nkanji_learning &lt;- c(\"一\", \"二\", \"三\", \"王\", \"玉\", \"十\", \"五\")\n\nLastly, I’ll tell R to filter the jukugos that only include kanji that are on my learning list. I also want to sort the jukugos from more to less used.\n\njukugo_learning &lt;- data_jukugo %&gt;% \n  filter(kanji_left %in% kanji_learning, kanji_right %in% kanji_learning) %&gt;% \n  arrange(desc(frequency))\n\nhead(jukugo_learning)\n\n     id comp_word frequency          grammatical_feature pronunciation\n1 17059      二三        32 possible to use as an adverb         nisan\n2 17059      二三        32 possible to use as an adverb         nisan\n3 20330      三一        12                 general noun        sanpin\n4 20330      三一        12                 general noun        sanpin\n5 23443      一一         4 possible to use as an adverb        itiiti\n6 23443      一一         4 possible to use as an adverb        itiiti\n  english_translation position kanji kanji_id kanji_left kanji_right\n1        two or three        L    二     1577         二          三\n2        two or three        R    三      744         二          三\n3 low-ranking samurai        R    一       41         三          一\n4 low-ranking samurai        L    三      744         三          一\n5          one-by-one        L    一       41         一          一\n6          one-by-one        R    一       41         一          一\n\n\nThe filter() function selects rows based on one or more conditions. I’ve passed two conditions: that kanji_left is included in the kanji_learning “list” (in R we’d call this a vector, not a list), and that kanji_right is also included in kanji_learning. The term “is included in” is represented in R with the operand %in%.\nThe arrange() function reorders the rows based on one or more variables. I’ve passed the argument desc(frequency) because I want the words to be sorted in descending order of frequency (from more to less frequency).\nNow, something odd has happened: now we have two copies of each jukugo. These are complete duplicates in the dataset, with the only difference of which kanji appears in the variables position, kanji, and kanji_id. For example, “nisan” (二三) appears twice, one with position L, kanji 二, and kanji_id 1577, and another with position R, kanji 三, and kanji_id 744. This is somethin that I didn’t see first time I explored the dataset.\nI could have done things differently. Instead of splitting the jukugos manually, I could have performed a “self-join” of the duplicated rows. But one cool thing about data cleaning and analysis is that there are always different ways to reach the same goal. It’s an iterative process, and by trial and error I can learn a lot and find alternative methods of doing things.\nMoving forward, since I’m only interested in keeping one record of each jukugo, I can drop these duplicates. Aditionally, I’ll keep only the variables I’m interested in.\n\njukugo_learning &lt;- jukugo_learning %&gt;% \n  select(id, comp_word, frequency, grammatical_feature, pronunciation, english_translation, kanji_left, kanji_right) %&gt;%\n  distinct()\n\nhead(jukugo_learning)\n\n     id comp_word frequency          grammatical_feature pronunciation\n1 17059      二三        32 possible to use as an adverb         nisan\n2 20330      三一        12                 general noun        sanpin\n3 23443      一一         4 possible to use as an adverb        itiiti\n4 25773      二王         2                 general noun          nioo\n          english_translation kanji_left kanji_right\n1                two or three         二          三\n2         low-ranking samurai         三          一\n3                  one-by-one         一          一\n4 the two guardian Deva kings         二          王\n\n\nI’ve used two new dplyr functions: select() keeps some columns or variables, and distinct() keeps only non-duplicated rows.\nThe final result contains four distinct jukugos: 二三, 三一, 一一, and 二王. All of them are very low-frequency, with the most common of them appearing only 32 times.\n\n\nNext step: making it interactive\nSo far, I have created a code that filters Japanese kanji words based on whatever Kanji components I want. However, the whole process would be nicer if I had a way of selecting the data interactivelly, maybe pressing some buttons. We can do just that using R Shiny applications. But that’s for another day!\n\n\nReferences\n\nKanji database."
  }
]