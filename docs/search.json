[
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archivo",
    "section": "",
    "text": "Mastering Regular Expressions: Dealing with String Data in R, Part II\n\n\n\n\n\n\n\n\n\nJun 21, 2024\n\n\n\n\n\n\n\n\nMastering Regular Expressions: Dealing with String Data in R, Part I\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\n\n\n\n\n\n\nExploring the count() Function in R’s tidyverse\n\n\n\n\n\n\n\n\n\nJun 3, 2024\n\n\n\n\n\n\n\n\nUseful RStudio Shortcuts\n\n\n\n\n\n\n\n\n\nMay 25, 2024\n\n\n\n\n\n\n\n\nCOVID-19 Outbreak Report: An Example Using R\n\n\n\n\n\n\n\n\n\nMar 12, 2024\n\n\n\n\n\n\n\n\nHow to Create an Alluvial Plot in Stata\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\n\n\n\n\n\n\nInteractive List of Japanese Words Using R Shiny Apps\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\n\n\n\n\n\n\nLoading and Exploring Japanese Kanji Data Using R\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\n\n\n\n\n\n\nHow to Create an Alluvial Plot in R\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n\n\n\n\n\n\nLasso Regression\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nMastering Regular Expressions: Dealing with String Data in R, Part II\n\n\n\n\n\n\n\nR\n\n\ntidyverse\n\n\nregex\n\n\nstring\n\n\ntext\n\n\n\n\nSolving R for Data Science (2ed) Exercises\n\n\n\n\n\n\nJun 21, 2024\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nMastering Regular Expressions: Dealing with String Data in R, Part I\n\n\n\n\n\n\n\nR\n\n\ntidyverse\n\n\nregex\n\n\nstring\n\n\ntext\n\n\n\n\nSolving R for Data Science (2ed) Exercises\n\n\n\n\n\n\nJun 19, 2024\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nExploring the count() Function in R’s tidyverse\n\n\n\n\n\n\n\nR\n\n\ntidyverse\n\n\nregex\n\n\n\n\nTips and Tricks in Data Cleaning and Visualization, Part III.\n\n\n\n\n\n\nJun 3, 2024\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nUseful RStudio Shortcuts\n\n\n\n\n\n\n\nR\n\n\nggplot\n\n\ntidyverse\n\n\n\n\nTips and Tricks in Data Cleaning and Visualization, Part I.\n\n\n\n\n\n\nMay 25, 2024\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nHow to Create an Alluvial Plot in Stata\n\n\n\n\n\n\n\nStata\n\n\nplot\n\n\nalluvial\n\n\n\n\nHow to draw an Alluvial Plot in Stata with example code.\n\n\n\n\n\n\nMar 11, 2024\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nInteractive List of Japanese Words Using R Shiny Apps\n\n\n\n\n\n\n\nR\n\n\nshiny\n\n\nexploratory\n\n\n\n\nUsing R Shiny to interactively explore data, with a Japanese Kanji database example.\n\n\n\n\n\n\nMar 6, 2024\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nLoading and Exploring Japanese Kanji Data Using R\n\n\n\n\n\n\n\nR\n\n\ndata cleaning\n\n\nexploratory\n\n\n\n\nUsing R to load, explore, describe, and filter data, with a Japanese Kanji database example.\n\n\n\n\n\n\nFeb 27, 2024\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nHow to Create an Alluvial Plot in R\n\n\n\n\n\n\n\nggplot\n\n\nplot\n\n\nalluvial\n\n\nR\n\n\n\n\nHow to draw an Alluvial Plot in R with example code.\n\n\n\n\n\n\nFeb 22, 2024\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nLasso Regression\n\n\n\n\n\n\n\nregression\n\n\nR\n\n\nlasso\n\n\n\n\nWhat is, what is it used for, and how to use Lasso regression, with code in R.\n\n\n\n\n\n\nFeb 6, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EpiStats",
    "section": "",
    "text": "Hello there! I’m Carlos Fernández-Escobar, MD, MPH, and I’m passionate about biostatistics, epidemiology, and programming.\nHere, I’ll share whatever I learn along the way regarding statistics, scientific methods, and coding in R, Stata, or Python.\nYou can reach me at carlosepistats@gmail.com\nThank you for stopping by!"
  },
  {
    "objectID": "posts/ggplot2-tips/ggplot-series.html",
    "href": "posts/ggplot2-tips/ggplot-series.html",
    "title": "Series: ggplot2-tips",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/new_blog_post/post.html",
    "href": "posts/new_blog_post/post.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/new_blog_post/post.html#merriweather",
    "href": "posts/new_blog_post/post.html#merriweather",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "posts/new_blog_post/post.html#columns",
    "href": "posts/new_blog_post/post.html#columns",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "geom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "posts/new_blog_post/post.html#margin-captions",
    "href": "posts/new_blog_post/post.html#margin-captions",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "ggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/lasso_regression/post.html",
    "href": "posts/lasso_regression/post.html",
    "title": "Lasso Regression",
    "section": "",
    "text": "In this post, I explain what Lasso regression is, what it is used for, and how to use it, with code in R.\n\n\nLasso regression is a modified version of linear regression whose objective is to find the simplest model possible. In order to do that, Lasso method penalizes large regression coefficients, leaving smaller coefficients and even removing some variables from the final model (i.e., setting their coefficients to zero).\nLasso is an acronym of Least Absolute Shrinkage and Selector Operator.\n\n\n\nLasso regression is used mainly in two applications:\n\nModel variable selection: Lasso can be used as a method to select the most important variables in a regression model. The least important variables will have their coefficients set to zero, effectively being removed from the final model.\nParameter shrinkage: Lasso’s coefficients are smaller thant those of a simple lineal regression. This helps to avoid overfitting problems.\n\nGiven their two main functions, Lasso regression is usually employed in the following situations:\n\nWhen we have a high-dimensionality dataset, i.e., with a large number of variables.\nWhen we have multicolineallity in our model, i.e., several variables are lineally dependent of one another.\nWhen we want to automatize the model building, via automatizing the selection of the included variables.\n\n\n\n\nA traditional multivariable lineal regression model finds a set of regression coefficients (\\(\\beta_0, \\beta_1, \\beta_2...\\)) that minimizes the residuals’ squared sum (RSS). That is, the distance between the datapoints and the model predictions.\nLasso regression adds another parameter called L1. L1 is defined as the sum of the absolute values of the model coefficients. Lasso method tries to minimize the sum of RSS and L1. As a consequence, Lasso finds a model with smaller regression coefficients. This whole process is known as “L1 regularization”, and it produces a coefficient “shrinkage”.\nEvery time we run a Lasso regression, whe need to specify the lambda parameter (\\(\\lambda\\)). Lambda represents the relative importance of the L1 parameter compared to the RSS part of the minimization formula.\n\nWith \\(\\lambda = 0\\), there is no coefficient shrinkage, and the Lasso model is effectively equal to a regular linear regression model.\nAs \\(\\lambda\\) grows, there is more shrinkage, and more variables are removed from the model.\nIf \\(\\lambda\\) were to be infinite, all coefficients would be removed, and we would end up with an empty model.\n\n\n\n\\(min(RSS + \\lambda \\sum |\\beta_j|)\\)\nWhere\n\n\\(RSS\\) is the residuals’ square sum.\n\\(\\lambda\\) is Lasso’s penalizing factor.\n\\(\\sum |\\beta_j|\\) is the sum of the absolute values of the regression coefficients."
  },
  {
    "objectID": "posts/lasso_regression/post.html#qué-es-la-regresión-lasso",
    "href": "posts/lasso_regression/post.html#qué-es-la-regresión-lasso",
    "title": "Regresión Lasso",
    "section": "",
    "text": "La regresión Lasso es una variante de la regresión lineal que busca obtener un modelo lo más simple posible. Para ello, penaliza los modelos con coeficientes grandes, dejando coeficientes pequeños e incluso eliminando variables del modelo (igualando sus coeficientes a cero).\nLasso es un acrónimo de Least Absolute Shrinkage and Selector Operator."
  },
  {
    "objectID": "posts/lasso_regression/post.html#para-qué-se-usa-la-regresión-lasso",
    "href": "posts/lasso_regression/post.html#para-qué-se-usa-la-regresión-lasso",
    "title": "Regresión Lasso",
    "section": "",
    "text": "Lasso tiene dos funciones principales:\n\nSelección de variables para un modelo: como puede convertir los coeficientes de las variables en ceros, puede usarse para seleccionar las variables más importantes del modelo y excluir otras.\nContracción de parámetros (shrinkage): los coeficientes del modelo Lasso son más pequeños que en una regresión lineal normal. Esto ayuda al reducir el peligro de sobreajuste (overfitting) del modelo."
  },
  {
    "objectID": "posts/lasso_regression/post.html#cuándo-se-usa-la-regresión-lasso",
    "href": "posts/lasso_regression/post.html#cuándo-se-usa-la-regresión-lasso",
    "title": "Regresión Lasso",
    "section": "",
    "text": "Lasso se suele utilizar en las siguientes situaciones:\n\nCuando tenemos bases de datos con alta dimensionalidad(con muchas variables).\nCuando tenemos modelos con multicolinealidad.\nCuando queremos automatizar la construcción de un modelo (automatizando la selección de variables)."
  },
  {
    "objectID": "posts/lasso_regression/post.html#cómo-funciona-la-regresión-lasso",
    "href": "posts/lasso_regression/post.html#cómo-funciona-la-regresión-lasso",
    "title": "Regresión Lasso",
    "section": "",
    "text": "Un modelo de regresión lineal múltiple busca el conjunto de coeficientes (\\(\\beta_0, \\beta_1, \\beta_2...\\)) que minimiza la suma de los cuadrados de los residuales. Esto es, la distancia entre los datos y la predicción del modelo (residual) se eleva al cuadrado y se suma. El método de mínimos cuadrados de la regresión lineal busca minimizar esta suma.\nLa regresión Lasso añade otro parámetro conocido como L1. L1 es la suma de los valores absolutos de los coeficientes del modelo. Lasso busca minimizar la suma de L1 y la suma de cuadrados. Por lo tanto, Lasso busca el menor valor absoluto posible de los coeficientes, que se traduce en menor L1. Este proceso se llama “regularización L1”, y produce una “contracción” de los coeficientes (los hace más pequeños).\nToda regresión Lasso incluye un parámetro adicional, lambda (\\(\\lambda\\)). Lambda mide cuánta importancia le da el modelo al parámetro L1:\n\nSi \\(\\lambda = 0\\), no hay contracción de coeficientes y el modelo es equivalente a una regresión lineal normal.\nConforme \\(\\lambda\\) aumenta, hay más contracción de coeficientes y se eliminan más variables.\nSi \\(\\lambda\\) = infinito, hay contracción máxima y se eliminan todos los coeficientes."
  },
  {
    "objectID": "posts/lasso_regression/post.html#fórmula-de-la-regresión-lasso",
    "href": "posts/lasso_regression/post.html#fórmula-de-la-regresión-lasso",
    "title": "Regresión Lasso",
    "section": "",
    "text": "\\(min(RSS + \\lambda \\sum |\\beta_j|)\\)\nDonde\n\n\\(RSS\\) es la suma de residuos cuadrados.\n\\(\\lambda\\) es el parámetro de penalización de Lasso.\n\\(\\sum |\\beta_j|\\) es la suma de los valores absolutos de los coeficientes de las variables."
  },
  {
    "objectID": "posts/lasso_regression/post.html#preparativos",
    "href": "posts/lasso_regression/post.html#preparativos",
    "title": "Regresión Lasso",
    "section": "Preparativos",
    "text": "Preparativos\nPara este ejemplo, usaremos la biblioteca glmnet y la biblioteca de ejemplo mtcars.\n\n# install.packages(\"glmnet\") # Instalar el paquete\nlibrary(glmnet)\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nUsaremos mpg (millas por galón) como la variable resultado, y cyl (nº. de cilindros), hp (caballos de vapor), wt (peso), gear (nº. de marchas) y drat (relación del eje) como predictoras.\n\n# definir la variable resultado\ny &lt;- mtcars$mpg\n\n# definir las variables predictoras\nx &lt;- data.matrix(mtcars[, c(\"cyl\", \"hp\", \"wt\", \"drat\", \"gear\")])"
  },
  {
    "objectID": "posts/lasso_regression/post.html#elegir-el-valor-de-lambda",
    "href": "posts/lasso_regression/post.html#elegir-el-valor-de-lambda",
    "title": "Regresión Lasso",
    "section": "Elegir el valor de lambda",
    "text": "Elegir el valor de lambda\nPodemos elegir el valor de \\(\\lambda\\) que minimice el error cuadrado medio (mean-squared error, MSE). La función cv.glmnet() realiza “validación cruzada de k iteraciones” (K-fold cross-validation) para identificar este valor de \\(\\lambda\\).\n\n# validación cruzada\ncv_model &lt;- cv.glmnet(x, y, alpha = 1)  # Cambiar el parámetro alpha da lugar a otros tipos de regresión\n\n# encontrar el valor lambda que minimiza el MSE\nbest_lambda &lt;- cv_model$lambda.min\nbest_lambda\n\n[1] 0.2389017\n\n# mostrar los resultados en un gráfico\nplot(cv_model)\n\n\n\n\nEl valor de lambda que minimiza el MSE resulta ser 0.2389017, que en la gráfica corresponde al punto \\(Log(\\lambda)\\) = -1.4317031."
  },
  {
    "objectID": "posts/lasso_regression/post.html#correr-el-modelo",
    "href": "posts/lasso_regression/post.html#correr-el-modelo",
    "title": "Regresión Lasso",
    "section": "Correr el modelo",
    "text": "Correr el modelo\n\n# coeficientes del modelo \nbest_model &lt;- glmnet(x, y, alpha = 1, lambda = best_lambda)\ncoef(best_model)\n\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) 35.74777065\ncyl         -0.83999245\nhp          -0.01680443\nwt          -2.89776526\ndrat         0.36928132\ngear         .         \n\n\nPodemos observar como el coeficiente de gear aparece como un punto, lo que indica que la regresión Lasso ha eliminado el coeficiente, ya que la variable no era lo suficientemente importante."
  },
  {
    "objectID": "posts/lasso_regression/post.html#comparación-con-la-regresión-lineal-sin-lasso",
    "href": "posts/lasso_regression/post.html#comparación-con-la-regresión-lineal-sin-lasso",
    "title": "Lasso regression",
    "section": "Comparación con la regresión lineal sin Lasso",
    "text": "Comparación con la regresión lineal sin Lasso\nComo comparación, podemos ver los coeficientes que resultarían de un modelo de regresión lineal múltiple sin contracción de parámetros ni selección de variables.\n\nlinear_model &lt;- lm(mpg ~ cyl + hp + wt + drat + gear, data = mtcars)\ncbind(coef(best_model), \"Linear\" = coef(linear_model))\n\n6 x 2 sparse Matrix of class \"dgCMatrix\"\n                     s0      Linear\n(Intercept) 35.74777065 33.99417771\ncyl         -0.83999245 -0.72169272\nhp          -0.01680443 -0.02227636\nwt          -2.89776526 -2.92715539\ndrat         0.36928132  0.73105753\ngear         .           0.16750690\n\n\nLos coeficientes del modelo Lasso se han contraído un poco, especialmente de la variable drat, y la variable gear ha sido automáticamente excluida."
  },
  {
    "objectID": "posts/dummy_post/post.html",
    "href": "posts/dummy_post/post.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/dummy_post/post.html#merriweather",
    "href": "posts/dummy_post/post.html#merriweather",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "posts/dummy_post/post.html#columns",
    "href": "posts/dummy_post/post.html#columns",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "geom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "posts/dummy_post/post.html#margin-captions",
    "href": "posts/dummy_post/post.html#margin-captions",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "ggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/lasso_regression/post.html#ajustar-el-modelo",
    "href": "posts/lasso_regression/post.html#ajustar-el-modelo",
    "title": "Lasso regression",
    "section": "Ajustar el modelo",
    "text": "Ajustar el modelo\n\n# coeficientes del modelo \nbest_model &lt;- glmnet(x, y, alpha = 1, lambda = best_lambda)\ncoef(best_model)\n\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) 35.74777065\ncyl         -0.83999245\nhp          -0.01680443\nwt          -2.89776526\ndrat         0.36928132\ngear         .         \n\n\nPodemos observar como el coeficiente de gear aparece como un punto, lo que indica que la regresión Lasso ha eliminado el coeficiente, ya que la variable no era lo suficientemente importante."
  },
  {
    "objectID": "dummy_post/post.html",
    "href": "dummy_post/post.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "dummy_post/post.html#merriweather",
    "href": "dummy_post/post.html#merriweather",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "dummy_post/post.html#columns",
    "href": "dummy_post/post.html#columns",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "geom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "dummy_post/post.html#margin-captions",
    "href": "dummy_post/post.html#margin-captions",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "ggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/alluvial_plot/post.html",
    "href": "posts/alluvial_plot/post.html",
    "title": "How to Create an Alluvial Plot in R",
    "section": "",
    "text": "In this post, I explain how to create an alluvial plot using code in R\n\n\nAn alluvial diagram or plot displays the flow of information from one categorical variable or stage to the next. The term “alluvial” refers to its resemblance to the flow of a river.\n\n\n\nAlluvial diagrams are commonly used in the following situations:\n\nTo demonstrate the number of participants in a study transitioning from one baseline category to a subsequent category. For example, the number of participants randomized at the beginning of the study and who continue to follow-up at the end. It serves as a graphical supplement to flowcharts, which present information with arrows and boxes containing text and numbers.\nTo show the distribution of participants among different categorical variables. In this case, it provides a more “visually appealing” alternative to stacked column charts."
  },
  {
    "objectID": "posts/alluvial_plot/post.html#qué-es-un-diagrama-aluvial",
    "href": "posts/alluvial_plot/post.html#qué-es-un-diagrama-aluvial",
    "title": "Diagrama aluvial (alluvial plot)",
    "section": "",
    "text": "Un diagrama o gráfico aluvial muestra el flujo de información de un estadio al siguiente. El nombre “aluvial” alude a su parecido con el flujo de un río."
  },
  {
    "objectID": "posts/alluvial_plot/post.html#para-qué-se-usan-los-diagramas-aluviales",
    "href": "posts/alluvial_plot/post.html#para-qué-se-usan-los-diagramas-aluviales",
    "title": "Diagrama aluvial (alluvial plot)",
    "section": "",
    "text": "Un diagrama aluvial se suele utilizar en las siguientes situaciones:\n\nPara mostrar el número de participantes de un estudio que cambian desde una categoría basal a una categoría siguiente. Por ejemplo, el número de participantes aleatorizados al principio del estudio y que continúan en seguimiento al final. Es un complemento gráfico a los diagramas de flujo, que presentan la información con flechas y cajas con texto y números.\nPara mostrar la distribución de participantes entre distintas variables categóricas. En este caso es una alternativa más “vistosa” a los diagramas de columnas apilados."
  },
  {
    "objectID": "posts/alluvial_plot/post.html#preparativos",
    "href": "posts/alluvial_plot/post.html#preparativos",
    "title": "How to Create an Alluvial Plot in R",
    "section": "Preparativos",
    "text": "Preparativos\nFor this example, we will use the ggalluvial and ggplot2 libraries. The data come from a study on language learning using the web application LingQ (Link to the article).\n\n\nShow the code\n# Load necessary libraries\nlibrary(tidyverse) # Includes ggplot2\nlibrary(ggalluvial)\nlibrary(knitr)\n\n# Input data\n\ndata &lt;- data.frame(\n  Initial = rep(c(\"First\", \"Second\", \"Third\", \"Fourth+\"), each = 4),\n  Final = rep(c(\"First\", \"Second\", \"Third\", \"Fourth+\"), 4), \n  Frequency = c(28, 28, 10, 4, 3, 6, 7, 5,  0, 3, 3, 3, 0, 0, 0, 1)\n) %&gt;% \n  mutate(\n    Initial = factor(Initial, levels = c(\"First\", \"Second\", \"Third\", \"Fourth+\")),\n    Final = factor(Final, levels = c(\"First\", \"Second\", \"Third\", \"Fourth+\"))\n  )\n\nhead(data)\n\n\n  Initial   Final Frequency\n1   First   First        28\n2   First  Second        28\n3   First   Third        10\n4   First Fourth+         4\n5  Second   First         3\n6  Second  Second         6\n\n\nIn this study, a total of 192 individuals were selected for the sample and completed the initial knowledge test. Out of these, 101 completed the final test and studied for at least two hours in the application, and were included in the analysis. The data show the equivalent language knowledge in formal education semesters (“First”, “Second”, “Third”, “Fourth or more”) at the beginning of the study (“Initial”) and after using the application (“Final”).\nNote: The data have been extracted from the example article from the result tables and are just an example of a data distribution compatible with those results."
  },
  {
    "objectID": "posts/alluvial_plot/post.html#gráfico",
    "href": "posts/alluvial_plot/post.html#gráfico",
    "title": "Diagrama aluvial (alluvial plot)",
    "section": "Gráfico",
    "text": "Gráfico\n\n# Diagrama aluvial\ndata %&gt;% \nggplot(aes(axis1 = fct_rev(Inicial), axis2 = fct_rev(Final),\n           y = Frecuencia)) +\n  scale_x_discrete(limits = c(\"Inicial\", \"Final\"), expand = c(.2, .05)) +\n  geom_alluvium(aes(fill = Final)) +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  theme_minimal() +\n  labs(x = \"Semestres equivalentes de educación formal\")\n\n\n\n\nEn el diagrama aluvial, el área de las zonas coloreadas es proporcional a la frecuencia, por lo que “flujos” más anchos representan más participantes.\nEn el gráfico se aprecia cómo la mayor parte del flujo ocurre de abajo arriba; es decir, el conocimiento tiende a mejorar más que a empeorar. Por ejemplo, más de la mitad de los participantes que partían del primer semestre de conocimientos subieron al segundo, tercer o cuarto semestre.\nTambién se puede ver como casi todos los participantes que acabaron con un nivel de conocimientos equivalente al primer semestre partían de ese nivel previo, y unos pocos bajaron del segundo semestre al primer semestre.\nEn definitiva, los diagramas aluviales proporcionan una opción visual para entender mejor los flujos de datos entre categorías, especialmente si hay algún componente temporal asociado a ellas."
  },
  {
    "objectID": "posts/lasso_regression/post.html#what-is-lasso-regression",
    "href": "posts/lasso_regression/post.html#what-is-lasso-regression",
    "title": "Lasso Regression",
    "section": "",
    "text": "Lasso regression is a modified version of linear regression whose objective is to find the simplest model possible. In order to do that, Lasso method penalizes large regression coefficients, leaving smaller coefficients and even removing some variables from the final model (i.e., setting their coefficients to zero).\nLasso is an acronym of Least Absolute Shrinkage and Selector Operator."
  },
  {
    "objectID": "posts/lasso_regression/post.html#what-is-lasso-regression-used-for",
    "href": "posts/lasso_regression/post.html#what-is-lasso-regression-used-for",
    "title": "Lasso Regression",
    "section": "",
    "text": "Lasso regression is used mainly in two applications:\n\nModel variable selection: Lasso can be used as a method to select the most important variables in a regression model. The least important variables will have their coefficients set to zero, effectively being removed from the final model.\nParameter shrinkage: Lasso’s coefficients are smaller thant those of a simple lineal regression. This helps to avoid overfitting problems.\n\nGiven their two main functions, Lasso regression is usually employed in the following situations:\n\nWhen we have a high-dimensionality dataset, i.e., with a large number of variables.\nWhen we have multicolineallity in our model, i.e., several variables are lineally dependent of one another.\nWhen we want to automatize the model building, via automatizing the selection of the included variables."
  },
  {
    "objectID": "posts/lasso_regression/post.html#how-does-lasso-regression-work",
    "href": "posts/lasso_regression/post.html#how-does-lasso-regression-work",
    "title": "Lasso Regression",
    "section": "",
    "text": "A traditional multivariable lineal regression model finds a set of regression coefficients (\\(\\beta_0, \\beta_1, \\beta_2...\\)) that minimizes the residuals’ squared sum (RSS). That is, the distance between the datapoints and the model predictions.\nLasso regression adds another parameter called L1. L1 is defined as the sum of the absolute values of the model coefficients. Lasso method tries to minimize the sum of RSS and L1. As a consequence, Lasso finds a model with smaller regression coefficients. This whole process is known as “L1 regularization”, and it produces a coefficient “shrinkage”.\nEvery time we run a Lasso regression, whe need to specify the lambda parameter (\\(\\lambda\\)). Lambda represents the relative importance of the L1 parameter compared to the RSS part of the minimization formula.\n\nWith \\(\\lambda = 0\\), there is no coefficient shrinkage, and the Lasso model is effectively equal to a regular linear regression model.\nAs \\(\\lambda\\) grows, there is more shrinkage, and more variables are removed from the model.\nIf \\(\\lambda\\) were to be infinite, all coefficients would be removed, and we would end up with an empty model.\n\n\n\n\\(min(RSS + \\lambda \\sum |\\beta_j|)\\)\nWhere\n\n\\(RSS\\) is the residuals’ square sum.\n\\(\\lambda\\) is Lasso’s penalizing factor.\n\\(\\sum |\\beta_j|\\) is the sum of the absolute values of the regression coefficients."
  },
  {
    "objectID": "posts/lasso_regression/post.html#getting-ready",
    "href": "posts/lasso_regression/post.html#getting-ready",
    "title": "Lasso Regression",
    "section": "Getting Ready",
    "text": "Getting Ready\nIn this example, we’ll use the glmnet library and the example dataset in mtcars.\n\n\nShow the code\n# install.packages(\"glmnet\") # Install the package (only once)\nlibrary(glmnet)\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nWe’ll use mpg (miles per galon) as the outcome variable, and cyl (number of cylinders), hp (horsepower), wt (weight), gear (gear number), and drat (rear axle ratio) as predictive variables.\n\n\nShow the code\n# Define the outcome variable\ny &lt;- mtcars$mpg\n\n# Define the predictive variables\nx &lt;- data.matrix(mtcars[, c(\"cyl\", \"hp\", \"wt\", \"drat\", \"gear\")])"
  },
  {
    "objectID": "posts/lasso_regression/post.html#choose-a-value-for-lambda",
    "href": "posts/lasso_regression/post.html#choose-a-value-for-lambda",
    "title": "Lasso Regression",
    "section": "Choose a Value for Lambda",
    "text": "Choose a Value for Lambda\nWe can choose the value of \\(\\lambda\\) that minimizes the mean-squared error (MSE). The cv.glmnet() function performs “K-fold cross-validation” to identify this \\(\\lambda\\) value.\n\n\nShow the code\n# Cross-validation\ncv_model &lt;- cv.glmnet(x, y, alpha = 1)  # Changing the alpha parameter leads to other types of regression\n\n# Find the lambda value that minimizes the MSE\nbest_lambda &lt;- cv_model$lambda.min\nbest_lambda\n\n\n[1] 0.3803991\n\n\nShow the code\n# Display the results in a plot\nplot(cv_model)\n\n\n\n\n\nThe value of lambda that minimizes the MSE turns out to be 0.3803991, which in the plot corresponds to the point \\(Log(\\lambda)\\) = -0.9665344."
  },
  {
    "objectID": "posts/kanji/post.html",
    "href": "posts/kanji/post.html",
    "title": "Loading and Exploring Japanese Kanji Data Using R",
    "section": "",
    "text": "Introduction\nIn this blog post, I’ll demonstrate how to use R to load, explore, and filter data from a dataset containing Japanese characters, known as “kanji”. The datasets were obtained from an online Kanji database. We’ll focus on using the tidyverse family of packages to illustrate how to select and filter relevant information efficiently.\n\n\nSetup and Loading\nTo begin, we need to load necessary libraries and import the datasets:\n\n# | warning: false\n# Loading necessary libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\n\n# Loading datasets\n\ndata_kanji &lt;- read.csv2(here(\"data/kanji\", \"Kanji_20240227_081842.csv\")) %&gt;% \n  clean_names()\n\ndata_jukugo &lt;- read.csv2(here(\"data/kanji\", \"Jukugo_20240227_081908.csv\")) %&gt;% \n  clean_names()\n\nHere’s a breakdown of the code:\n\nlibrary(tidyverse): We load the tidyverse package, which includes dplyr, ggplot2, and other useful packages.\nlibrary(here): This package helps manage file paths conveniently.\nlibrary(janitor): Useful for standardizing variable names and data cleaning.\nWe use read.csv2() to import CSV (comma-separated value) files with semicolons (;) as separators.\nhere(\"data/kanji\", \"Kanji_20240227_081842.csv\") uses the function here() to access the data file, which is saved inside the folders data &gt; kanji.\nThe characters %&gt;% are called a “pipe” in tidyverse. It can be written simply by pressing Ctrl + Shift + M (in Windows). Basically, it tells R that we want to apply some step to the previous data. In this example, I tell R that I want to use the function clean_names() to the data that I’ve already loaded using read.csv2().\nclean_names() is a janitor function that renames all variables in a standard format to make it easier to manipulate. Specifically, clean_names() sets all names to lowercase, removes punctuation and symbols, and replaces spaces with underscores.\n\nNow I have two separate datasets: one for kanji (single characters), and one for jukugo (compound words). Let’s take a look at them.\n\n\nExploring the data\nLet’s examine the first few rows of each dataset:\n\nhead(data_kanji)\n\n    id kanji strokes grade\n1   41    一       1     1\n2  124    乙       1     7\n3 2060    了       2     7\n4 2074    力       2     1\n5 1577    二       2     1\n6 1070    人       2     1\n\nhead(data_jukugo)\n\n   id comp_word frequency          grammatical_feature pronunciation\n1 173      一部     46289 possible to use as an adverb         itibu\n2 234      一般     39274                 general noun         ippan\n3 432      一時     25126 possible to use as an adverb         itizi\n4 461      一番     24155 possible to use as an adverb        itiban\n5 481      一緒     23453    light-verb -suru attached         issyo\n6 529      一致     21388    light-verb -suru attached          itti\n  english_translation position kanji kanji_id\n1            one part        L    一       41\n2             general        L    一       41\n3         one o'clock        L    一       41\n4                best        L    一       41\n5            together        L    一       41\n6         coincidence        L    一       41\n\n\nWe’re using the base function head()to show the first rows or observations of our datasets.\nWe can see that data_kanji has four columns or variables:\n\nid shows a unique identification number.\nkanji stores the actual character.\nstrokes represents the number of distinct lines or strokes that the character has.\ngrade means the official categorization of Kanji by educational year in Japan. Grade 1 includes the easiest or most common kanji, and it goes all up to grade 7.\n\nOn the other hand, data_jukugo contains nine variables:\n\nid is the identification number for jukugos.\ncomp_word is the actual word.\nfrequency is a measure of how many times each jukugo appear in a selected corpus of Japanese literature (extracted from Japanese newspapers).\ngrammatical_feature gives us more context of how the word is used in grammatical terms.\npronunciation tells us the pronunciation in “romaji”, or the Latin alphabet.\nenglish_translation stores the English translation.\n\nThe last three variables in data_jukugo describes the kanji which is part of the jukugo:\n\nposition tells us if the kanji is used in left position “L” or right position “R”.\nkanji shows the kanji used in the jukugo. The first rows all show jukugos composed with the kanji “一”.\nkanji_id is the identification number of the kanji part. We can use this id to link data_jukugo with data_kanji if we want to.\n\nAnother way of looking into a dataset is to explore how each variable is encoded:\n\nglimpse(data_kanji)\n\nRows: 2,136\nColumns: 4\n$ id      &lt;int&gt; 41, 124, 2060, 2074, 1577, 1070, 1584, 829, 359, 1647, 1903, 1…\n$ kanji   &lt;chr&gt; \"一\", \"乙\", \"了\", \"力\", \"二\", \"人\", \"入\", \"七\", \"九\", \"八\", \"…\n$ strokes &lt;int&gt; 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3,…\n$ grade   &lt;int&gt; 1, 7, 7, 1, 1, 1, 1, 1, 1, 1, 7, 3, 1, 2, 1, 2, 6, 7, 6, 1, 2,…\n\nglimpse(data_jukugo)\n\nRows: 52,791\nColumns: 9\n$ id                  &lt;int&gt; 173, 234, 432, 461, 481, 529, 937, 1465, 1521, 156…\n$ comp_word           &lt;chr&gt; \"一部\", \"一般\", \"一時\", \"一番\", \"一緒\", \"一致\", \"…\n$ frequency           &lt;int&gt; 46289, 39274, 25126, 24155, 23453, 21388, 12477, 7…\n$ grammatical_feature &lt;chr&gt; \"possible to use as an adverb\", \"general noun\", \"p…\n$ pronunciation       &lt;chr&gt; \"itibu\", \"ippan\", \"itizi\", \"itiban\", \"issyo\", \"itt…\n$ english_translation &lt;chr&gt; \"one part\", \"general\", \"one o'clock\", \"best\", \"tog…\n$ position            &lt;chr&gt; \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", …\n$ kanji               &lt;chr&gt; \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"…\n$ kanji_id            &lt;int&gt; 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41…\n\n\nThe glimpse() function allows us to quickly glance at the data structure.\nWe can see that data_kanji has 2,136 rows or observations and 4 columns or variables. We also see the first values of each of its four variables. More importantly, we can see which data type each variable stores. The kanji variable has &lt;chr&gt; type, which means “character” or “text”, while the rest of variables have &lt;int&gt; type, which means “integer” number, or a round number. R automatically detects the data types when importing data using functions like read.csv2().\nRegarding data_jukugo, it has 52,791 rows and 9 columns, of which 3 have &lt;int&gt; type, and 6 have &lt;char&gt; type.\n\n\nManipulating the data\nNow that I’m familiarized with this dataset, it’s useful to lay down what my analysis plan is. In other words, what do I want to learn from this data? In this case, I want to be able to find words (jukugo) that only contain kanji from a selected list of kanji that I’m learning. So, for example, if I only know kanjis 一, 人, and 十, I want to know all the possible combinations of these three kanjis.\nFor this exercise, I’m interested in separating jukugos in two parts: the left kanji, and the right kanji. The dataset already has half of this information, but sometimes it tells us the left kanji, and sometimes the right kanji (more on this later). I want to get sistematically both left and right kanjis in the same row, so I’ll create new variables called kanji_left and kanji_right.\n\ndata_jukugo &lt;- data_jukugo %&gt;% \n  mutate(kanji_left = substr(comp_word, 1, 1),\n         kanji_right = substr(comp_word, 2, 2))\n\nglimpse(data_jukugo)\n\nRows: 52,791\nColumns: 11\n$ id                  &lt;int&gt; 173, 234, 432, 461, 481, 529, 937, 1465, 1521, 156…\n$ comp_word           &lt;chr&gt; \"一部\", \"一般\", \"一時\", \"一番\", \"一緒\", \"一致\", \"…\n$ frequency           &lt;int&gt; 46289, 39274, 25126, 24155, 23453, 21388, 12477, 7…\n$ grammatical_feature &lt;chr&gt; \"possible to use as an adverb\", \"general noun\", \"p…\n$ pronunciation       &lt;chr&gt; \"itibu\", \"ippan\", \"itizi\", \"itiban\", \"issyo\", \"itt…\n$ english_translation &lt;chr&gt; \"one part\", \"general\", \"one o'clock\", \"best\", \"tog…\n$ position            &lt;chr&gt; \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", …\n$ kanji               &lt;chr&gt; \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"…\n$ kanji_id            &lt;int&gt; 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41…\n$ kanji_left          &lt;chr&gt; \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"一\", \"…\n$ kanji_right         &lt;chr&gt; \"部\", \"般\", \"時\", \"番\", \"緒\", \"致\", \"定\", \"連\", \"…\n\n\nLet’s explain the code:\n\nmutate() is the dplyr function used to create or change variables. Here, I create two variables, kanji_left and kanji_right.\nsubstr() is a base function that subtracts a string of text from a character variable. substr(comp_word, 1, 1) means to subtract only the first character, and substr(comp_word, 2, 2) gets the second character.\n\nAlright, now I need to define the list of kanjis that I’m currently learning. This I need to do it manually, but later I’ll explain how to do it more dinamically.\n\nkanji_learning &lt;- c(\"一\", \"二\", \"三\", \"王\", \"玉\", \"十\", \"五\")\n\nLastly, I’ll tell R to filter the jukugos that only include kanji that are on my learning list. I also want to sort the jukugos from more to less used.\n\njukugo_learning &lt;- data_jukugo %&gt;% \n  filter(kanji_left %in% kanji_learning, kanji_right %in% kanji_learning) %&gt;% \n  arrange(desc(frequency))\n\nhead(jukugo_learning)\n\n     id comp_word frequency          grammatical_feature pronunciation\n1 17059      二三        32 possible to use as an adverb         nisan\n2 17059      二三        32 possible to use as an adverb         nisan\n3 20330      三一        12                 general noun        sanpin\n4 20330      三一        12                 general noun        sanpin\n5 23443      一一         4 possible to use as an adverb        itiiti\n6 23443      一一         4 possible to use as an adverb        itiiti\n  english_translation position kanji kanji_id kanji_left kanji_right\n1        two or three        L    二     1577         二          三\n2        two or three        R    三      744         二          三\n3 low-ranking samurai        R    一       41         三          一\n4 low-ranking samurai        L    三      744         三          一\n5          one-by-one        L    一       41         一          一\n6          one-by-one        R    一       41         一          一\n\n\nThe filter() function selects rows based on one or more conditions. I’ve passed two conditions: that kanji_left is included in the kanji_learning “list” (in R we’d call this a vector, not a list), and that kanji_right is also included in kanji_learning. The term “is included in” is represented in R with the operand %in%.\nThe arrange() function reorders the rows based on one or more variables. I’ve passed the argument desc(frequency) because I want the words to be sorted in descending order of frequency (from more to less frequency).\nHowever, something odd has happened: now we have two copies of each jukugo. There are complete duplicates in the dataset, with the only difference of which kanji appears in the variables position, kanji, and kanji_id. For example, “nisan” (二三) appears twice, one with position L, kanji 二, and kanji_id 1577, and another with position R, kanji 三, and kanji_id 744. This is something that I didn’t see first time I explored the dataset.\nI could have done things differently. Instead of splitting the jukugos manually, I could have performed a “self-join” of the duplicated rows. But one cool thing about data cleaning and analysis is that there are always different ways to reach the same goal. It’s an iterative process, and by trial and error I can learn a lot and find alternative methods of doing things.\nMoving forward, since I’m only interested in keeping one record of each jukugo, I can drop these duplicates. Aditionally, I’ll keep only the variables I’m interested in.\n\njukugo_learning &lt;- jukugo_learning %&gt;% \n  select(id, comp_word, frequency, grammatical_feature, pronunciation, english_translation, kanji_left, kanji_right) %&gt;%\n  distinct()\n\nhead(jukugo_learning)\n\n     id comp_word frequency          grammatical_feature pronunciation\n1 17059      二三        32 possible to use as an adverb         nisan\n2 20330      三一        12                 general noun        sanpin\n3 23443      一一         4 possible to use as an adverb        itiiti\n4 25773      二王         2                 general noun          nioo\n          english_translation kanji_left kanji_right\n1                two or three         二          三\n2         low-ranking samurai         三          一\n3                  one-by-one         一          一\n4 the two guardian Deva kings         二          王\n\n\nI’ve used two new dplyr functions: select() keeps some columns or variables, and distinct() keeps only non-duplicated rows.\nThe final result contains four distinct jukugos: 二三, 三一, 一一, and 二王. All of them are very low-frequency, with the most common of them appearing only 32 times.\n\n\nNext step: making it interactive\nSo far, I have created a code that filters Japanese kanji words based on whatever Kanji components I want. However, the whole process would be nicer if I had a way of selecting the data interactivelly, maybe pressing some buttons. We can do just that using R Shiny applications. Find how in this post!\n\n\nReferences\n\nKanji database."
  },
  {
    "objectID": "posts/kanji_app/post.html",
    "href": "posts/kanji_app/post.html",
    "title": "Interactive List of Japanese Words Using R Shiny Apps",
    "section": "",
    "text": "In a previous post, I demonstrated how to use R to load, explore, and filter data from a dataset containing Japanese characters, known as “kanji”.\nToday, we’ll focus on making that data exploration interactive using the R package shiny."
  },
  {
    "objectID": "posts/kanji_app/post.html#setup-and-loading-data",
    "href": "posts/kanji_app/post.html#setup-and-loading-data",
    "title": "Interactive List of Japanese Words Using R Shiny Apps",
    "section": "Setup and Loading Data",
    "text": "Setup and Loading Data\nTo begin, we need to load the necessary libraries and import the datasets. I’ll use five different datasets to form the whole “dictionary”:\n\nA list of kanji downloaded from Kanji database\nA list of jukugo (words composed by two kanji) downloaded from Kanji database\nA list of words obtained from an Anki deck from here\nA list of sentences obtained from an Anki deck from here\nA dataset of Japanese numbers from 0 to 100, created manually.\n\nEach dataset contains three columns: word (in kanji), pronunciation (in hiragana or romaji), and meaning (in English).\n\n\nShow the code\n# Loading necessary libraries ####\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(readxl)\nlibrary(writexl)\nlibrary(shiny)\nlibrary(DT)\nlibrary(flextable)\n\n# Loading datasets ####\n\nfile_kanji &lt;- \"Kanji_20240227_081842.csv\"\nfile_jukugo &lt;- \"Jukugo_20240227_081908.csv\"\nfile_words &lt;- \"Optimized Kore - Sheet1.csv\"\nfile_sentences1 &lt;- \"Sentences.xlsx\"\nfile_sentences2 &lt;- \"Sentences_core.xlsx\"\n\n## Kanji list ####\ndata_kanji &lt;- read.csv2(here(\"data/kanji\", file_kanji)) %&gt;% \n  clean_names()\n\n## Jukugo list ####\ndata_jukugo &lt;- read.csv2(here(\"data/kanji\", file_jukugo)) %&gt;% \n  clean_names() %&gt;% \n  arrange(desc(frequency)) %&gt;% \n  select(comp_word, pronunciation, english_translation) %&gt;%\n  rename(word = comp_word,\n         meaning = english_translation) %&gt;% \n  distinct(word, .keep_all = TRUE) %&gt;% \n  # Change romaji\n  mutate(\n    pronunciation = gsub(\"zi\", \"ji\", pronunciation),\n    pronunciation = gsub(\"zy\", \"jy\", pronunciation),\n    pronunciation = gsub(\"ti\", \"chi\", pronunciation),\n    pronunciation = gsub(\"ty\", \"ch\", pronunciation),\n    pronunciation = gsub(\"si\", \"shi\", pronunciation),\n    pronunciation = gsub(\"sy\", \"shy\", pronunciation),\n    pronunciation = gsub(\"tu\", \"tsu\", pronunciation),\n    pronunciation = gsub(\"hu\", \"fu\", pronunciation)\n    )\n\n## word list ####\ndata_words &lt;- read.csv(here(\"data/kanji\", file_words)) %&gt;% \n  clean_names() %&gt;% \n  arrange(core_index) %&gt;% \n  select(vocab_expression, vocab_kana, vocab_meaning) %&gt;% \n  rename(word = vocab_expression,\n         pronunciation = vocab_kana,\n         meaning = vocab_meaning) %&gt;% \n  distinct(word, .keep_all = TRUE) \n\n## Number list ####\n\ndata_numbers &lt;- data.frame(\n  word = c(\"零\", \"一\", \"二\", \"三\", \"四\", \"五\", \"六\", \"七\", \"八\", \"九\", \"十\", \n     \"十一\", \"十二\", \"十三\", \"十四\", \"十五\", \"十六\", \"十七\", \"十八\", \"十九\", \n     \"二十\", \"二十一\", \"二十二\", \"二十三\", \"二十四\", \"二十五\", \"二十六\", \"二十七\", \n     \"二十八\", \"二十九\", \"三十\", \"三十一\", \"三十二\", \"三十三\", \"三十四\", \"三十五\", \n     \"三十六\", \"三十七\", \"三十八\", \"三十九\", \"四十\", \"四十一\", \"四十二\", \"四十三\", \n     \"四十四\", \"四十五\", \"四十六\", \"四十七\", \"四十八\", \"四十九\", \"五十\", \"五十一\", \n     \"五十二\", \"五十三\", \"五十四\", \"五十五\", \"五十六\", \"五十七\", \"五十八\", \"五十九\", \n     \"六十\", \"六十一\", \"六十二\", \"六十三\", \"六十四\", \"六十五\", \"六十六\", \"六十七\", \n     \"六十八\", \"六十九\", \"七十\", \"七十一\", \"七十二\", \"七十三\", \"七十四\", \"七十五\", \n     \"七十六\", \"七十七\", \"七十八\", \"七十九\", \"八十\", \"八十一\", \"八十二\", \"八十三\", \n     \"八十四\", \"八十五\", \"八十六\", \"八十七\", \"八十八\", \"八十九\", \"九十\", \"九十一\", \n     \"九十二\", \"九十三\", \"九十四\", \"九十五\", \"九十六\", \"九十七\", \"九十八\", \"九十九\", \"百\"),\n  pronunciation = c(\"れい\", \"いち\", \"に\", \"さん\", \"し\", \"ご\", \"ろく\", \"しち\", \"はち\", \"きゅう\", \"じゅう\",\n                    \"じゅういち\", \"じゅうに\", \"じゅうさん\", \"じゅうし\", \"じゅうご\", \"じゅうろく\", \"じゅうしち\", \"じゅうはち\", \"じゅうきゅう\",\n                    \"にじゅう\", \"にじゅういち\", \"にじゅうに\", \"にじゅうさん\", \"にじゅうし\", \"にじゅうご\", \"にじゅうろく\", \"にじゅうしち\", \"にじゅうはち\", \"にじゅうきゅう\",\n                    \"さんじゅう\", \"さんじゅういち\", \"さんじゅうに\", \"さんじゅうさん\", \"さんじゅうし\", \"さんじゅうご\", \"さんじゅうろく\", \"さんじゅうしち\", \"さんじゅうはち\", \"さんじゅうきゅう\",\n                    \"よんじゅう\", \"よんじゅういち\", \"よんじゅうに\", \"よんじゅうさん\", \"よんじゅうし\", \"よんじゅうご\", \"よんじゅうろく\", \"よんじゅうしち\", \"よんじゅうはち\", \"よんじゅうきゅう\",\n                    \"ごじゅう\", \"ごじゅういち\", \"ごじゅうに\", \"ごじゅうさん\", \"ごじゅうし\", \"ごじゅうご\", \"ごじゅうろく\", \"ごじゅうしち\", \"ごじゅうはち\", \"ごじゅうきゅう\",\n                    \"ろくじゅう\", \"ろくじゅういち\", \"ろくじゅうに\", \"ろくじゅうさん\", \"ろくじゅうし\", \"ろくじゅうご\", \"ろくじゅうろく\", \"ろくじゅうしち\", \"ろくじゅうはち\", \"ろくじゅうきゅう\",\n                    \"しちじゅう\", \"しちじゅういち\", \"しちじゅうに\", \"しちじゅうさん\", \"しちじゅうし\", \"しちじゅうご\", \"しちじゅうろく\", \"しちじゅうしち\", \"しちじゅうはち\", \"しちじゅうきゅう\",\n                    \"はちじゅう\", \"はちじゅういち\", \"はちじゅうに\", \"はちじゅうさん\", \"はちじゅうし\", \"はちじゅうご\", \"はちじゅうろく\", \"はちじゅうしち\", \"はちじゅうはち\", \"はちじゅうきゅう\",\n                    \"きゅうじゅう\", \"きゅうじゅういち\", \"きゅうじゅうに\", \"きゅうじゅうさん\", \"きゅうじゅうし\", \"きゅうじゅうご\", \"きゅうじゅうろく\", \"きゅうじゅうしち\", \"きゅうじゅうはち\", \"きゅうじゅうきゅう\",\n                    \"ひゃく\"),\n  meaning = as.character(0:100)\n  )\n\n# Sentence list ####\ndata_sentences1 &lt;- read_excel(here(\"data/kanji\", file_sentences1)) \ndata_sentences2 &lt;- read_excel(here(\"data/kanji\", file_sentences2))\n\ndata_sentences &lt;- rbind(data_sentences1, data_sentences2)\nrm(data_sentences1, data_sentences2)\n\n# Extract kana ####\n\nhiragana_chars &lt;- intToUtf8(seq(12353, 12438)) # Unicode range for hiragana characters\nkatakana_chars &lt;- intToUtf8(seq(12448, 12543)) # Unicode range for katakana characters\n\nkana &lt;- paste0(hiragana_chars, katakana_chars) %&gt;% \n  str_split_1(pattern = \"\") %&gt;% \n  paste(collapse = \"|\")\n\n\n# All data ####\ndata_bind &lt;- data_words %&gt;% \n  rbind(data_sentences %&gt;% select(word, pronunciation, meaning),\n        data_numbers,\n        data_jukugo) %&gt;% \n  mutate(word = str_remove_all(word, \"[ a-zA-Z]\")) %&gt;% \n  distinct(word, .keep_all = TRUE)\n\n# Extract kanji ####\n\nall_kanji &lt;- data_kanji %&gt;% \n  select(kanji) %&gt;% \n  rbind(data_bind %&gt;% \n          mutate(kanji = word, .keep = \"none\")) %&gt;% \n  mutate(kanji = str_remove_all(kanji, kana)) %&gt;% \n  filter(kanji != \"\") %&gt;% \n  pull() %&gt;% \n  paste(collapse = \"\") %&gt;% \n  str_split_1(pattern = \"\") %&gt;% \n  unique()\n  \n## Separate individual kanji from all words ####\ndata_all &lt;- data_bind %&gt;% \n  mutate(kanji = str_remove_all(word, kana)) %&gt;% \n  separate_wider_position(kanji, widths = c(\"kanji_1\" = 1, \n                                            \"kanji_2\" = 2,\n                                            \"kanji_3\" = 3,\n                                            \"kanji_4\" = 4,\n                                            \"kanji_5\" = 5),\n                          too_few = \"align_start\") %&gt;% \n  filter(!is.na(kanji_1))\n\nflextable(head(data_all))\n\n\n\nwordpronunciationmeaningkanji_1kanji_2kanji_3kanji_4kanji_5見るみるsee, look at見円えんcircle円多いおおいlots of多家うちhouse, home家新しいあたらしいnew新私わたしI私\n\n\nHere’s a breakdown of the final “dictionary”:\n\nword: This column contains the Japanese word.\npronunciation: It shows the pronunciation of the word.\nmeaning: This column provides the English translation or meaning of the word.\nkanji_1, kanji_2, kanji_3, kanji_4, kanji_5: These columns represents the kanji characters that compose each word.\n\nEach row corresponds to a different Japanese word, and the columns contain related information about each word, including its pronunciation, meaning in English, and associated kanji characters.\nFor example:\n\nThe word “見る” (miru) means “see, look at”, and its associated kanji is “見”.\nThe word “円” (en) means “circle”, and its associated kanji is “円”.\nThe word “多い” (ooi) means “lots of”, and its associated kanji is “多”."
  },
  {
    "objectID": "posts/kanji_app/post.html#shiny-app-code",
    "href": "posts/kanji_app/post.html#shiny-app-code",
    "title": "Interactive List of Japanese Words Using R Shiny Apps",
    "section": "Shiny App Code",
    "text": "Shiny App Code\n\n\nShow the code\n# Shiny App ####\n\nui &lt;- fluidPage(\n  \n  titlePanel(\"List Japanese Words Including Selected Kanji\"),\n  \n  sidebarLayout(\n    \n    sidebarPanel(\n      p(\"Created by CarlosEpiStats\"),\n      uiOutput(\"blog_link\"),\n      downloadButton(\"downloadExcel\", \"Download Excel file\"),\n      downloadButton(\"downloadCSV\", \"Download CSV file\"),\n      selectInput (\"kanji_selected\", \"Select Kanji:\",  \n                  choices = all_kanji, multiple = TRUE)\n\n    ),\n    \n    mainPanel(\n      \n      tabsetPanel(\n        \n        tabPanel(\n          \"List of words\",\n          textOutput(\"n_kanji\"),\n          textOutput(\"n_words\"),\n          DTOutput(\"table_words\")\n          ),\n        \n        tabPanel(\n          \"List of sentences\",\n          textOutput(\"n_sentences\"),\n          DTOutput(\"table_sentences\")\n        ),\n        \n        tabPanel(\n          \"Stats\",\n          tableOutput(\"number_words_per_kanji\")\n        )\n      )\n      \n      \n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  # Blog link ####\n  url &lt;- a(\"CarlosEpiStats\", href=\"https://carlosepistats.github.io/blog/\")\n \n  output$blog_link &lt;- renderUI({\n    tagList(\"Blog:\", url)\n  })\n  \n  \n  # Reactive table words ####\n  rval_table_words &lt;- reactive({\n    study_table &lt;- make_study_list(input$kanji_selected)\n    study_table\n  })\n  \n  \n  # Reactive table sentences ####\n  rval_table_sentences &lt;- reactive({\n    study_table &lt;- make_sentences_list(input$kanji_selected)\n    study_table\n  })\n  \n  \n  # Reactive table counter ####\n  rval_table_counter &lt;- reactive({\n    \n    study_words &lt;- data_all %&gt;% \n      filter(kanji_1 %in% input$kanji_selected, \n             kanji_2 %in% input$kanji_selected | is.na(kanji_2),\n             kanji_3 %in% input$kanji_selected | is.na(kanji_3),\n             kanji_4 %in% input$kanji_selected | is.na(kanji_4),\n             kanji_5 %in% input$kanji_selected | is.na(kanji_5))\n    \n    learning_length &lt;- length(input$kanji_selected)\n    \n    kanji_counter &lt;- c()\n    \n    for (i in 1:learning_length){\n      \n      kanji_counter[i] &lt;- study_words %&gt;% \n        filter(kanji_1 %in% input$kanji_selected[i] | \n                 kanji_2 %in% input$kanji_selected[i] | \n                 kanji_3 %in% input$kanji_selected[i] |\n                 kanji_4 %in% input$kanji_selected[i] | \n                 kanji_5 %in% input$kanji_selected[i] ) %&gt;% \n        nrow()\n      \n    }\n    \n    table_counter &lt;- data.frame(kanji = input$kanji_selected,\n                                times = kanji_counter)\n    \n    table_counter\n    \n  })\n  \n  # Reactive number kanji ####\n  rval_n_kanji &lt;- reactive({\n    number_words &lt;- length(input$kanji_selected)\n    number_words\n  }) \n  \n  \n  # Reactive number words ####\n  rval_n_words &lt;- reactive({\n    study_table &lt;- rval_table_words()\n    number_words &lt;- nrow(study_table)\n    number_words\n  })\n  \n  # Reactive number sentences ####\n  rval_n_sentences &lt;- reactive({\n    study_table &lt;- rval_table_sentences()\n    number_sentences &lt;- nrow(study_table)\n    number_sentences\n  })\n  \n  # output number words ####\n  output$n_words &lt;- renderText({\n    n_words &lt;- rval_n_words()\n    paste0(\"Number of words: \", n_words)\n  })\n  \n  # output number Kanji ####\n  output$n_kanji &lt;- renderText({\n    number_kanji &lt;- rval_n_kanji()\n    paste0(\"Number of kanji: \", number_kanji)\n  })\n  \n  # output number sentences ####\n  output$n_sentences &lt;- renderText({\n    number_sentences &lt;- rval_n_sentences()\n    paste0(\"Number of sentences: \", number_sentences)\n  })\n  \n  # output table words ####\n  output$table_words &lt;- renderDT({\n    study_table &lt;- rval_table_words()\n    study_table\n  })\n  \n  # output table sentences ####\n  output$table_sentences &lt;- renderDT({\n    study_table &lt;- rval_table_sentences()\n    study_table\n  })\n  \n  # output table counter ####\n  \n  output$number_words_per_kanji &lt;- renderTable({\n    table_counter &lt;- rval_table_counter()\n    table_counter\n  })\n  \n  # download Excel ####\n  \n  output$downloadExcel &lt;- downloadHandler(\n    \n    filename = function() {\n      paste0(\"study_list_k\", rval_n_kanji(), \"_w\", rval_n_words(),\".xlsx\")\n    },\n    content = function(file) {\n      write_xlsx(rval_table_words(), file)\n      \n    })\n  \n  # download CSV ####\n  \n  output$downloadCSV &lt;- downloadHandler(\n    \n    filename = function() {\n      paste0(\"study_list_k\", rval_n_kanji(), \"_w\", rval_n_words(),\".csv\")\n    },\n    content = function(file) {\n      write_csv(rval_table_words(), file)\n      \n    })\n\n}\n\n\nshinyApp(ui = ui, server = server)\n\n\nThis code defines a Shiny web application that allows users to generate lists of Japanese words and sentences containing specific kanji characters they want to learn. Here’s an explanation of the key components:\n\nUser Interface Definition (ui):\n\n\nThe UI is defined using fluidPage layout.\nIt consists of a title panel, a sidebar layout with a sidebar panel containing download buttons and a select input to choose kanji characters, and a main panel containing tabs to display the lists of words and sentences.\nThe tabs include a list of words, a list of sentences, and a stats tab.\nVarious output elements like text, tables, and DT (DataTables) are defined to display data.\n\n\nServer Logic (server):\n\n\nThe server logic defines the functionality of the Shiny app.\nIt contains reactive expressions to dynamically update the output elements based on user input.\nIt defines functions to generate lists of words and sentences based on selected kanji characters (make_study_list and make_sentences_list).\nIt calculates the number of occurrences of each kanji character in the selected words and displays them in a table on the “Stats” tab.\nIt provides output functions to display the generated lists of words and sentences.\nIt includes download handlers to allow users to download the generated lists in Excel and CSV formats.\n\n\nShinyApp function:\n\n\nThe shinyApp function combines the UI and server logic to create the Shiny application.\n\nOverall, this Shiny app provides an interactive interface for users to explore and download lists of Japanese words and sentences containing specific kanji characters they are interested in learning."
  },
  {
    "objectID": "posts/alluvial_plot_stata/post.html",
    "href": "posts/alluvial_plot_stata/post.html",
    "title": "How to Create an Alluvial Plot in Stata",
    "section": "",
    "text": "In this post, I explain how to create an alluvial plot using code in Stata.\n\n\nAn alluvial diagram or plot displays the flow of information from one categorical variable or stage to the next. The term “alluvial” refers to its resemblance to the flow of a river.\n\n\n\nAlluvial diagrams are commonly used in the following situations:\n\nTo demonstrate the number of participants in a study transitioning from one baseline category to a subsequent category. For example, the number of participants randomized at the beginning of the study and who continue to follow-up at the end. It serves as a graphical supplement to flowcharts, which present information with arrows and boxes containing text and numbers.\nTo show the distribution of participants among different categorical variables. In this case, it provides a more “visually appealing” alternative to stacked column charts."
  },
  {
    "objectID": "posts/alluvial_plot_stata/post.html#qué-es-un-diagrama-aluvial",
    "href": "posts/alluvial_plot_stata/post.html#qué-es-un-diagrama-aluvial",
    "title": "How to Create an Alluvial Plot in Stata",
    "section": "",
    "text": "Un diagrama o gráfico aluvial muestra el flujo de información de un estadio al siguiente. El nombre “aluvial” alude a su parecido con el flujo de un río."
  },
  {
    "objectID": "posts/alluvial_plot_stata/post.html#para-qué-se-usan-los-diagramas-aluviales",
    "href": "posts/alluvial_plot_stata/post.html#para-qué-se-usan-los-diagramas-aluviales",
    "title": "How to Create an Alluvial Plot in Stata",
    "section": "",
    "text": "Un diagrama aluvial se suele utilizar en las siguientes situaciones:\n\nPara mostrar el número de participantes de un estudio que cambian desde una categoría basal a una categoría siguiente. Por ejemplo, el número de participantes aleatorizados al principio del estudio y que continúan en seguimiento al final. Es un complemento gráfico a los diagramas de flujo, que presentan la información con flechas y cajas con texto y números.\nPara mostrar la distribución de participantes entre distintas variables categóricas. En este caso es una alternativa más “vistosa” a los diagramas de columnas apilados."
  },
  {
    "objectID": "posts/alluvial_plot_stata/post.html#preparativos",
    "href": "posts/alluvial_plot_stata/post.html#preparativos",
    "title": "How to Create an Alluvial Plot in Stata",
    "section": "Preparativos",
    "text": "Preparativos\nPara este ejemplo, usaremos el comando alluvial, que necesita la instalación de los comandos palettes y colrspace y ggplot2. Los datos son una simulación de una comparación de dos tests diagnósticos ficticios, llamados “Test A” y “Test B”, que pueden tener como valor “1-very low”, “2-low”, “3-medium”, “4-high”, y “5-very high”.\n\n\nShow the code\n// Install user-written commands\nnet install alluvial, from(\"https://raw.githubusercontent.com/asjadnaqvi/stata-alluvial/main/installation/\") replace\nssc install palettes, replace\nssc install colrspace, replace\n\n// Input data\nclear\n\ninput test_b test_a\n    4   4\n    1   1\n    1   1\n    2   2\n    3   1\n    1   2\n    3   2\n    2   2\n    5   2\n    3   1\n    3   3\n    3   3\n    5   3\n    4   3\n    1   1\n    3   3\n    4   1\n    2   3\n    5   2\n    4   2\n    4   2\n    2   1\n    5   3\n    3   3\n    5   1\n    3   3\nend\n\n// Label data\n\nlabel var test_a \"Test A\"\nlabel var test_b \"Test B\"\n\nlabel define high_low 1 \"1-very low\" 2 \"2-low\" 3 \"3-medium\" 4 \"4-high\" 5 \"5-very high\", replace\nlabel values test_a test_b high_low\n\n// Tabulate\n\ntab test_a test_b\n\n\n\n\n\n\n\n            |                   Test B\n     Test A | 1-very lo      2-low   3-medium     4-high |     Total\n------------+--------------------------------------------+----------\n 1-very low |         3          1          2          1 |         8 \n      2-low |         1          2          1          2 |         8 \n   3-medium |         0          1          5          1 |         9 \n     4-high |         0          0          0          1 |         1 \n------------+--------------------------------------------+----------\n      Total |         4          4          8          5 |        26 \n\n\n            |   Test B\n     Test A | 5-very hi |     Total\n------------+-----------+----------\n 1-very low |         1 |         8 \n      2-low |         2 |         8 \n   3-medium |         2 |         9 \n     4-high |         0 |         1 \n------------+-----------+----------\n      Total |         5 |        26 \n\n\nEn este estudio ficticio, un total de 26 personas fueron sometidas a los dos tests diagnósticos. Podemos ver en la tabla que tres personas obtuvieron un resultado “very low” en ambos tests, que una persona obtuvo un resultado “very low” en el test A y “low” en el test B, etc.\nLa tabla en sí misma es informativa, pero no permite extraer información de forma rápida e intuitiva. Aquí es donde entra el alluvial plot."
  },
  {
    "objectID": "posts/alluvial_plot_stata/post.html#gráfico",
    "href": "posts/alluvial_plot_stata/post.html#gráfico",
    "title": "How to Create an Alluvial Plot in Stata",
    "section": "Gráfico",
    "text": "Gráfico\n\n\nShow the code\n// Alluvial plot\n\nalluvial test_a test_b, palette(RdYlBu, n(5)) labangle(0) novalues boxwidth(5) offset(10) title(\"Alluvial plot of results of Test A vs Test B.\", size(3)) note (\"N = 26\") graphregion(color(white)) showtot lwidth(0.01) lcolor(gray) labpos(3)\n\nqui: graph export fig1.svg, width(1600) replace\n\n\n\n\n\nalluvial_plot\n\n\nIn the alluvial diagram, the area of the colored zones is proportional to the frequency, so wider “flows” represent more participants.\nThe graph shows how there is a considerable “transfer” of participants between result levels in the two tests. For example, we see that slightly less than half of those with a “very low” result in Test A also obtained “very low” in Test B, but others obtained “low”, “medium”, “high”, and even “very high” results. We also clearly see how there are no participants with “very high” results in Test A, or that the only participant with a “high” result in Test A also had a “high” result in Test B.\nIn conclusion, alluvial diagrams provide a visual option for better understanding data flows between categories."
  },
  {
    "objectID": "posts/alluvial_plot_stata/post.html#what-is-an-alluvial-diagram",
    "href": "posts/alluvial_plot_stata/post.html#what-is-an-alluvial-diagram",
    "title": "How to Create an Alluvial Plot in Stata",
    "section": "",
    "text": "An alluvial diagram or plot displays the flow of information from one categorical variable or stage to the next. The term “alluvial” refers to its resemblance to the flow of a river."
  },
  {
    "objectID": "posts/alluvial_plot_stata/post.html#uses-of-alluvial-diagrams",
    "href": "posts/alluvial_plot_stata/post.html#uses-of-alluvial-diagrams",
    "title": "How to Create an Alluvial Plot in Stata",
    "section": "",
    "text": "Alluvial diagrams are commonly used in the following situations:\n\nTo demonstrate the number of participants in a study transitioning from one baseline category to a subsequent category. For example, the number of participants randomized at the beginning of the study and who continue to follow-up at the end. It serves as a graphical supplement to flowcharts, which present information with arrows and boxes containing text and numbers.\nTo show the distribution of participants among different categorical variables. In this case, it provides a more “visually appealing” alternative to stacked column charts."
  },
  {
    "objectID": "posts/alluvial_plot_stata/post.html#preparations",
    "href": "posts/alluvial_plot_stata/post.html#preparations",
    "title": "How to Create an Alluvial Plot in Stata",
    "section": "Preparations",
    "text": "Preparations\nFor this example, we will use the alluvial command, which requires the installation of the palettes and colrspace commands. The data is a simulation of a comparison of two fictional diagnostic tests, called “Test A” and “Test B”, which can take values such as “1-very low”, “2-low”, “3-medium”, “4-high”, and “5-very high”.\n\n\nShow the code\n// Install user-written commands\nnet install alluvial, from(\"https://raw.githubusercontent.com/asjadnaqvi/stata-alluvial/main/installation/\") replace\nssc install palettes, replace\nssc install colrspace, replace\n\n\n\n\nShow the code\n// Input data\nclear\n\ninput test_b test_a\n    4   4\n    1   1\n    1   1\n    2   2\n    3   1\n    1   2\n    3   2\n    2   2\n    5   2\n    3   1\n    3   3\n    3   3\n    5   3\n    4   3\n    1   1\n    3   3\n    4   1\n    2   3\n    5   2\n    4   2\n    4   2\n    2   1\n    5   3\n    3   3\n    5   1\n    3   3\nend\n\n// Label data\n\nlabel var test_a \"Test A\"\nlabel var test_b \"Test B\"\n\nlabel define high_low 1 \"1-very low\" 2 \"2-low\" 3 \"3-medium\" 4 \"4-high\" 5 \"5-very high\", replace\nlabel values test_a test_b high_low\n\n// Tabulate\n\ntab test_a test_b\n\n\n\n\n            |                   Test B\n     Test A | 1-very lo      2-low   3-medium     4-high |     Total\n------------+--------------------------------------------+----------\n 1-very low |         3          1          2          1 |         8 \n      2-low |         1          2          1          2 |         8 \n   3-medium |         0          1          5          1 |         9 \n     4-high |         0          0          0          1 |         1 \n------------+--------------------------------------------+----------\n      Total |         4          4          8          5 |        26 \n\n\n            |   Test B\n     Test A | 5-very hi |     Total\n------------+-----------+----------\n 1-very low |         1 |         8 \n      2-low |         2 |         8 \n   3-medium |         2 |         9 \n     4-high |         0 |         1 \n------------+-----------+----------\n      Total |         5 |        26 \n\n\nIn this fictional study, a total of 26 people underwent both diagnostic tests. We can see in the table that three people obtained a “very low” result in both tests, one person obtained a “very low” result in test A and “low” in test B, etc.\nThe table itself is informative but does not allow for quick and intuitive information extraction. This is where the alluvial plot comes in."
  },
  {
    "objectID": "posts/alluvial_plot_stata/post.html#alluvial-plot",
    "href": "posts/alluvial_plot_stata/post.html#alluvial-plot",
    "title": "How to Create an Alluvial Plot in Stata",
    "section": "Alluvial Plot",
    "text": "Alluvial Plot\n\n\nShow the code\n// Alluvial plot\n\nalluvial test_a test_b, palette(RdYlBu, n(5)) labangle(0) novalues boxwidth(5) offset(10) title(\"Alluvial plot of results of Test A vs Test B.\", size(3)) note (\"N = 26\") graphregion(color(white)) showtot lwidth(0.01) lcolor(gray) labpos(3)\n\nqui: graph export fig1.svg, width(1600) replace\n\n\n\n\n\nalluvial_plot\n\n\nIn the alluvial diagram, the area of the colored zones is proportional to the frequency, so wider “flows” represent more participants.\nThe graph shows how there is a considerable “transfer” of participants between result levels in the two tests. For example, we see that slightly less than half of those with a “very low” result in Test A also obtained “very low” in Test B, but others obtained “low”, “medium”, “high”, and even “very high” results. We also clearly see how there are no participants with “very high” results in Test A, or that the only participant with a “high” result in Test A also had a “high” result in Test B.\nIn conclusion, alluvial diagrams provide a visual option for better understanding data flows between categories."
  },
  {
    "objectID": "posts/alluvial_plot/post.html#what-is-an-alluvial-diagram",
    "href": "posts/alluvial_plot/post.html#what-is-an-alluvial-diagram",
    "title": "How to Create an Alluvial Plot in R",
    "section": "",
    "text": "An alluvial diagram or plot displays the flow of information from one categorical variable or stage to the next. The term “alluvial” refers to its resemblance to the flow of a river."
  },
  {
    "objectID": "posts/alluvial_plot/post.html#uses-of-alluvial-diagrams",
    "href": "posts/alluvial_plot/post.html#uses-of-alluvial-diagrams",
    "title": "How to Create an Alluvial Plot in R",
    "section": "",
    "text": "Alluvial diagrams are commonly used in the following situations:\n\nTo demonstrate the number of participants in a study transitioning from one baseline category to a subsequent category. For example, the number of participants randomized at the beginning of the study and who continue to follow-up at the end. It serves as a graphical supplement to flowcharts, which present information with arrows and boxes containing text and numbers.\nTo show the distribution of participants among different categorical variables. In this case, it provides a more “visually appealing” alternative to stacked column charts."
  },
  {
    "objectID": "posts/alluvial_plot/post.html#alluvial-plot",
    "href": "posts/alluvial_plot/post.html#alluvial-plot",
    "title": "How to Create an Alluvial Plot in R",
    "section": "Alluvial Plot",
    "text": "Alluvial Plot\n\n\nShow the code\ndata %&gt;% \nggplot(aes(axis1 = fct_rev(Initial), axis2 = fct_rev(Final),\n           y = Frequency)) +\n  scale_x_discrete(limits = c(\"Initial\", \"Final\"), expand = c(.2, .05)) +\n  geom_alluvium(aes(fill = Final)) +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  theme_minimal() +\n  labs(x = \"Equivalent formal education semesters\")\n\n\n\n\n\nIn the alluvial diagram, the area of the colored zones is proportional to the frequency, so wider “flows” represent more participants.\nThe graph shows how the majority of the flow occurs from bottom to top; that is, knowledge tends to improve more than deteriorate. For example, more than half of the participants who started from the first semester of knowledge progressed to the second, third, or fourth semester.\nIt can also be seen that almost all participants who ended with a knowledge level equivalent to the first semester started from that previous level, and a few decreased from the second semester to the first semester.\nIn conclusion, alluvial diagrams provide a visual option for better understanding data flows between categories."
  },
  {
    "objectID": "posts/lasso_regression/post.html#fitting-the-model",
    "href": "posts/lasso_regression/post.html#fitting-the-model",
    "title": "Lasso Regression",
    "section": "Fitting the Model",
    "text": "Fitting the Model\n\n\nShow the code\n# Model coefficients\nbest_model &lt;- glmnet(x, y, alpha = 1, lambda = best_lambda)\ncoef(best_model)\n\n\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) 35.99744772\ncyl         -0.85518839\nhp          -0.01598015\nwt          -2.88233444\ndrat         0.27858106\ngear         .         \n\n\nWe can observe that the coefficient of gear appears as a point, indicating that the Lasso regression has eliminated the coefficient since the variable was not important enough."
  },
  {
    "objectID": "posts/lasso_regression/post.html#comparison-with-linear-regression-without-lasso",
    "href": "posts/lasso_regression/post.html#comparison-with-linear-regression-without-lasso",
    "title": "Lasso Regression",
    "section": "Comparison with Linear Regression without Lasso",
    "text": "Comparison with Linear Regression without Lasso\nFor comparison, we can see the coefficients that would result from a multiple linear regression model without parameter shrinkage or variable selection.\n\n\nShow the code\nlinear_model &lt;- lm(mpg ~ cyl + hp + wt + drat + gear, data = mtcars)\nmodel_table &lt;- cbind(coef(best_model), coef(linear_model))\ncolnames(model_table) &lt;- c(\"Lasso\", \"Linear\")\nmodel_table\n\n\n6 x 2 sparse Matrix of class \"dgCMatrix\"\n                  Lasso      Linear\n(Intercept) 35.99744772 33.99417771\ncyl         -0.85518839 -0.72169272\nhp          -0.01598015 -0.02227636\nwt          -2.88233444 -2.92715539\ndrat         0.27858106  0.73105753\ngear         .           0.16750690\n\n\nThe coefficients of the Lasso model have been shrunk slightly, especially for the drat variable, and the gear variable has been automatically excluded."
  },
  {
    "objectID": "posts/kanji_app/post.html#custom-functions-to-create-the-study-table",
    "href": "posts/kanji_app/post.html#custom-functions-to-create-the-study-table",
    "title": "Interactive List of Japanese Words Using R Shiny Apps",
    "section": "Custom Functions to Create the Study Table",
    "text": "Custom Functions to Create the Study Table\n\n\nShow the code\n# Functions ####\n\nmake_study_list &lt;- function(kanji_learning) {\n  \n  # Select words with only those kanji\n  study_words &lt;- data_all %&gt;% \n    filter(kanji_1 %in% kanji_learning,  \n             kanji_2 %in% kanji_learning | is.na(kanji_2),\n             kanji_3 %in% kanji_learning | is.na(kanji_3),\n             kanji_4 %in% kanji_learning | is.na(kanji_4),\n             kanji_5 %in% kanji_learning | is.na(kanji_5)) %&gt;% \n    select(!starts_with(\"kanji_\"))\n  \n  study_words\n}\n\n\nmake_sentences_list &lt;- function(kanji_learning) {\n  \n  kanji_list &lt;- paste(kanji_learning, collapse = \"|\")\n  \n  letters_lower &lt;- paste(letters, collapse = \"|\")\n  letters_upper &lt;- paste(LETTERS, collapse = \"|\")\n  numbers &lt;- paste(0:9, collapse = \"|\")\n  symbols &lt;- \"。|、|「|」|？|!|%|！|々\"\n  \n  other_characters &lt;- paste(c(letters_lower, letters_upper, numbers, symbols), collapse = \"|\")\n  \n  sentences_list &lt;- data_sentences %&gt;% \n    mutate(only_kanji = str_remove_all(sentence, paste(kana, other_characters, sep = \"|\")),\n           unknown_kanji = str_remove_all(only_kanji, kanji_list)) %&gt;% \n    filter(unknown_kanji == \"\", grepl(kanji_list, sentence)) %&gt;% \n    select(sentence, sentence_hiragana, sentence_meaning)\n  \n  sentences_list\n  \n}\n\n\nThis code defines two functions: make_study_list and make_sentences_list. Let’s break down what each function does:\n\nmake_study_list function:\n\n\nInput: kanji_learning is a vector containing the kanji characters that the user wants to learn.\nOutput: study_words is a data frame containing words from data_all dataset that contain only the kanji characters specified in kanji_learning.\nFunctionality:\n\nFilters the data_all dataset to select words that contain only the kanji characters specified in kanji_learning.\nThis is done by filtering the rows where each kanji column (kanji_1, kanji_2, kanji_3, kanji_4, kanji_5) matches one of the kanji characters in kanji_learning, or where the kanji column is NA (missing).\nRemoves the kanji_ columns from the resulting dataset.\nReturns the filtered dataset containing only the selected words.\n\n\n\nmake_sentences_list function:\n\n\nInput: kanji_learning is a vector containing the kanji characters that the user wants to learn.\nOutput: sentences_list is a data frame containing sentences from data_sentences dataset that contain kanji characters specified in kanji_learning.\nFunctionality:\n\nConcatenates the kanji characters in kanji_learning into a single regular expression pattern kanji_list.\nDefines regular expression patterns for lowercase letters, uppercase letters, numbers, and symbols.\nUses these patterns to define other_characters, which includes all characters that are not kanji.\nFor each sentence in data_sentences, removes all characters that are not kanji, then removes all kanji characters that are not in kanji_learning.\nFilters the sentences to keep only those that contain kanji characters specified in kanji_learning.\nSelects the filtered sentences along with their corresponding hiragana representation and meaning.\n\nNote: The str_remove_all function from the stringr package is used to remove all occurrences of a pattern from a string.\n\nThese functions are designed to help users generate study lists of words and sentences containing specific kanji characters they want to learn."
  },
  {
    "objectID": "posts/tips_and_tricks_r4ds/post.html",
    "href": "posts/tips_and_tricks_r4ds/post.html",
    "title": "Useful RStudio Shortcuts",
    "section": "",
    "text": "I recently bought the acclaimed book R for Data Science”(2nd ed.) by Hadley Wichkam et al., and I’ve been finding interesting tidbits that I’d like to share. Most of them are quality-of-life tools that I wasn’t aware of, while others are simply cool tricks.\nNote: These shortcuts works for RStudio on Windows. On macOS, replace “Ctrl” with “Cmd”."
  },
  {
    "objectID": "posts/tips_and_tricks_r4ds/post.html#what-is-an-alluvial-diagram",
    "href": "posts/tips_and_tricks_r4ds/post.html#what-is-an-alluvial-diagram",
    "title": "Tips and tricks from R for Data Science",
    "section": "",
    "text": "An alluvial diagram or plot displays the flow of information from one categorical variable or stage to the next. The term “alluvial” refers to its resemblance to the flow of a river."
  },
  {
    "objectID": "posts/tips_and_tricks_r4ds/post.html#uses-of-alluvial-diagrams",
    "href": "posts/tips_and_tricks_r4ds/post.html#uses-of-alluvial-diagrams",
    "title": "Tips and tricks from R for Data Science",
    "section": "Uses of Alluvial Diagrams",
    "text": "Uses of Alluvial Diagrams\nAlluvial diagrams are commonly used in the following situations:\n\nTo demonstrate the number of participants in a study transitioning from one baseline category to a subsequent category. For example, the number of participants randomized at the beginning of the study and who continue to follow-up at the end. It serves as a graphical supplement to flowcharts, which present information with arrows and boxes containing text and numbers.\nTo show the distribution of participants among different categorical variables. In this case, it provides a more “visually appealing” alternative to stacked column charts."
  },
  {
    "objectID": "posts/tips_and_tricks_r4ds/post.html#preparativos",
    "href": "posts/tips_and_tricks_r4ds/post.html#preparativos",
    "title": "Tips and tricks from R for Data Science",
    "section": "Preparativos",
    "text": "Preparativos\nFor this example, we will use the ggalluvial and ggplot2 libraries. The data come from a study on language learning using the web application LingQ (Link to the article).\n\n\nShow the code\n# Load necessary libraries\nlibrary(tidyverse) # Includes ggplot2\nlibrary(ggalluvial)\nlibrary(knitr)\n\n# Input data\n\ndata &lt;- data.frame(\n  Initial = rep(c(\"First\", \"Second\", \"Third\", \"Fourth+\"), each = 4),\n  Final = rep(c(\"First\", \"Second\", \"Third\", \"Fourth+\"), 4), \n  Frequency = c(28, 28, 10, 4, 3, 6, 7, 5,  0, 3, 3, 3, 0, 0, 0, 1)\n) %&gt;% \n  mutate(\n    Initial = factor(Initial, levels = c(\"First\", \"Second\", \"Third\", \"Fourth+\")),\n    Final = factor(Final, levels = c(\"First\", \"Second\", \"Third\", \"Fourth+\"))\n  )\n\nhead(data)\n\n\n  Initial   Final Frequency\n1   First   First        28\n2   First  Second        28\n3   First   Third        10\n4   First Fourth+         4\n5  Second   First         3\n6  Second  Second         6\n\n\nIn this study, a total of 192 individuals were selected for the sample and completed the initial knowledge test. Out of these, 101 completed the final test and studied for at least two hours in the application, and were included in the analysis. The data show the equivalent language knowledge in formal education semesters (“First”, “Second”, “Third”, “Fourth or more”) at the beginning of the study (“Initial”) and after using the application (“Final”).\nNote: The data have been extracted from the example article from the result tables and are just an example of a data distribution compatible with those results."
  },
  {
    "objectID": "posts/tips_and_tricks_r4ds/post.html#alluvial-plot",
    "href": "posts/tips_and_tricks_r4ds/post.html#alluvial-plot",
    "title": "Tips and tricks from R for Data Science",
    "section": "Alluvial Plot",
    "text": "Alluvial Plot\n\n\nShow the code\ndata %&gt;% \nggplot(aes(axis1 = fct_rev(Initial), axis2 = fct_rev(Final),\n           y = Frequency)) +\n  scale_x_discrete(limits = c(\"Initial\", \"Final\"), expand = c(.2, .05)) +\n  geom_alluvium(aes(fill = Final)) +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  theme_minimal() +\n  labs(x = \"Equivalent formal education semesters\")\n\n\n\n\n\nIn the alluvial diagram, the area of the colored zones is proportional to the frequency, so wider “flows” represent more participants.\nThe graph shows how the majority of the flow occurs from bottom to top; that is, knowledge tends to improve more than deteriorate. For example, more than half of the participants who started from the first semester of knowledge progressed to the second, third, or fourth semester.\nIt can also be seen that almost all participants who ended with a knowledge level equivalent to the first semester started from that previous level, and a few decreased from the second semester to the first semester.\nIn conclusion, alluvial diagrams provide a visual option for better understanding data flows between categories."
  },
  {
    "objectID": "posts/outbreak_report/post.html",
    "href": "posts/outbreak_report/post.html",
    "title": "COVID-19 Outbreak Report: An Example Using R",
    "section": "",
    "text": "Show the code\n# install the latest version of the Epi R Handbook package\npacman::p_install_gh(\"appliedepi/epirhandbook\")\n# load the package for use\npacman::p_load(rio, \n               tidyverse, \n               here, \n               epirhandbook, \n               openxlsx, \n               janitor, \n               skimr,\n               epikit)\n\n\n\n\nShow the code\n# Calculating age from two dates\n\ncalculate_age &lt;- function(from, to) {\n  \n  # Convert dates to POSIXlt format\n  from_lt = as.POSIXlt(from)\n  to_lt = as.POSIXlt(to)\n  \n  # Get number of years of difference\n  age = to_lt$year - from_lt$year\n  \n  # Check if day and month in \"to\" date is later than in \"from\" date\n  ifelse(to_lt$mon &lt; from_lt$mon |\n           (to_lt$mon == from_lt$mon & to_lt$mday &lt; from_lt$mday),\n         age - 1, age)\n}\n\n\n\n\nShow the code\n# Download file ####\n\nfile_url &lt;- \"https://raw.githubusercontent.com/appliedepi/epiRhandbook_eng/master/data/covid_example_data/covid_example_data.xlsx\"\ndestination &lt;- here(\"posts\", \"outbreak_report\", \"data\", \"outbreak_data.xlsx\")\ndownload.file(file_url, destination, mode = \"wb\")\n\n\n# Load and clean file ####\n\ndata &lt;- import(here(\"posts\", \"outbreak_report\", \"data\", \"outbreak_data.xlsx\")) %&gt;% \n  \n  # Standardize variable names\n  clean_names() %&gt;%                     \n  \n  # Remove ending \"_false\" in variable names\n  rename_with(~ gsub(\"_false\",\"\", .x)) %&gt;% \n  \n  # De-duplicate\n  distinct() %&gt;%        #  (0 duplicated rows)                   \n  \n  mutate(\n   # Convert all \"Unk\" and \"Unknown\" in character variables to NA \n     across(where(is.character) & !pid,\n           ~ case_when(\n             .x %in% c(\"Unk\", \"Unknown\", \"Unknown symptom status\", \"UNKNOWN\", \"NOT SPECIFIED\", \"Under Review\", \"Pending\") ~ NA,\n             TRUE ~ .x)\n           ),                                                \n    \n    # Simplify options for some variables\n    sym_resolved = case_when(\n      sym_resolved %in% c(\"Yes, date specified below\", \"Yes, date unknown\") ~ \"Yes\",  \n      sym_resolved %in% c(\"No, still symptomatic\") ~ \"No\"),\n    \n    contact_id = case_when(\n      contact_id %in% c(\"Yes-Symptomatic\") ~ \"Yes\", \n      contact_id %in% c(\"No-Asymptomatic\") ~ \"No\"),\n    \n    # Replace \"YES\" for \"Yes\"\n    sym_myalgia = case_when(\n      sym_myalgia == \"YES\"~ \"Yes\",\n      TRUE ~ sym_myalgia),  \n    \n    # Convert all character variables (except pid) into factors\n    across(where(is.character) & !pid,\n           ~ factor(.x)),  \n    \n    # Standardize answer order in all yes-no variables: first \"Yes\", then \"No\"\n    across(where(is.factor) & !c(case_gender, case_race, case_eth, covid_dx),\n           ~ factor(.x, levels = c(\"Yes\", \"No\"))),\n\n    # Clean dates\n    \n    # Some dates of birth are later than the dates of report creation, which is impossible\n    case_dob = case_when(case_dob &gt; reprt_creationdt ~ NA,\n                         TRUE ~ case_dob),\n    \n    \n    # Recalculate ages with custom function which is more accurate\n    age_recalc = calculate_age(from = case_dob, to = reprt_creationdt)\n    # # Create age groups\n    # age_cat = age_categories()\n\n    \n    \n        )\n\n# To-Do\n# Clean symptoms start and end dates, report date, hospital_admision date, hospital discharche,\n# death date\n\n\n\n\nShow the code\ndata %&gt;% \n  glimpse()\n\n\nRows: 82,101\nColumns: 32\n$ pid                &lt;chr&gt; \"3a85e6992a5ac52f\", \"c6b5281d5fc50b96\", \"53495ad0dc…\n$ reprt_creationdt   &lt;dttm&gt; 2020-03-22, 2020-02-01, 2020-02-10, 2020-03-20, 20…\n$ case_dob           &lt;dttm&gt; 2004-11-08, 1964-06-07, 1944-04-06, 1964-06-25, 19…\n$ case_age           &lt;dbl&gt; 16, 57, 77, 57, 56, 65, 47, 61, 36, 42, 74, 27, 80,…\n$ case_gender        &lt;fct&gt; Male, Male, Female, Female, Male, Male, Male, Femal…\n$ case_race          &lt;fct&gt; WHITE, WHITE, BLACK, BLACK, WHITE, BLACK, BLACK, BL…\n$ case_eth           &lt;fct&gt; NON-HISPANIC/LATINO, NON-HISPANIC/LATINO, NON-HISPA…\n$ case_zip           &lt;dbl&gt; 30308, 30308, 30315, 30213, 30004, 30314, 30313, 30…\n$ contact_id         &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Y…\n$ sym_startdt        &lt;dttm&gt; 2020-03-20, 2020-01-28, 2020-02-10, 2021-05-19, 20…\n$ sym_fever          &lt;fct&gt; Yes, No, Yes, No, Yes, Yes, No, Yes, No, Yes, NA, N…\n$ sym_subjfever      &lt;fct&gt; Yes, No, NA, Yes, Yes, Yes, No, Yes, Yes, No, NA, Y…\n$ sym_myalgia        &lt;fct&gt; No, Yes, Yes, Yes, Yes, No, NA, No, No, Yes, NA, No…\n$ sym_losstastesmell &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, Yes, NA, NA, NA…\n$ sym_sorethroat     &lt;fct&gt; Yes, No, Yes, Yes, No, NA, Yes, Yes, Yes, Yes, NA, …\n$ sym_cough          &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Y…\n$ sym_headache       &lt;fct&gt; Yes, No, NA, Yes, No, NA, Yes, No, Yes, No, NA, No,…\n$ sym_resolved       &lt;fct&gt; No, No, No, No, No, Yes, No, No, No, No, No, No, Ye…\n$ sym_resolveddt     &lt;dttm&gt; NA, NA, NA, NA, NA, 2020-02-21, NA, NA, NA, NA, NA…\n$ contact_household  &lt;fct&gt; Yes, No, NA, No, No, No, No, No, No, Yes, No, No, Y…\n$ hospitalized       &lt;fct&gt; No, No, Yes, NA, Yes, Yes, Yes, No, No, Yes, NA, No…\n$ hosp_admidt        &lt;dttm&gt; NA, NA, 2020-02-08, NA, 2020-02-26, 2020-01-27, 20…\n$ hosp_dischdt       &lt;dttm&gt; NA, NA, NA, NA, NA, 2020-02-21, NA, NA, NA, 2020-1…\n$ died               &lt;fct&gt; No, No, No, No, NA, Yes, No, NA, No, No, NA, No, Ye…\n$ died_covid         &lt;fct&gt; No, No, No, No, NA, Yes, No, NA, No, No, NA, No, Ye…\n$ died_dt            &lt;dttm&gt; NA, NA, NA, NA, NA, 2020-02-21, NA, NA, NA, NA, NA…\n$ confirmed_case     &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Y…\n$ covid_dx           &lt;fct&gt; Confirmed, Confirmed, Confirmed, Confirmed, Confirm…\n$ pos_sampledt       &lt;dttm&gt; 2020-03-22, 2020-02-01, 2020-02-10, 2021-01-17, 20…\n$ latitude_jitt      &lt;dbl&gt; 33.776645460, 33.780510140, 33.730233310, 33.555067…\n$ longitude_jitt     &lt;dbl&gt; -84.385685230, -84.389474740, -84.384251890, -84.62…\n$ age_recalc         &lt;dbl&gt; 15, 55, 75, 55, 55, 63, 45, 60, 34, 41, 73, 26, 78,…\n\n\nShow the code\ndata %&gt;% \n  skim()\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n82101\n\n\nNumber of columns\n32\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n18\n\n\nnumeric\n5\n\n\nPOSIXct\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npid\n0\n1\n8\n22\n0\n82087\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ncase_gender\n409\n1.00\nFALSE\n2\nFem: 43299, Mal: 38393\n\n\ncase_race\n6353\n0.92\nFALSE\n6\nBLA: 35048, WHI: 31599, OTH: 5863, ASI: 3075\n\n\ncase_eth\n10799\n0.87\nFALSE\n2\nNON: 62677, HIS: 8625\n\n\ncontact_id\n33298\n0.59\nFALSE\n2\nYes: 41282, No: 7521\n\n\nsym_fever\n33023\n0.60\nFALSE\n2\nNo: 33951, Yes: 15127\n\n\nsym_subjfever\n38932\n0.53\nFALSE\n2\nNo: 30457, Yes: 12712\n\n\nsym_myalgia\n33357\n0.59\nFALSE\n2\nNo: 29210, Yes: 19534\n\n\nsym_losstastesmell\n51258\n0.38\nFALSE\n2\nNo: 18109, Yes: 12734\n\n\nsym_sorethroat\n33479\n0.59\nFALSE\n2\nNo: 36106, Yes: 12516\n\n\nsym_cough\n32684\n0.60\nFALSE\n2\nNo: 27474, Yes: 21943\n\n\nsym_headache\n33230\n0.60\nFALSE\n2\nNo: 27196, Yes: 21675\n\n\nsym_resolved\n44370\n0.46\nFALSE\n2\nYes: 23265, No: 14466\n\n\ncontact_household\n42003\n0.49\nFALSE\n2\nNo: 28500, Yes: 11598\n\n\nhospitalized\n32482\n0.60\nFALSE\n2\nNo: 44322, Yes: 5297\n\n\ndied\n36990\n0.55\nFALSE\n2\nNo: 43407, Yes: 1704\n\n\ndied_covid\n42425\n0.48\nFALSE\n2\nNo: 38338, Yes: 1338\n\n\nconfirmed_case\n13\n1.00\nFALSE\n2\nYes: 82062, No: 26\n\n\ncovid_dx\n0\n1.00\nFALSE\n1\nCon: 82101\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncase_age\n48\n1\n39.69\n19.16\n-20.0\n25.00\n37.00\n53.00\n106.00\n▁▇▇▃▁\n\n\ncase_zip\n13\n1\n30249.72\n123.83\n30000.0\n30213.00\n30312.00\n30331.00\n31707.00\n▇▂▁▁▁\n\n\nlatitude_jitt\n94\n1\n32.58\n6.38\n0.0\n33.69\n33.78\n33.98\n34.19\n▁▁▁▁▇\n\n\nlongitude_jitt\n200\n1\n-81.39\n15.65\n-84.8\n-84.46\n-84.38\n-84.35\n0.00\n▇▁▁▁▁\n\n\nage_recalc\n125\n1\n39.02\n19.09\n0.0\n25.00\n36.00\n52.00\n105.00\n▃▇▅▂▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreprt_creationdt\n0\n1.00\n2019-12-27\n2021-07-27\n2020-12-04\n533\n\n\ncase_dob\n125\n1.00\n1914-12-28\n2021-05-13\n1984-05-03\n26713\n\n\nsym_startdt\n37480\n0.54\n1941-03-08\n2030-12-03\n2020-11-22\n601\n\n\nsym_resolveddt\n65799\n0.20\n2001-11-08\n2102-02-03\n2020-11-27\n612\n\n\nhosp_admidt\n77115\n0.06\n2019-12-26\n2032-05-22\n2020-10-14\n517\n\n\nhosp_dischdt\n78600\n0.04\n2019-12-04\n2029-12-29\n2020-10-24\n504\n\n\ndied_dt\n80394\n0.02\n2020-02-21\n2021-07-01\n2020-12-11\n429\n\n\npos_sampledt\n122\n1.00\n2020-02-01\n2021-07-23\n2020-12-03\n526\n\n\n\n\n\nShow the code\nggplot(data, aes(x = case_age, y = age_recalc)) + geom_point()\n\n\n\n\n\nShow the code\ncor(data$case_age, data$age_recalc, use = \"complete.obs\")\n\n\n[1] 0.9996529\n\n\nShow the code\ndata %&gt;% \n  filter(case_age &lt; 0 ) %&gt;% \n  View()\n\ndata %&gt;% \n  filter(sym_startdt &gt; sym_resolveddt) %&gt;% \n  arrange(desc(sym_startdt)) %&gt;% \n  select(case_dob, reprt_creationdt, case_age, age_recalc, sym_startdt, sym_resolveddt)\n\n\n      case_dob reprt_creationdt case_age age_recalc sym_startdt sym_resolveddt\n1   1973-12-12       2021-03-19       47         47  2030-12-03     2020-12-28\n2   1949-02-08       2020-12-09       72         71  2028-12-09     2020-12-15\n3   1950-07-26       2020-12-05       71         70  2021-06-01     2020-12-14\n4   1986-09-11       2021-05-27       34         34  2021-05-24     2021-01-11\n5   1990-07-10       2021-05-30       31         30  2021-05-21     2021-05-02\n6   1985-10-17       2021-01-16       35         35  2021-05-13     2021-01-18\n7   2005-03-03       2021-05-14       16         16  2021-05-06     2021-03-14\n8   2007-03-11       2021-05-08       14         14  2021-05-03     2020-05-11\n9   1993-02-28       2021-04-19       28         28  2021-04-22     2021-04-16\n10  2002-08-06       2021-04-07       18         18  2021-04-05     2021-03-30\n11  1979-03-17       2021-04-11       42         42  2021-04-01     2021-03-12\n12  1994-06-29       2021-04-14       27         26  2021-04-01     2021-02-12\n13  1968-03-07       2021-03-23       53         53  2021-03-18     2021-03-10\n14  2003-02-15       2021-03-11       18         18  2021-03-13     2020-03-15\n15  1998-08-01       2021-03-17       22         22  2021-03-13     2020-03-25\n16  2006-09-07       2021-03-15       14         14  2021-03-11     2021-03-09\n17  1979-01-09       2021-03-10       42         42  2021-03-08     2020-03-08\n18  1996-04-07       2021-03-16       25         24  2021-03-07     2020-03-15\n19  1986-10-30       2021-03-09       34         34  2021-03-07     2021-01-07\n20  1987-02-10       2021-03-01       34         34  2021-02-26     2021-02-24\n21  1997-08-05       2021-03-07       23         23  2021-02-26     2020-03-04\n22  2013-01-23       2021-03-02        8          8  2021-02-21     2021-02-20\n23  2003-05-17       2021-01-21       18         17  2021-02-18     2021-01-26\n24  1955-05-25       2021-02-26       66         65  2021-02-18     2020-02-27\n25  1989-03-25       2021-03-10       32         31  2021-02-17     2020-12-25\n26  1943-05-11       2021-02-19       78         77  2021-02-16     2021-02-15\n27  1984-05-20       2021-02-18       37         36  2021-02-15     2018-02-16\n28  1969-11-21       2021-02-13       51         51  2021-02-12     2021-01-22\n29  1997-12-28       2021-02-16       23         23  2021-02-11     2021-01-17\n30  1990-11-18       2021-02-21       30         30  2021-02-10     2021-02-08\n31  1993-07-02       2021-01-13       28         27  2021-02-09     2021-01-18\n32  2007-09-02       2021-02-03       13         13  2021-02-05     2021-02-01\n33  2020-01-03       2020-04-10        1          0  2021-02-02     2012-02-04\n34  1983-01-11       2021-02-08       38         38  2021-02-02     2020-02-17\n35  1981-02-13       2021-04-17       40         40  2021-02-02     2021-01-15\n36  1984-04-14       2021-01-03       37         36  2021-01-29     2021-01-12\n37  1962-02-07       2021-01-31       59         58  2021-01-27     2020-01-31\n38  2001-06-20       2021-01-26       20         19  2021-01-26     2020-01-27\n39  1984-01-14       2021-01-23       37         37  2021-01-25     2020-01-29\n40  1950-01-14       2021-01-27       71         71  2021-01-23     2020-01-28\n41  2005-09-18       2021-01-29       15         15  2021-01-22     2020-01-24\n42  1972-05-24       2021-02-02       49         48  2021-01-22     2020-01-30\n43  1994-01-26       2021-01-31       27         27  2021-01-22     2021-01-01\n44  1940-05-24       2021-02-05       81         80  2021-01-21     2021-01-17\n45  1986-05-28       2021-03-02       35         34  2021-01-20     2020-02-22\n46  1980-10-11       2021-01-18       40         40  2021-01-20     2021-01-18\n47  1984-11-21       2020-09-06       36         35  2021-01-19     2020-01-28\n48  1988-11-26       2021-01-28       32         32  2021-01-19     2020-12-26\n49  1944-10-01       2020-12-22       76         76  2021-01-17     2020-12-29\n50  1997-04-08       2021-01-16       24         23  2021-01-17     2021-01-16\n51  1992-02-03       2021-01-30       29         28  2021-01-17     2020-12-30\n52  1988-04-22       2021-01-26       33         32  2021-01-16     2020-01-23\n53  1926-10-21       2021-01-12       94         94  2021-01-15     2021-01-13\n54  1962-03-06       2021-01-25       59         58  2021-01-15     2014-02-04\n55  1988-12-28       2021-01-16       32         32  2021-01-15     2020-01-18\n56  1963-09-01       2021-01-26       57         57  2021-01-14     2020-01-23\n57  2001-04-24       2021-01-20       20         19  2021-01-14     2020-01-24\n58  1985-04-15       2021-01-15       36         35  2021-01-14     2020-01-21\n59  1979-04-20       2021-06-15       42         42  2021-01-14     2020-01-16\n60  1990-09-30       2021-01-19       30         30  2021-01-13     2020-02-07\n61  1948-06-27       2021-01-16       73         72  2021-01-12     2020-02-03\n62  2005-10-05       2021-01-25       15         15  2021-01-12     2020-01-26\n63  1986-01-23       2021-01-20       35         34  2021-01-11     2020-01-22\n64  1998-12-15       2021-01-22       22         22  2021-01-11     2020-01-25\n65  1984-08-24       2021-01-04       36         36  2021-01-10     2021-01-02\n66  1964-04-11       2021-01-17       57         56  2021-01-06     2020-01-23\n67  1985-04-22       2021-01-10       36         35  2021-01-06     2020-12-29\n68  1980-09-01       2021-01-06       40         40  2021-01-05     2020-01-07\n69  1970-05-01       2021-01-11       51         50  2021-01-05     2020-01-10\n70  1965-03-02       2021-01-11       56         55  2021-01-05     2020-01-10\n71  1980-10-18       2021-01-08       40         40  2021-01-05     2020-01-09\n72  1989-12-16       2021-01-12       31         31  2021-01-04     2020-01-17\n73  1981-01-03       2021-01-08       40         40  2021-01-03     2020-01-13\n74  1938-01-25       2021-01-08       83         82  2021-01-03     2020-01-17\n75  1979-12-12       2021-01-19       41         41  2021-01-02     2020-12-19\n76  1982-08-09       2021-01-02       38         38  2020-12-31     2020-01-04\n77  1979-03-04       2021-01-03       42         41  2020-12-30     2020-12-21\n78  1976-09-15       2020-12-02       44         44  2020-12-28     2020-12-14\n79  1956-02-14       2021-01-09       65         64  2020-12-28     2020-01-19\n80  1996-04-05       2020-12-31       25         24  2020-12-27     2020-01-14\n81  1971-10-25       2020-12-31       49         49  2020-12-27     2020-01-14\n82  1982-03-23       2021-01-01       39         38  2020-12-26     2020-01-05\n83  1976-04-06       2020-12-31       45         44  2020-12-25     2020-12-21\n84  1967-02-22       2021-01-05       54         53  2020-12-25     2020-12-24\n85  1959-03-02       2020-12-23       62         61  2020-12-24     2020-11-08\n86  1981-05-23       2020-12-30       40         39  2020-12-24     2019-12-29\n87  1950-09-07       2020-12-29       70         70  2020-12-22     2014-01-06\n88  1988-08-03       2020-12-31       32         32  2020-12-22     2020-10-25\n89  1997-09-29       2020-12-21       23         23  2020-12-21     2020-12-20\n90  1955-05-25       2020-12-23       66         65  2020-12-20     2019-12-22\n91  2011-05-21       2021-01-04       10          9  2020-12-16     2020-01-04\n92  1973-05-10       2020-12-22       48         47  2020-12-16     2019-12-21\n93  1972-05-15       2020-12-16       49         48  2020-12-14     2020-11-30\n94  1965-03-20       2020-12-22       56         55  2020-12-14     2020-11-27\n95  1963-03-21       2020-12-20       58         57  2020-12-14     2019-12-19\n96  1971-09-01       2020-07-21       49         48  2020-12-12     2020-08-14\n97  1969-01-05       2020-12-17       52         51  2020-12-12     2019-12-20\n98  1993-07-24       2020-12-17       27         27  2020-12-08     2019-12-26\n99  2004-01-26       2020-12-15       17         16  2020-12-07     2019-12-12\n100 1945-08-28       2020-12-12       75         75  2020-12-06     2019-12-14\n101 1966-05-03       2020-12-20       55         54  2020-12-02     2019-12-15\n102 1993-01-02       2020-11-29       28         27  2020-12-01     2020-11-28\n103 1954-10-22       2020-12-16       66         66  2020-12-01     2019-12-17\n104 1990-12-14       2020-12-05       30         29  2020-11-27     2020-11-11\n105 2002-10-13       2020-12-03       18         18  2020-11-27     2020-07-31\n106 2000-03-03       2020-12-03       21         20  2020-11-26     2019-12-06\n107 1946-07-18       2020-11-22       75         74  2020-11-24     2020-11-11\n108 1972-06-19       2021-05-12       49         48  2020-11-21     2019-12-16\n109 1995-01-27       2020-11-23       26         25  2020-11-19     2020-10-24\n110 2008-11-01       2020-11-21       12         12  2020-11-17     2020-02-17\n111 1976-01-09       2020-07-26       45         44  2020-11-15     2020-07-29\n112 1934-06-18       2020-11-18       87         86  2020-11-14     2020-11-08\n113 1984-04-23       2020-11-16       37         36  2020-11-11     2020-10-23\n114 1963-07-27       2020-11-19       58         57  2020-11-11     2020-10-24\n115 2003-10-09       2020-11-20       17         17  2020-11-08     2020-11-07\n116 1986-02-08       2020-11-08       35         34  2020-11-05     2020-11-04\n117 1995-06-05       2020-11-06       26         25  2020-10-31     2001-11-08\n118 1986-02-28       2020-11-07       35         34  2020-10-27     2020-10-09\n119 1984-08-04       2020-04-22       36         35  2020-10-19     2020-05-11\n120 1983-03-07       2020-10-12       38         37  2020-10-10     2020-10-09\n121 1991-10-21       2020-07-22       29         28  2020-10-09     2020-07-22\n122 2003-05-04       2020-10-18       18         17  2020-10-09     2020-09-29\n123 2002-06-20       2020-07-24       19         18  2020-10-04     2020-07-30\n124 1990-02-26       2020-10-20       31         30  2020-10-03     2002-10-23\n125 1992-11-21       2020-10-06       28         27  2020-09-29     2020-09-13\n126 1993-10-15       2020-09-30       27         26  2020-09-28     2020-08-01\n127 1998-07-09       2020-09-24       23         22  2020-09-27     2020-09-21\n128 1973-05-05       2020-09-21       48         47  2020-09-20     2020-09-14\n129 1982-08-23       2020-09-02       38         38  2020-09-14     2020-08-29\n130 1996-05-02       2020-09-17       25         24  2020-09-13     2020-09-12\n131 1998-03-27       2020-07-13       23         22  2020-09-09     2020-07-18\n132 1998-11-10       2020-09-16       22         21  2020-09-07     2020-07-11\n133 1981-06-20       2020-09-08       40         39  2020-09-04     2020-08-27\n134 1975-08-22       2020-08-10       45         44  2020-08-25     2020-08-06\n135 1986-09-08       2020-09-01       34         33  2020-08-25     2020-07-05\n136 2002-10-09       2020-10-07       18         17  2020-08-24     2020-08-07\n137 1937-09-11       2020-09-01       83         82  2020-08-22     2020-07-04\n138 1995-01-09       2020-08-24       26         25  2020-08-19     2020-06-19\n139 1991-03-28       2020-07-21       30         29  2020-08-17     2020-07-20\n140 1995-03-14       2020-08-16       26         25  2020-08-15     2020-08-13\n141 1997-03-18       2020-09-05       24         23  2020-08-15     2020-07-30\n142 1975-02-15       2020-08-16       46         45  2020-08-14     2020-07-25\n143 1968-01-14       2020-08-22       53         52  2020-08-13     2020-08-01\n144 1991-07-13       2020-08-04       30         29  2020-08-12     2020-07-21\n145 1985-10-24       2020-07-22       35         34  2020-08-11     2020-07-17\n146 1992-11-12       2020-07-16       28         27  2020-08-06     2020-07-16\n147 1951-04-11       2020-08-15       70         69  2020-08-05     2020-05-07\n148 2017-11-21       2020-08-15        3          2  2020-08-01     2020-07-12\n149 1986-01-19       2020-07-16       35         34  2020-08-01     2020-07-13\n150 1993-01-28       2020-08-04       28         27  2020-08-01     2020-07-30\n151 2002-01-28       2020-07-15       19         18  2020-07-31     2020-07-18\n152 1980-12-22       2020-08-04       40         39  2020-07-31     2020-07-29\n153 1994-07-05       2020-08-05       27         26  2020-07-29     2020-07-06\n154 1994-10-27       2020-08-01       26         25  2020-07-28     2020-07-10\n155 1996-02-21       2020-07-27       25         24  2020-07-28     2020-07-23\n156 1993-05-07       2020-07-11       28         27  2020-07-27     2020-07-01\n157 1992-11-10       2020-08-11       28         27  2020-07-26     2020-07-15\n158 1987-07-01       2020-07-16       34         33  2020-07-25     2020-06-29\n159 1978-01-24       2020-08-07       43         42  2020-07-25     2020-02-10\n160 2006-02-15       2020-08-04       15         14  2020-07-23     2020-06-30\n161 1947-10-12       2020-07-21       73         72  2020-07-22     2020-07-17\n162 1987-07-31       2020-07-24       33         32  2020-07-21     2020-06-23\n163 1981-04-11       2020-07-16       40         39  2020-07-20     2020-06-30\n164 1985-11-03       2020-06-30       35         34  2020-07-18     2020-07-03\n165 1992-05-08       2020-07-30       29         28  2020-07-17     2020-07-01\n166 1969-03-09       2020-07-10       52         51  2020-07-17     2020-06-24\n167 1980-06-06       2020-07-15       41         40  2020-07-17     2020-07-15\n168 1972-07-03       2020-07-18       49         48  2020-07-16     2020-06-30\n169 2007-02-06       2020-07-18       14         13  2020-07-15     2020-07-14\n170 1990-07-19       2020-07-14       31         29  2020-07-14     2020-06-26\n171 1989-06-07       2020-07-14       32         31  2020-07-11     2020-06-16\n172 1988-06-08       2020-07-12       33         32  2020-07-10     2020-06-22\n173 2006-11-24       2020-06-17       14         13  2020-07-09     2020-06-15\n174 1994-05-27       2020-07-24       27         26  2020-07-09     2020-01-23\n175 1977-01-15       2020-07-17       44         43  2020-07-09     2012-07-05\n176 2005-03-09       2020-07-11       16         15  2020-07-08     2020-07-06\n177 1958-11-12       2020-07-27       62         61  2020-07-07     2020-06-13\n178 1988-01-29       2020-07-30       33         32  2020-07-07     2019-12-19\n179 1969-03-09       2020-07-05       52         51  2020-06-30     2002-07-03\n180 1971-05-06       2020-06-19       50         49  2020-06-30     2020-06-24\n181 1983-12-05       2020-06-29       37         36  2020-06-26     2020-06-08\n182 1994-12-07       2020-07-15       26         25  2020-06-25     2020-06-13\n183 1971-11-17       2020-07-23       49         48  2020-06-25     2020-06-09\n184 2001-01-29       2020-06-27       20         19  2020-06-24     2020-06-23\n185 1987-12-04       2020-06-06       33         32  2020-06-20     2020-06-03\n186 2000-12-17       2020-07-02       20         19  2020-06-20     2020-06-11\n187 2004-03-26       2020-06-22       17         16  2020-06-19     2020-05-23\n188 2008-03-01       2020-07-04       13         12  2020-06-18     2020-06-16\n189 1977-05-06       2020-06-26       44         43  2020-06-12     2020-03-25\n190 1990-05-02       2020-06-24       31         30  2020-06-08     2020-05-29\n191 1967-01-15       2020-06-18       54         53  2020-06-07     2020-03-10\n192 1979-02-11       2020-06-22       42         41  2020-06-05     2020-06-04\n193 2006-11-09       2020-06-19       14         13  2020-06-05     2020-05-20\n194 1957-06-28       2020-06-05       64         62  2020-06-01     2020-05-30\n195 1983-01-06       2020-06-03       38         37  2020-05-30     2020-05-04\n196 1993-04-06       2020-05-31       28         27  2020-05-28     2010-06-19\n197 1995-07-29       2020-06-01       25         24  2020-05-25     2020-05-23\n198 1987-02-27       2020-05-05       34         33  2020-04-26     2020-04-01\n199 2003-06-17       2020-05-09       18         16  2020-04-25     2020-04-06\n200 1986-07-12       2020-03-26       35         33  2020-03-12     2020-03-03\n201 1971-10-31       2020-03-11       49         48  2020-03-09     2020-03-07\n202 1983-12-13       2020-03-18       37         36  2020-03-06     2020-02-12"
  },
  {
    "objectID": "posts/tips_and_tricks_r4ds/post.html#alt----",
    "href": "posts/tips_and_tricks_r4ds/post.html#alt----",
    "title": "Useful RStudio Shortcuts",
    "section": "Alt + - : <-",
    "text": "Alt + - : &lt;-\nHere I’ve used the first of the shortcuts, believe it or not: Alt + - to create the assignment arrow &lt;-. Instead of having to type ““, then”&lt;“, then”-“, then”” (4 keys in total), now just with Alt + - (2 keys simultaneously) I can get the job done, because spacing is added automatically. A silly little trick, but very useful, especially when I’m working with a keyboard that doesn’t have the &lt; or &gt; keys in the same place as others."
  },
  {
    "objectID": "posts/tips_and_tricks_r4ds/post.html#ctrl-up-list-all-similar-commands-in-the-consoles-history",
    "href": "posts/tips_and_tricks_r4ds/post.html#ctrl-up-list-all-similar-commands-in-the-consoles-history",
    "title": "Useful RStudio Shortcuts",
    "section": "Ctrl + Up : List All Similar Commands in the Console’s History",
    "text": "Ctrl + Up : List All Similar Commands in the Console’s History\nThis I can’t show you with code, but with a screenshot of RStudio’s console. If you press Ctrl + Up arrow, you can see a list of previous commands that start with the same words. You can access commands that were typed even in previous R sessions working with different files. I find it useful to trace back to some command that I want to replicate.\n\n\n\nCtrl + Up lets you look at your Console’s history of similar commands."
  },
  {
    "objectID": "posts/tips_and_tricks_r4ds/post.html#f1-when-the-autocomplete-yellow-box-pops-up-access-the-help-file-of-that-function",
    "href": "posts/tips_and_tricks_r4ds/post.html#f1-when-the-autocomplete-yellow-box-pops-up-access-the-help-file-of-that-function",
    "title": "Useful RStudio Shortcuts",
    "section": "F1 When the Autocomplete Yellow Box Pops Up : Access the Help File of That Function",
    "text": "F1 When the Autocomplete Yellow Box Pops Up : Access the Help File of That Function\nHow many times have I tried to remember the specific syntax of a function and had to go back and forth between the actual code and typing help() or ?function ? There was an easier way: just press F1 When the little yellow box of autocomplete appears while typing the function. It makes the R Documentation file appear in the RStudio help panel, and I can look into it without even needing to stop from coding.\n\n\n\nF1 when the autocomplete yellow box pops up…\n\n\n\n\n\n…the R Documentation help file appears right away in RStudio."
  },
  {
    "objectID": "posts/tips_and_tricks_r4ds/post.html#ctrl-shift-r-create-sections-in-your-code",
    "href": "posts/tips_and_tricks_r4ds/post.html#ctrl-shift-r-create-sections-in-your-code",
    "title": "Useful RStudio Shortcuts",
    "section": "Ctrl + Shift + R: Create Sections in Your Code",
    "text": "Ctrl + Shift + R: Create Sections in Your Code\nOne way to organize your R code more neatly is to use sections and sub-sections. To create a section, start with a comment (# followed by text), and end with a sequence of four or more hyphens or hashtags (---- or ####). That will tell RStudio that that particular line of text is a section, and you can jump between sections using the R editor window.\nThe shortcut Ctrl + Shift + R pops up a window for you to enter the label of the section, and a new section appears, saving you the trouble of typing all those hyphens manually.\n\n\n\nCtrl + Shift + R makes creating sections easy and fast.\n\n\n\n\nShow the code\n# This is a section -------------------------------------------------------"
  },
  {
    "objectID": "posts/tips_and_tricks_r4ds/post.html#other-shortcuts",
    "href": "posts/tips_and_tricks_r4ds/post.html#other-shortcuts",
    "title": "Useful RStudio Shortcuts",
    "section": "Other Shortcuts",
    "text": "Other Shortcuts\nLastly, I’ll share some other useful shortcuts:\n\nCtrl + Shift + P: Open the “command palette” of RStudio, useful to find every command the IDE and its add-ins (like the “styler” package) have to offer, including their own shortcuts.\nCtrl + Shift + S: Runs the whole script.\nCtrl + Shift + F10: Restart the R session.\n\nThat’s all for now. See you next time!"
  },
  {
    "objectID": "posts/dplyr_count/post.html",
    "href": "posts/dplyr_count/post.html",
    "title": "Exploring the count() Function in R’s tidyverse",
    "section": "",
    "text": "R’s tidyverse is a collection of packages designed for data science. One of the most useful functions within this suite is count(), part of the dplyr package. This function is used to count occurrences of unique values within a dataset, which is crucial for data analysis and exploratory data analysis (EDA). In this blog post, we’ll delve into how count() works, particularly with the sort = TRUE option, and we’ll use datasets from the medicaldata package to illustrate its application."
  },
  {
    "objectID": "posts/dplyr_count/post.html#example-with-the-medicaldata-package",
    "href": "posts/dplyr_count/post.html#example-with-the-medicaldata-package",
    "title": "Exploring the count() Function in R’s tidyverse",
    "section": "Example with the medicaldata package",
    "text": "Example with the medicaldata package\nThe medicaldata package contains various datasets from medical research, which are great for demonstration. We’ll use the covid_testing dataset for our example.\nFirst, install and load the required packages:\n\n\nShow the code\n# install.packages(\"tidyverse\")\n# install.packages(\"medicaldata\")\n\nlibrary(tidyverse)\nlibrary(medicaldata)\n\n\nNext, load the covid_testing dataset and take a look at its structure:\n\n\nShow the code\ndata(\"covid_testing\")\nglimpse(covid_testing)\n\n\nRows: 15,524\nColumns: 17\n$ subject_id      &lt;dbl&gt; 1412, 533, 9134, 8518, 8967, 11048, 663, 2158, 3794, 4…\n$ fake_first_name &lt;chr&gt; \"jhezane\", \"penny\", \"grunt\", \"melisandre\", \"rolley\", \"…\n$ fake_last_name  &lt;chr&gt; \"westerling\", \"targaryen\", \"rivers\", \"swyft\", \"karstar…\n$ gender          &lt;chr&gt; \"female\", \"female\", \"male\", \"female\", \"male\", \"female\"…\n$ pan_day         &lt;dbl&gt; 4, 7, 7, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 1…\n$ test_id         &lt;chr&gt; \"covid\", \"covid\", \"covid\", \"covid\", \"covid\", \"covid\", …\n$ clinic_name     &lt;chr&gt; \"inpatient ward a\", \"clinical lab\", \"clinical lab\", \"c…\n$ result          &lt;chr&gt; \"negative\", \"negative\", \"negative\", \"negative\", \"negat…\n$ demo_group      &lt;chr&gt; \"patient\", \"patient\", \"patient\", \"patient\", \"patient\",…\n$ age             &lt;dbl&gt; 0.0, 0.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.0, 0.0, 0.9, 0.9,…\n$ drive_thru_ind  &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, …\n$ ct_result       &lt;dbl&gt; 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45…\n$ orderset        &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, …\n$ payor_group     &lt;chr&gt; \"government\", \"commercial\", NA, NA, \"government\", \"com…\n$ patient_class   &lt;chr&gt; \"inpatient\", \"not applicable\", NA, NA, \"emergency\", \"r…\n$ col_rec_tat     &lt;dbl&gt; 1.4, 2.3, 7.3, 5.8, 1.2, 1.4, 2.6, 0.7, 1.0, 7.1, 2.5,…\n$ rec_ver_tat     &lt;dbl&gt; 5.2, 5.8, 4.7, 5.0, 6.4, 7.0, 4.2, 6.3, 5.6, 7.0, 3.8,…\n\n\nThe covid_testing dataset contains data from deidentified results of COVID-19 testing at the Children’s Hospital of Pennsylvania (CHOP) in 2020. Suppose we want to count the number of participants according to the COVID-19 test result and sort the results by the count in descending order. Here’s how we can do that:\n\n\nShow the code\ncovid_testing %&gt;%\n  count(result, sort = TRUE)\n\n\n# A tibble: 3 × 2\n  result       n\n  &lt;chr&gt;    &lt;int&gt;\n1 negative 14358\n2 positive   865\n3 invalid    301"
  },
  {
    "objectID": "posts/dplyr_count/post.html#interpreting-the-results",
    "href": "posts/dplyr_count/post.html#interpreting-the-results",
    "title": "Exploring the count() Function in R’s tidyverse",
    "section": "Interpreting the Results",
    "text": "Interpreting the Results\nThe output is a tibble where the first column is the test result, and the second column, named n, shows the counts. Because we used sort = TRUE, the result with the highest count (negative) will appear first (n = 14,358 patients), followed by positive results (n = 865), and invalid results (n = 301)."
  },
  {
    "objectID": "posts/dplyr_count/post.html#adding-multiple-variables",
    "href": "posts/dplyr_count/post.html#adding-multiple-variables",
    "title": "Exploring the count() Function in R’s tidyverse",
    "section": "Adding Multiple Variables",
    "text": "Adding Multiple Variables\nYou can also count combinations of multiple variables. For example, if we want to count combinations of result and gender:\n\n\nShow the code\ncovid_testing %&gt;%\n  count(result, gender, sort = TRUE)\n\n\n# A tibble: 6 × 3\n  result   gender     n\n  &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;\n1 negative female  7237\n2 negative male    7121\n3 positive female   449\n4 positive male     416\n5 invalid  male     155\n6 invalid  female   146\n\n\nThis will give us a data frame with the counts of each result-gender combination, sorted by the count."
  },
  {
    "objectID": "posts/regular_expressions/post.html",
    "href": "posts/regular_expressions/post.html",
    "title": "Mastering Regular Expressions: Dealing with String Data in R, Part I",
    "section": "",
    "text": "Regular expressions (regex) are a powerful tool for working with string data in R. They might seem complex at first, but with some practice, they can become an invaluable part of your data science toolkit. In this blog post, we will tackle the first three exercises from the “R for Data Science” (2nd edition) book on regular expressions. Let’s dive into the world of regex and see how we can manipulate and search text data effectively."
  },
  {
    "objectID": "posts/regular_expressions/post.html#example-with-the-medicaldata-package",
    "href": "posts/regular_expressions/post.html#example-with-the-medicaldata-package",
    "title": "Exploring Regular Expressions (regex) in R’s tidyverse",
    "section": "Example with the medicaldata package",
    "text": "Example with the medicaldata package\nThe medicaldata package contains various datasets from medical research, which are great for demonstration. We’ll use the covid_testing dataset for our example.\nFirst, install and load the required packages:\n\n# install.packages(\"tidyverse\")\n# install.packages(\"medicaldata\")\n\nlibrary(tidyverse)\nlibrary(medicaldata)\n\nNext, load the covid_testing dataset and take a look at its structure:\n\ndata(\"covid_testing\")\nglimpse(covid_testing)\n\nRows: 15,524\nColumns: 17\n$ subject_id      &lt;dbl&gt; 1412, 533, 9134, 8518, 8967, 11048, 663, 2158, 3794, 4…\n$ fake_first_name &lt;chr&gt; \"jhezane\", \"penny\", \"grunt\", \"melisandre\", \"rolley\", \"…\n$ fake_last_name  &lt;chr&gt; \"westerling\", \"targaryen\", \"rivers\", \"swyft\", \"karstar…\n$ gender          &lt;chr&gt; \"female\", \"female\", \"male\", \"female\", \"male\", \"female\"…\n$ pan_day         &lt;dbl&gt; 4, 7, 7, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 1…\n$ test_id         &lt;chr&gt; \"covid\", \"covid\", \"covid\", \"covid\", \"covid\", \"covid\", …\n$ clinic_name     &lt;chr&gt; \"inpatient ward a\", \"clinical lab\", \"clinical lab\", \"c…\n$ result          &lt;chr&gt; \"negative\", \"negative\", \"negative\", \"negative\", \"negat…\n$ demo_group      &lt;chr&gt; \"patient\", \"patient\", \"patient\", \"patient\", \"patient\",…\n$ age             &lt;dbl&gt; 0.0, 0.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.0, 0.0, 0.9, 0.9,…\n$ drive_thru_ind  &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, …\n$ ct_result       &lt;dbl&gt; 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45…\n$ orderset        &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, …\n$ payor_group     &lt;chr&gt; \"government\", \"commercial\", NA, NA, \"government\", \"com…\n$ patient_class   &lt;chr&gt; \"inpatient\", \"not applicable\", NA, NA, \"emergency\", \"r…\n$ col_rec_tat     &lt;dbl&gt; 1.4, 2.3, 7.3, 5.8, 1.2, 1.4, 2.6, 0.7, 1.0, 7.1, 2.5,…\n$ rec_ver_tat     &lt;dbl&gt; 5.2, 5.8, 4.7, 5.0, 6.4, 7.0, 4.2, 6.3, 5.6, 7.0, 3.8,…\n\n\nThe covid_testing dataset contains data from deidentified results of COVID-19 testing at the Children’s Hospital of Pennsylvania (CHOP) in 2020. Suppose we want to count the number of participants according to the COVID-19 test result and sort the results by the count in descending order. Here’s how we can do that:\n\ncovid_testing %&gt;%\n  count(result, sort = TRUE)\n\n# A tibble: 3 × 2\n  result       n\n  &lt;chr&gt;    &lt;int&gt;\n1 negative 14358\n2 positive   865\n3 invalid    301"
  },
  {
    "objectID": "posts/regular_expressions/post.html#interpreting-the-results",
    "href": "posts/regular_expressions/post.html#interpreting-the-results",
    "title": "Exploring Regular Expressions (regex) in R’s tidyverse",
    "section": "Interpreting the Results",
    "text": "Interpreting the Results\nThe output is a tibble where the first column is the test result, and the second column, named n, shows the counts. Because we used sort = TRUE, the result with the highest count (negative) will appear first (n = 14,358 patients), followed by positive results (n = 865), and invalid results (n = 301)."
  },
  {
    "objectID": "posts/regular_expressions/post.html#adding-multiple-variables",
    "href": "posts/regular_expressions/post.html#adding-multiple-variables",
    "title": "Exploring Regular Expressions (regex) in R’s tidyverse",
    "section": "Adding Multiple Variables",
    "text": "Adding Multiple Variables\nYou can also count combinations of multiple variables. For example, if we want to count combinations of result and gender:\n\ncovid_testing %&gt;%\n  count(result, gender, sort = TRUE)\n\n# A tibble: 6 × 3\n  result   gender     n\n  &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;\n1 negative female  7237\n2 negative male    7121\n3 positive female   449\n4 positive male     416\n5 invalid  male     155\n6 invalid  female   146\n\n\nThis will give us a data frame with the counts of each result-gender combination, sorted by the count."
  },
  {
    "objectID": "posts/dplyr_count/post.html#exercise-2-why-patterns-dont-match-a-backslash",
    "href": "posts/dplyr_count/post.html#exercise-2-why-patterns-dont-match-a-backslash",
    "title": "Mastering Regular Expressions: Dealing with String Data in R, Part I",
    "section": "Exercise 2: Why Patterns Don’t Match a Backslash",
    "text": "Exercise 2: Why Patterns Don’t Match a Backslash\nQuestion: Explain why each of these patterns don’t match a \\:\n\n“\\”\n“\\\\”\n“\\\\\\”\n\nSolution:\nLet’s break down why these patterns fail to match a single backslash:\n\nstring &lt;- r\"(\\)\"\nstr_view(string)\n\n[1] │ \\\n\n# str_view(string, \"\\\") \n# This escapes the \", and the code is left incomplete\n\n# str_view(string, \"\\\\\") \n# This throws an error \"Unrecognized backslash escape secuence\", \\\\ is used to escape special characters, but none follows it\n\n# str_view(string, \"\\\\\\\")\n# This escapes the \", and the code is left incomplete\n\n# Correct way:\nstr_view(string, \"\\\\\\\\\") # This works because \\\\ in regex represents a literal backslash.\n\n[1] │ &lt;\\&gt;\n\n# Using raw strings (simplifies escaping):\nstr_view(string, r\"(\\\\)\") # Only needs to escape the backslash once.\n\n[1] │ &lt;\\&gt;"
  },
  {
    "objectID": "posts/dplyr_count/post.html#exercise-3-searching-within-a-corpus",
    "href": "posts/dplyr_count/post.html#exercise-3-searching-within-a-corpus",
    "title": "Mastering Regular Expressions: Dealing with String Data in R, Part I",
    "section": "Exercise 3: Searching Within a Corpus",
    "text": "Exercise 3: Searching Within a Corpus\nQuestion: Given the corpus of common words in stringr::words, create regular expressions that find all words that:\na. Start with “y”.\nb. Don’t start with “y”.\nc. End with “x”.\nd. Are exactly three letters long. (Don’t cheat by using str_length()!)\ne. Have seven letters or more.\nf. Contain a vowel-consonant pair.\ng. Contain at least two vowel-consonant pairs in a row.\nh. Only consists of repeated vowel-consonant pairs.\nSolution:\nHere are the regex patterns to match each condition:\n\n# Visualize all words\nstr_view(words)\n\n [1] │ a\n [2] │ able\n [3] │ about\n [4] │ absolute\n [5] │ accept\n [6] │ account\n [7] │ achieve\n [8] │ across\n [9] │ act\n[10] │ active\n[11] │ actual\n[12] │ add\n[13] │ address\n[14] │ admit\n[15] │ advertise\n[16] │ affect\n[17] │ afford\n[18] │ after\n[19] │ afternoon\n[20] │ again\n... and 960 more\n\n# a. Start with \"y\".\nstr_view(words, \"^y\")\n\n[975] │ &lt;y&gt;ear\n[976] │ &lt;y&gt;es\n[977] │ &lt;y&gt;esterday\n[978] │ &lt;y&gt;et\n[979] │ &lt;y&gt;ou\n[980] │ &lt;y&gt;oung\n\n# b. Don't start with \"y\".\nstr_view(words[!str_detect(words, \"^y\")])\n\n [1] │ a\n [2] │ able\n [3] │ about\n [4] │ absolute\n [5] │ accept\n [6] │ account\n [7] │ achieve\n [8] │ across\n [9] │ act\n[10] │ active\n[11] │ actual\n[12] │ add\n[13] │ address\n[14] │ admit\n[15] │ advertise\n[16] │ affect\n[17] │ afford\n[18] │ after\n[19] │ afternoon\n[20] │ again\n... and 954 more\n\n# c. End with \"x\".\nstr_view(words, \"x$\")\n\n[108] │ bo&lt;x&gt;\n[747] │ se&lt;x&gt;\n[772] │ si&lt;x&gt;\n[841] │ ta&lt;x&gt;\n\n# d. Are exactly three letters long. (Don't cheat by using str_length()!)\nstr_view(words, \"^[a-z]{3}$\")\n\n  [9] │ &lt;act&gt;\n [12] │ &lt;add&gt;\n [22] │ &lt;age&gt;\n [24] │ &lt;ago&gt;\n [26] │ &lt;air&gt;\n [27] │ &lt;all&gt;\n [38] │ &lt;and&gt;\n [41] │ &lt;any&gt;\n [51] │ &lt;arm&gt;\n [54] │ &lt;art&gt;\n [56] │ &lt;ask&gt;\n [68] │ &lt;bad&gt;\n [69] │ &lt;bag&gt;\n [73] │ &lt;bar&gt;\n [82] │ &lt;bed&gt;\n [89] │ &lt;bet&gt;\n [91] │ &lt;big&gt;\n [94] │ &lt;bit&gt;\n[108] │ &lt;box&gt;\n[109] │ &lt;boy&gt;\n... and 90 more\n\n# e. Have seven letters or more.\nstr_view(words, \"^[a-z]{7,}$\")\n\n [4] │ &lt;absolute&gt;\n [6] │ &lt;account&gt;\n [7] │ &lt;achieve&gt;\n[13] │ &lt;address&gt;\n[15] │ &lt;advertise&gt;\n[19] │ &lt;afternoon&gt;\n[21] │ &lt;against&gt;\n[31] │ &lt;already&gt;\n[32] │ &lt;alright&gt;\n[34] │ &lt;although&gt;\n[36] │ &lt;america&gt;\n[39] │ &lt;another&gt;\n[43] │ &lt;apparent&gt;\n[46] │ &lt;appoint&gt;\n[47] │ &lt;approach&gt;\n[48] │ &lt;appropriate&gt;\n[53] │ &lt;arrange&gt;\n[57] │ &lt;associate&gt;\n[61] │ &lt;authority&gt;\n[62] │ &lt;available&gt;\n... and 198 more\n\n# f. Contain a vowel-consonant pair.\nstr_view(words, \"[aeiou][^aeiou]\")\n\n [2] │ &lt;ab&gt;le\n [3] │ &lt;ab&gt;o&lt;ut&gt;\n [4] │ &lt;ab&gt;s&lt;ol&gt;&lt;ut&gt;e\n [5] │ &lt;ac&gt;c&lt;ep&gt;t\n [6] │ &lt;ac&gt;co&lt;un&gt;t\n [7] │ &lt;ac&gt;hi&lt;ev&gt;e\n [8] │ &lt;ac&gt;r&lt;os&gt;s\n [9] │ &lt;ac&gt;t\n[10] │ &lt;ac&gt;t&lt;iv&gt;e\n[11] │ &lt;ac&gt;tu&lt;al&gt;\n[12] │ &lt;ad&gt;d\n[13] │ &lt;ad&gt;dr&lt;es&gt;s\n[14] │ &lt;ad&gt;m&lt;it&gt;\n[15] │ &lt;ad&gt;v&lt;er&gt;t&lt;is&gt;e\n[16] │ &lt;af&gt;f&lt;ec&gt;t\n[17] │ &lt;af&gt;f&lt;or&gt;d\n[18] │ &lt;af&gt;t&lt;er&gt;\n[19] │ &lt;af&gt;t&lt;er&gt;no&lt;on&gt;\n[20] │ &lt;ag&gt;a&lt;in&gt;\n[21] │ &lt;ag&gt;a&lt;in&gt;st\n... and 924 more\n\n# g. Contain at least two vowel-consonant pairs in a row.\nstr_view(words, \"([aeiou][^aeiou]){2,}\")\n\n  [4] │ abs&lt;olut&gt;e\n [23] │ &lt;agen&gt;t\n [30] │ &lt;alon&gt;g\n [36] │ &lt;americ&gt;a\n [39] │ &lt;anot&gt;her\n [42] │ &lt;apar&gt;t\n [43] │ app&lt;aren&gt;t\n [61] │ auth&lt;orit&gt;y\n [62] │ ava&lt;ilab&gt;le\n [63] │ &lt;awar&gt;e\n [64] │ &lt;away&gt;\n [70] │ b&lt;alan&gt;ce\n [75] │ b&lt;asis&gt;\n [81] │ b&lt;ecom&gt;e\n [83] │ b&lt;efor&gt;e\n [84] │ b&lt;egin&gt;\n [85] │ b&lt;ehin&gt;d\n [87] │ b&lt;enefit&gt;\n[119] │ b&lt;usines&gt;s\n[143] │ ch&lt;arac&gt;ter\n... and 149 more\n\n# h. Only consists of repeated vowel-consonant pairs.\nstr_view(words, \"^([aeiou][^aeiou])\\\\1+$\") # No matches, let's try a positive match\n\n# Test with known patterns\npattern &lt;- \"^([aeiou][^aeiou])\\\\1+$\"\npos &lt;- c(\"anananan\", \"erer\")\nneg &lt;- c(\"nananana\", \"erere\", \"ananerer\")\n\nstr_view(pos, pattern)\n\n[1] │ &lt;anananan&gt;\n[2] │ &lt;erer&gt;\n\nstr_view(neg, pattern)\n# It seems to work, there are no words with this pattern in stringr::words\n\nWith these exercises, we’ve demonstrated how regular expressions can be used to manipulate and search text data in R. Whether you are searching for specific patterns or validating text data, regex provides a robust solution for your string processing needs.\nSee you soon in Part II of these exercises.\nHappy regexing!"
  },
  {
    "objectID": "posts/regular_expressions/post.html#exercise-2-why-patterns-dont-match-a-backslash",
    "href": "posts/regular_expressions/post.html#exercise-2-why-patterns-dont-match-a-backslash",
    "title": "Mastering Regular Expressions: Dealing with String Data in R, Part I",
    "section": "Exercise 2: Why Patterns Don’t Match a Backslash",
    "text": "Exercise 2: Why Patterns Don’t Match a Backslash\nQuestion: Explain why each of these patterns don’t match a \\:\n\n“\\”\n“\\\\”\n“\\\\\\”\n\nSolution:\nLet’s break down why these patterns fail to match a single backslash:\n\nstring &lt;- r\"(\\)\"\nstr_view(string)\n\n[1] │ \\\n\n# str_view(string, \"\\\") \n# This escapes the \", and the code is left incomplete\n\n# str_view(string, \"\\\\\") \n# This throws an error \"Unrecognized backslash escape secuence\", \\\\ is used to escape special characters, but none follows it\n\n# str_view(string, \"\\\\\\\")\n# This escapes the \", and the code is left incomplete\n\n# Correct way:\nstr_view(string, \"\\\\\\\\\") # This works because \\\\ in regex represents a literal backslash.\n\n[1] │ &lt;\\&gt;\n\n# Using raw strings (simplifies escaping):\nstr_view(string, r\"(\\\\)\") # Only needs to escape the backslash once.\n\n[1] │ &lt;\\&gt;"
  },
  {
    "objectID": "posts/regular_expressions/post.html#exercise-3-searching-within-a-corpus",
    "href": "posts/regular_expressions/post.html#exercise-3-searching-within-a-corpus",
    "title": "Mastering Regular Expressions: Dealing with String Data in R, Part I",
    "section": "Exercise 3: Searching Within a Corpus",
    "text": "Exercise 3: Searching Within a Corpus\nQuestion: Given the corpus of common words in stringr::words, create regular expressions that find all words that:\na. Start with “y”.\nb. Don’t start with “y”.\nc. End with “x”.\nd. Are exactly three letters long. (Don’t cheat by using str_length()!)\ne. Have seven letters or more.\nf. Contain a vowel-consonant pair.\ng. Contain at least two vowel-consonant pairs in a row.\nh. Only consists of repeated vowel-consonant pairs.\nSolution:\nHere are the regex patterns to match each condition:\n\n# Visualize all words\nstr_view(words)\n\n [1] │ a\n [2] │ able\n [3] │ about\n [4] │ absolute\n [5] │ accept\n [6] │ account\n [7] │ achieve\n [8] │ across\n [9] │ act\n[10] │ active\n[11] │ actual\n[12] │ add\n[13] │ address\n[14] │ admit\n[15] │ advertise\n[16] │ affect\n[17] │ afford\n[18] │ after\n[19] │ afternoon\n[20] │ again\n... and 960 more\n\n# a. Start with \"y\".\nstr_view(words, \"^y\")\n\n[975] │ &lt;y&gt;ear\n[976] │ &lt;y&gt;es\n[977] │ &lt;y&gt;esterday\n[978] │ &lt;y&gt;et\n[979] │ &lt;y&gt;ou\n[980] │ &lt;y&gt;oung\n\n# b. Don't start with \"y\".\nstr_view(words[!str_detect(words, \"^y\")])\n\n [1] │ a\n [2] │ able\n [3] │ about\n [4] │ absolute\n [5] │ accept\n [6] │ account\n [7] │ achieve\n [8] │ across\n [9] │ act\n[10] │ active\n[11] │ actual\n[12] │ add\n[13] │ address\n[14] │ admit\n[15] │ advertise\n[16] │ affect\n[17] │ afford\n[18] │ after\n[19] │ afternoon\n[20] │ again\n... and 954 more\n\n# c. End with \"x\".\nstr_view(words, \"x$\")\n\n[108] │ bo&lt;x&gt;\n[747] │ se&lt;x&gt;\n[772] │ si&lt;x&gt;\n[841] │ ta&lt;x&gt;\n\n# d. Are exactly three letters long. (Don't cheat by using str_length()!)\nstr_view(words, \"^[a-z]{3}$\")\n\n  [9] │ &lt;act&gt;\n [12] │ &lt;add&gt;\n [22] │ &lt;age&gt;\n [24] │ &lt;ago&gt;\n [26] │ &lt;air&gt;\n [27] │ &lt;all&gt;\n [38] │ &lt;and&gt;\n [41] │ &lt;any&gt;\n [51] │ &lt;arm&gt;\n [54] │ &lt;art&gt;\n [56] │ &lt;ask&gt;\n [68] │ &lt;bad&gt;\n [69] │ &lt;bag&gt;\n [73] │ &lt;bar&gt;\n [82] │ &lt;bed&gt;\n [89] │ &lt;bet&gt;\n [91] │ &lt;big&gt;\n [94] │ &lt;bit&gt;\n[108] │ &lt;box&gt;\n[109] │ &lt;boy&gt;\n... and 90 more\n\n# e. Have seven letters or more.\nstr_view(words, \"^[a-z]{7,}$\")\n\n [4] │ &lt;absolute&gt;\n [6] │ &lt;account&gt;\n [7] │ &lt;achieve&gt;\n[13] │ &lt;address&gt;\n[15] │ &lt;advertise&gt;\n[19] │ &lt;afternoon&gt;\n[21] │ &lt;against&gt;\n[31] │ &lt;already&gt;\n[32] │ &lt;alright&gt;\n[34] │ &lt;although&gt;\n[36] │ &lt;america&gt;\n[39] │ &lt;another&gt;\n[43] │ &lt;apparent&gt;\n[46] │ &lt;appoint&gt;\n[47] │ &lt;approach&gt;\n[48] │ &lt;appropriate&gt;\n[53] │ &lt;arrange&gt;\n[57] │ &lt;associate&gt;\n[61] │ &lt;authority&gt;\n[62] │ &lt;available&gt;\n... and 198 more\n\n# f. Contain a vowel-consonant pair.\nstr_view(words, \"[aeiou][^aeiou]\")\n\n [2] │ &lt;ab&gt;le\n [3] │ &lt;ab&gt;o&lt;ut&gt;\n [4] │ &lt;ab&gt;s&lt;ol&gt;&lt;ut&gt;e\n [5] │ &lt;ac&gt;c&lt;ep&gt;t\n [6] │ &lt;ac&gt;co&lt;un&gt;t\n [7] │ &lt;ac&gt;hi&lt;ev&gt;e\n [8] │ &lt;ac&gt;r&lt;os&gt;s\n [9] │ &lt;ac&gt;t\n[10] │ &lt;ac&gt;t&lt;iv&gt;e\n[11] │ &lt;ac&gt;tu&lt;al&gt;\n[12] │ &lt;ad&gt;d\n[13] │ &lt;ad&gt;dr&lt;es&gt;s\n[14] │ &lt;ad&gt;m&lt;it&gt;\n[15] │ &lt;ad&gt;v&lt;er&gt;t&lt;is&gt;e\n[16] │ &lt;af&gt;f&lt;ec&gt;t\n[17] │ &lt;af&gt;f&lt;or&gt;d\n[18] │ &lt;af&gt;t&lt;er&gt;\n[19] │ &lt;af&gt;t&lt;er&gt;no&lt;on&gt;\n[20] │ &lt;ag&gt;a&lt;in&gt;\n[21] │ &lt;ag&gt;a&lt;in&gt;st\n... and 924 more\n\n# g. Contain at least two vowel-consonant pairs in a row.\nstr_view(words, \"([aeiou][^aeiou]){2,}\")\n\n  [4] │ abs&lt;olut&gt;e\n [23] │ &lt;agen&gt;t\n [30] │ &lt;alon&gt;g\n [36] │ &lt;americ&gt;a\n [39] │ &lt;anot&gt;her\n [42] │ &lt;apar&gt;t\n [43] │ app&lt;aren&gt;t\n [61] │ auth&lt;orit&gt;y\n [62] │ ava&lt;ilab&gt;le\n [63] │ &lt;awar&gt;e\n [64] │ &lt;away&gt;\n [70] │ b&lt;alan&gt;ce\n [75] │ b&lt;asis&gt;\n [81] │ b&lt;ecom&gt;e\n [83] │ b&lt;efor&gt;e\n [84] │ b&lt;egin&gt;\n [85] │ b&lt;ehin&gt;d\n [87] │ b&lt;enefit&gt;\n[119] │ b&lt;usines&gt;s\n[143] │ ch&lt;arac&gt;ter\n... and 149 more\n\n# h. Only consists of repeated vowel-consonant pairs.\nstr_view(words, \"^([aeiou][^aeiou])\\\\1+$\") # No matches, let's try a positive match\n\n# Test with known patterns\npattern &lt;- \"^([aeiou][^aeiou])\\\\1+$\"\npos &lt;- c(\"anananan\", \"erer\")\nneg &lt;- c(\"nananana\", \"erere\", \"ananerer\")\n\nstr_view(pos, pattern)\n\n[1] │ &lt;anananan&gt;\n[2] │ &lt;erer&gt;\n\nstr_view(neg, pattern)\n# It seems to work, there are no words with this pattern in stringr::words\n\nWith these exercises, we’ve demonstrated how regular expressions can be used to manipulate and search text data in R. Whether you are searching for specific patterns or validating text data, regex provides a robust solution for your string processing needs.\nSee you soon in Part II of these exercises.\nHappy regexing!"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Carlos Fernández-Escobar, Curriculum Vitae",
    "section": "",
    "text": "Contact Information\nLocation: Alicante, Spain\nEmail: carlosepistats@gmail.com\n\n\nSocial Media\nLinkedIn: CFEpidemiology\n\n\nTechnical Skills\nStatistical Programming:\n\nAdvanced R: tidyverse, ggplot, shiny, markdown, Quarto\nIntermediate Python: pandas, numpy, matplotlib\nAdvanced Stata\nSQL, Power BI, Microsoft Excel\n\nData Analysis Skills:\n\nData cleaning and manipulation\nDescriptive analysis and data visualization\nInferential analysis, regression models, time-series, and multilevel models\nReporting, table, list, and figure generation\n\nDesign and analysis of observational and interventional studies: cohort, case-control, transversal, modelling, time-series, randomized clinical trials, causal inference.\nExperience with large healthcare databases, real-world evidence, national health surveys, public health surveillance databases.\n\n\nWork Experience\nMedical Research Fellow\nBarcelona Institute for Global Health (ISGlobal), Spain\n2024 - now\n\nBiostatistician in charge of analysis of Randomized Controlled Trials and observational studies\nProvide epidemiological and statistical support to trainees.\n\nBiostatistician & Scientific Advisor\nMedical Statistics Consulting (MSC), Valencia, Spain\n2022 - 2023\n\nCollaborated with clinical and industry clients on Real World Evidence projects.\nPerformed data cleaning, manipulation, analysis, and reporting.\nTaught Biostatistics, Study Design, and Critical Appraisal of medical literature to clinicians and pharmaceutical companies.\n\nResearcher & Data Manager\nEpidemiology and Public Health Research Group, University of Alcalá (UAH), Madrid, Spain\n2022\n\nManaged data and designed observational studies.\nConducted data analysis.\n\nMedical Resident, Preventive Medicine and Public Health\nInstitute of Health Carlos III, Madrid, Spain\n2017 - 2021\n\nSpecialized in research methodology, healthcare research support, database management, and statistical analyses.\nDeveloped national COVID-19 situation reports at the National Center of Epidemiology.\nProvided epidemiological and statistical support to over 10 healthcare and public health research projects.\nTaught Biostatistics and Epidemiology.\n\n\n\nEducation\nMedical Specialist in Preventive Medicine and Public Health\nInstitute of Health Carlos III, Madrid, Spain\n2017 - 2021\nMaster in Public Health (MPH)\nInstitute of Health Carlos III, Madrid, Spain\n2018\nDoctor of Medicine (MD)\nUniversity of Castilla-La Mancha (UCLM), Spain\n2010 - 2016\n\n\nScientific Publications\nORCID: 0000-0001-7361-3383\n\n\nLanguages\n\nEnglish: C2 ECFR\nSpanish: Native\n\n\n\nGeneral Competences\n\nQuick learner\nAnalytical problem-solving\nDetail-oriented\nCommunication skills\nMulti-disciplinary teamwork\n\n\n\nOther Interests\n\nMember of MENSA International\nLudoSport athlete"
  },
  {
    "objectID": "posts/regular_expressions_2/post.html",
    "href": "posts/regular_expressions_2/post.html",
    "title": "Mastering Regular Expressions: Dealing with String Data in R, Part II",
    "section": "",
    "text": "Regular expressions (regex) are a powerful tool for working with string data in R. They might seem complex at first, but with some practice, they can become an invaluable part of your data science toolkit. In this blog post, we will tackle the last four exercises from the “R for Data Science” (2nd edition) book on regular expressions. You can find the first part of the exercises here.\nLet’s dive into the world of regex and see how we can manipulate and search text data effectively."
  },
  {
    "objectID": "posts/regular_expressions_2/post.html#crossword-1",
    "href": "posts/regular_expressions_2/post.html#crossword-1",
    "title": "Mastering Regular Expressions: Dealing with String Data in R, Part II",
    "section": "Crossword 1",
    "text": "Crossword 1\n\n\npattern_row_1 &lt;- \"HE|LL|O+\"\npattern_row_2 &lt;- \"[PLEASE]+\"\npattern_column_1 &lt;- \"[^SPEAK]+\"\npattern_column_2 &lt;- \"EP|IP|EF\"\n\nsolution &lt;- c(\"H\", \"E\", \"L\", \"P\")\n\ncheck_crosswords(pattern_row_1, pattern_row_2, pattern_column_1, pattern_column_2, solution)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/regular_expressions_2/post.html#crossword-2",
    "href": "posts/regular_expressions_2/post.html#crossword-2",
    "title": "Mastering Regular Expressions: Dealing with String Data in R, Part II",
    "section": "Crossword 2",
    "text": "Crossword 2\n\n\npattern_row_1 &lt;- \".*M?O.*\"\npattern_row_2 &lt;- \"(AN|FE|BE)\"\npattern_column_1 &lt;- \"(A|B|C)\\\\1\"\npattern_column_2 &lt;- \"(AB|OE|SK)\"\n\nsolution &lt;- c(\"B\", \"O\", \"B\", \"E\")\n\ncheck_crosswords(pattern_row_1, pattern_row_2, pattern_column_1, pattern_column_2, solution)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/regular_expressions_2/post.html#crossword-3",
    "href": "posts/regular_expressions_2/post.html#crossword-3",
    "title": "Mastering Regular Expressions: Dealing with String Data in R, Part II",
    "section": "Crossword 3",
    "text": "Crossword 3\n\n\npattern_row_1 &lt;- \"(.)+\\\\1\"\npattern_row_2 &lt;- \"[^ABRC]+\"\npattern_column_1 &lt;- \"[COBRA]+\"\npattern_column_2 &lt;- \"(AB|O|OR)+\"\n\nsolution &lt;- c(\"O\", \"O\", \"O\", \"O\")\n\ncheck_crosswords(pattern_row_1, pattern_row_2, pattern_column_1, pattern_column_2, solution)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/regular_expressions_2/post.html#crossword-4",
    "href": "posts/regular_expressions_2/post.html#crossword-4",
    "title": "Mastering Regular Expressions: Dealing with String Data in R, Part II",
    "section": "Crossword 4",
    "text": "Crossword 4\n\n\npattern_row_1 &lt;- \"[*]+\"\npattern_row_2 &lt;- \"/+\"\npattern_column_1 &lt;- \".?.+\"\npattern_column_2 &lt;- \".+\"\n\nsolution &lt;- c(\"*\", \"*\", \"/\", \"/\")\n\ncheck_crosswords(pattern_row_1, pattern_row_2, pattern_column_1, pattern_column_2, solution)\n\n[1] TRUE"
  },
  {
    "objectID": "posts/regular_expressions_2/post.html#crossword-5",
    "href": "posts/regular_expressions_2/post.html#crossword-5",
    "title": "Mastering Regular Expressions: Dealing with String Data in R, Part II",
    "section": "Crossword 5",
    "text": "Crossword 5\n\n\npattern_row_1 &lt;- \"18|19|20\"\npattern_row_2 &lt;- \"[6789]\\\\d\"\npattern_column_1 &lt;- \"\\\\d[2480]\"\npattern_column_2 &lt;- \"56|94|73\"\n\nsolution &lt;- c(\"1\", \"9\", \"8\", \"4\")\n\ncheck_crosswords(pattern_row_1, pattern_row_2, pattern_column_1, pattern_column_2, solution)\n\n[1] TRUE"
  }
]