---
title: 'Regresión Lasso'
date: "2024-02-26"
categories: ['regresión', 'R', 'lasso']
description: "Qué es, para qué se usa y cómo usar la regresión Lasso, con código en R."
execute: 
  message: false
  warning: false
editor_options: 
  chunk_output_type: console
---

# Introducción

En este post explico qué es la regresión Lasso, para qué se usa y cómo realizarla usando código en R.


## Qué es la regresión Lasso

La regresión Lasso es una variante de la regresión lineal que busca obtener un modelo lo más simple posible. Para ello, penaliza los modelos con coeficientes grandes, dejando coeficientes pequeños e incluso eliminando variables del modelo (igualando sus coeficientes a cero).

Lasso es un acrónimo de Least Absolute Shrinkage and Selector Operator.

## Para qué se usa la regresión Lasso

Lasso tiene dos funciones principales:

* Selección de variables para un modelo: como puede convertir los coeficientes de las variables en ceros, puede usarse para seleccionar las variables más importantes del modelo y excluir otras. 
* Contracción de parámetros (*shrinkage*): los coeficientes del modelo Lasso son más pequeños que en una regresión lineal normal. Esto ayuda al reducir el peligro de sobreajuste (*overfitting*) del modelo.

## Cuándo se usa la regresión Lasso

Lasso se suele utilizar en las siguientes situaciones:

* Cuando tenemos bases de datos con **alta dimensionalidad**(con muchas variables).
* Cuando tenemos modelos con **multicolinealidad**.
* Cuando queremos **automatizar la construcción de un modelo** (automatizando la selección de variables).

## Cómo funciona la regresión Lasso

Un **modelo de regresión lineal múltiple** busca el conjunto de coeficientes ($\beta_0, \beta_1, \beta_2...$) que minimiza la suma de los cuadrados de los residuales. Esto es, la distancia entre los datos y la predicción del modelo (residual) se eleva al cuadrado y se suma. El método de mínimos cuadrados de la regresión lineal busca minimizar esta suma.

La **regresión Lasso añade otro parámetro conocido como L1**. L1 es la suma de los valores absolutos de los coeficientes del modelo. Lasso busca minimizar la suma de L1 y la suma de cuadrados. Por lo tanto, Lasso busca el menor valor absoluto posible de los coeficientes, que se traduce en menor L1. Este proceso se llama "regularización L1", y produce una "contracción" de los coeficientes (los hace más pequeños).

Toda regresión Lasso incluye un parámetro adicional, **lambda** ($\lambda$). Lambda mide cuánta importancia le da el modelo al parámetro L1:

* Si $\lambda = 0$, no hay contracción de coeficientes y el modelo es equivalente a una regresión lineal normal.
* Conforme $\lambda$ aumenta, hay más contracción de coeficientes y se eliminan más variables.
* Si $\lambda$ = infinito, hay contracción máxima y se eliminan todos los coeficientes.

## Fórmula de la regresión Lasso

$min(RSS + \lambda \sum |\beta_j|)$

Donde

* $RSS$ es la suma de residuos cuadrados.
* $\lambda$ es el parámetro de penalización de Lasso.
* $\sum |\beta_j|$ es la suma de los valores absolutos de los coeficientes de las variables.

# Código en R

## Preparativos

Para este ejemplo, usaremos la biblioteca `glmnet` y la biblioteca de ejemplo `mtcars`.

```{r}
# install.packages("glmnet") # Instalar el paquete
library(glmnet)
head(mtcars)
```

Usaremos `mpg` (millas por galón) como la variable resultado, y `cyl` (nº. de cilindros), `hp` (caballos de vapor), `wt` (peso), `gear` (nº. de marchas) y `drat` (relación del eje) como predictoras. 

```{r}
# definir la variable resultado
y <- mtcars$mpg

# definir las variables predictoras
x <- data.matrix(mtcars[, c("cyl", "hp", "wt", "drat", "gear")])
```


## Elegir el valor de lambda

Podemos elegir el valor de $\lambda$ que minimice el error cuadrado medio (*mean-squared error*, MSE). La función `cv.glmnet()` realiza "validación cruzada de k iteraciones" ([K-fold cross-validation](https://www.statology.org/k-fold-cross-validation/)) para identificar este valor de $\lambda$.

```{r}
# validación cruzada
cv_model <- cv.glmnet(x, y, alpha = 1)  # Cambiar el parámetro alpha da lugar a otros tipos de regresión

# encontrar el valor lambda que minimiza el MSE
best_lambda <- cv_model$lambda.min
best_lambda

# mostrar los resultados en un gráfico
plot(cv_model)
```

El valor de lambda que minimiza el MSE resulta ser `r best_lambda`, que en la gráfica corresponde al punto $Log(\lambda)$ = `r log(best_lambda)`.

## Ajustar el modelo

```{r}
# coeficientes del modelo 
best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)
```

Podemos observar como el coeficiente de `gear` aparece como un punto, lo que indica que la regresión Lasso ha eliminado el coeficiente, ya que la variable no era lo suficientemente importante.

## Comparación con la regresión lineal sin Lasso

Como comparación, podemos ver los coeficientes que resultarían de un modelo de regresión lineal múltiple sin contracción de parámetros ni selección de variables.

```{r}
linear_model <- lm(mpg ~ cyl + hp + wt + drat + gear, data = mtcars)
cbind(coef(best_model), "Linear" = coef(linear_model))
```

Los coeficientes del modelo Lasso se han contraído un poco, especialmente de la variable `drat`, y la variable `gear` ha sido automáticamente excluida.

# Dudas y curiosidades

Algunas preguntas que me surgen para investigar y escribir nuevos posts:

* ¿Cómo elegir el valor de lambda?
* ¿Qué es el método de *K-fold cross-validation*?
* ¿Qué diferencia a Lasso de otros modelos similares como Ridge?
* ¿Qué utilidad tiene la regresión Lasso en el mundo de la salud pública? ¿Qué base de datos puedo usar como ejemplo?
* ¿Qué artículos hay publicados con esta metodología?


# Referencias

* [Statology](https://www.statology.org/lasso-regression-in-r/)
* [Geeks for Geeks](https://www.geeksforgeeks.org/lasso-regression-in-r-programming/)

